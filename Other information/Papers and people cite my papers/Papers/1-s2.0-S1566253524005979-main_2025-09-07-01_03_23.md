

<!-- Meanless: Information Fusion 117 (2025) 102819 Contents lists available at ScienceDirect Information Fusion ELSEVIER journal homepage: www.elsevier.com/locate/inffus updates-->

Full length article

# Pred-ID: Future event prediction based on event type schema mining by graph induction and deduction

Huan Rong ${}^{a}$ , Zhongfeng Chen ${}^{b, * }$ , Zhenyu Lu ${}^{a, * }$ , Xiao-ke Xu ${}^{c}$ , Kai Huang ${}^{a}$ , Victor S. Sheng ${}^{d}$

${}^{a}$ School of Artificial Intelligence,Nanjing University of Information Science & Technology,Jiangsu,Nanjing,210-044,China

${}^{b}$ School of Electronics and Information Engineering,Nanjing University of Information Science & Technology,Jiangsu,Nanjing,210-044,China

${}^{c}$ Computational Communication Research Center and the School of Journalism and Communication,Beijing Normal University,Beijing,100875,China

${}^{d}$ Department of Computer Science,Texas Tech University,2500 Broadway,Lubbock,TX,79409,United States of America

## ARTICLEINFO

Keywords:

Event intelligence analysis

Event graph mining

Graph generation

Event prediction

## A B S T R A C T

In the field of information management, effective event intelligence management is crucial for its development. With the continuous evolution of events, predicting future events has become a key task in information management. Event Prediction aims to predict upcoming events based on given contextual information. This requires modeling events and their relationships in the context to infer the structure of future events. However, the existing event prediction methods ignore that the event graph schema based on core events can provide more knowledge about history and future for event prediction through induction and deduction, so as to achieve accurate event prediction. In addressing this issue, we directed our focus towards Event Schema Induction. Inspired by it, we propose the Pred-ID model, designed to build event evolutionary pattern through Inductive Event Graph Generation, Deductive Event Graph Expansion, and Graph Fusion for Event Prediction. Specifically, in the Inductive Event Graph Generation phase, Pred-ID extracts the event core subgraph and event developmental trends from the instance event graph, learning the global structure and uncovering the main processes of event development. Then, in the Deductive Event Graph Expansion phase, by expanding future event node and stretching the main processes of event development into future directions, Pred-ID obtains deductive results, so as to construct the event evolutionary pattern. Finally, in the Graph Fusion for Event Prediction phase, aligning and merging the event evolutionary pattern with the instance event graph enables collaborative prediction of future events. The experimental results indicate that our proposed Pred-ID achieves optimal performance in event evolutionary pattern generation and event prediction tasks.

## 1. Introduction

As it said by Rabindranath Tagore (the Indian Poet and Philosopher, 1861-1941): "The world is in motion, which is a complete fact". The "motion" of our daily life can be depicted by a series of events, which are usually linked in temporal or causal order $\left\lbrack  {1,2}\right\rbrack$ . Obviously, mining event evolutionary pattern or core event-dependency from an already-occurred situation can bring about more insights into the future, in turn, facilitating more precise event prediction to take effective measures earlier, especially in the scenario of public security monitoring [3-5] and event intelligence analysis [6-10].

Generally, an event can consist of a trigger (or core action) and a set of arguments including subjects, objects, location, time, etc [11]. From the aspect of knowledge acquisition and application current knowledge-enhanced works usually extract event mentions (e.g., triggers and arguments) from large-scale corpora, then organize event-event, event-argument and argument-argument relations as Event-centric Knowledge Graph (EKG) [12]. In EKG or event graph, the event-pairs, event-segments, and event-chains will be attended to ascertain the main event-dependency, to facilitate the event prediction [13-16]. However, the above works are all mainly based on event (knowledge) extraction [17] and representation [18], where the future event is predicted by checking the similarity between the candidate options and the context (e.g., the most associated event-segments or event-chains), lacking fine-grained consideration on event evolutionary pattern that consist of event types (i.e., generalized event triggers) and temporal orders between events in the schema-level.

Fortunately, with the differentiation of "Instance" and "Schema", current works on Event Schema Induction have offered valuable inspiration on event evolutionary pattern mining. As shown in Fig. 1 (a), event schema induction aims at equating an appropriate template to define the critical attributes or arguments constituting the complete event. Such the template will be considered as the schema to identify event type [19], then entities will be extracted from the given corpus and filled into the template to construct multiple event-instances in the same event type (i.e., event schema) [20]. For example, as shown in Fig. 1 (b), the left side represents different instance event knowledge graphs, and their contents will be instance in the event schema graph shown on the right side. Where specific nouns in the instance event graph (e.g. Russia, Ukraine, police, tank et al.) are inducted into event or instance types in the event schema graph (e.g. PER, WEA, FAC, GPE et al.), and event-triggers (deploy, carry et al.) will be translated as event types node in the event schema(Transport, Attack et al.) [21].

---

<!-- Footnote -->

* Corresponding authors.

E-mail addresses: 1227558210@qq.com (H. Rong), 643803488@qq.com (Z. Chen), luzhenyu76@163.com (Z. Lu), xuxiaoke@foxmail.com (X. Xu), 3326811917@qq.com (K. Huang), victor.sheng@ttu.edu (V.S. Sheng).

<!-- Footnote -->

---

<!-- Meanless: https://doi.org/10.1016/j.inffus.2024.102819 Received 19 September 2024; Received in revised form 3 November 2024; Accepted 19 November 2024 Available online 26 November 2024 1566-2535/(C) 2024 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

<!-- Media -->

<!-- figureText: Sentence Pre-defined Template as Event (Type) Schema Bombing Type Perpetrator $\mathbf{{Victim}}$ Target Instrument El Salvador The police chief Ministry Explosives The guerrillas Students The embassy car bomb The drag mafia The Peruvian embassy The police station Dynamite Drug traffickers The diplomat Organization incendiary bomb The Atlacatl battalion soldiers bridge vehicle bomb Different Events in the Same Type after Information Extraction (a) Template & Information Extraction based Event Schema Induction to define single Event Type. instrumen Induction Transport WEA place PER Event Schema Graph for Complex Transport-Attack Type A car bomb exploded in front of the U.S. embassy residence Bombing Template entity 1 entity 2 Perpetrator: Person in the Peruvian capital Person entity 3 Entity Representation Target: Public Instrument: bomb Entity 1: h=bomb, p=explode, d=subject, f=\{hyper=\{explosive, weaponry..\} sentence=5, passage=41\} Given Text PER Transport Transport GPE destination origin ......... target Different Instance Event Knowledge Graphs for Transport-Attack (b) Graph Generation based Event Schema Induction to define complex Event-Event Type. -->

<img src="https://cdn.noedgeai.com/bo_d2u6h13ef24c73b31eqg_1.jpg?x=205&y=148&w=1334&h=754&r=0"/>

Fig. 1. Existing works on event schema induction for event type definition.

<!-- Media -->

Consequently, inspired by the existing works on Event Schema Induction to define complex event-event type (see Fig. 1(b)), in this paper, we now address two broader inquiries. First, when given an instance event (knowledge) graph where the nodes are events (conclude event triggers and event arguments) and edges include trigger-trigger (i.e., directed in temporal order), trigger-argument, and argument-argument interactions, not only the dependencies between event pairs should be considered, at the same time, the main structure of the entire instance event graph should be induced at schema-level, and it is regarded as the core evolution process of the event graph. Second, based on the event pattern evolution process, the induced event graph is further extended by connecting the existing event pattern to multiple appropriate future candidate events (i.e., chronological orientation). This process can estimate the evolution pattern of the event from the history to the future from induction and deduction, so as to provide more knowledge for the prediction of future events.

Based on all mentioned above, focusing on the induction and deduction of the entire event (knowledge) graph, we have proposed Pred-ID. The proposed model can extract event evolution pattern from instance event graph, and improve the accuracy of event prediction by inducing historical events and deducing future events on the event pattern. The overall conception of our proposed Pred-ID has been illustrated in Fig. 2, with the main contribution listed as follows:

1. Inductive Event Graph Generation: At each time step, an instance event graph will be given to depict the event development from history to present, where events are all represented by their triggers and other relationship (e.g. Fig. 1: PER, WEA, FAC, GPE et al.) in temporal order. After that, an event-to-event inductive event graph is generated at each time step, specifically, we will extract the development schema of all event pairs from the whole Instance event graph (e.g. Fig. 1(b): Different instance event Knowledge Graphs for Transport-Attack). The core Event evolution (or main Graph structure) is induced from the same pair of events (e.g. Fig. 1(b): Event Schema Graph for Complex Transport-Attack Type). Finally, we use pre-defined event-type schema to inductive events and their relationship into Event-Type Graph (e.g. "[bomb, kidnapping, robbery] $\rightarrow$ attack (✓)").

2. Deductive Event Graph Expansion: As the graph deduction for expansion, all the events (i.e., the generalized event triggers) in the induced event graph will seek the opportunity to be connected to the most possible event types (i.e., the generalized event triggers) selected from the event-library via multiple iterations. Here, the connections between the already-existed events and those newly added ones will be directed in temporal order and should obey the overall distribution of the induced event graph. In this way, at each time step, the core event development depicted by event graph induction and the expansion towards future event types contributed by event graph deduction will altogether constitute the estimated event evolutionary pattern.

3. Graph Fusion for Event Prediction: At each time step, the estimated event evolutionary pattern (i.e., the graph structure derived from both the event graph induction and deduction) will be fused with the original instance event graph through proper node alignment. Taking the fusion graph as knowledge, the association between the fusion graph and all the possible future candidate events (i.e., the event triggers and corresponding arguments at instance-level) will be attended, where the future event about to occur in the next time step will be decided by the most critical context in the fusion graph. In this way, in Fig. 2, after adding the predicted future event into the current instance event graph, all the mentioned above will be continued to facilitate the following prediction steps.

According to the experimental results, our proposed Pred-ID outperforms state-of-the-art counterparts in achieving optimal event prediction performance at both the single-step and long-term multi-step levels. Notably, by conducting event graph induction (capturing core event developments) and event graph deduction (gaining insights into potential future event types) at each time step, Pred-ID offers the broadest observational context within the fused graph to determine future events, thereby ensuring more accurate event predictions.

<!-- Meanless: 2-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

<!-- Media -->

<!-- figureText: Event Deduction schema Deduction schema Deduction schema Deduction schema Expansion Expansion Induction Induction Event Schema Schema Ontology Graph Gen ...... Graph Ger Schema $\rightarrow$ Pred T3 TN Evolutionary Expansion Expansion Pattern Mining Induction Induction Event Schema Schema Ontology Graph Gen Graph Ger APred T1 -->

<img src="https://cdn.noedgeai.com/bo_d2u6h13ef24c73b31eqg_2.jpg?x=229&y=145&w=1289&h=335&r=0"/>

Fig. 2. The conception of our proposed Pred-ID on inductive and deductive graph-based evolutionary pattern mining for event prediction.

<!-- Media -->

## 2. Related works

The proposed Pred-ID aims to describe the evolution of events from the past to the present through the use of event schemas, revealing the schema of event evolution through the processes of schema induction and deduction. Ultimately, the identified event evolutionary pattern are integrated with the original instance event graph to facilitate event prediction.

### 2.1. Time-series & script event prediction

Script event prediction aims to accurately predict future events based on given context events, enabling appropriate deployment for subsequent potential events. Existing script event prediction models generally represent scripts in three forms: event pairs, event chains, and event graphs. These models predict subsequent events based on the corresponding script form.

The prediction models based on event pairs primarily focus on modeling the associations between event pairs. PMI [22] determined pairwise relationships between events using the distribution score of frequency regarding shared syntactic arguments between two events. Event-Comp [23] used a neural network model to learn event embed-dings. It constructs event embeddings based on word embeddings and predicts the strength of association between two events to predicted subsequent events. Inspired by knowledge graph embedding models, EventTransE [24] built a rich set of relationships and learns event em-beddings for a multi-relational problem. Additionally, representation learning methods like Word2Vec [25] can also train event embeddings to capture relationships between event pairs. Pair-based models are limited to two consecutive events, whereas scripts typically comprise multiple events. These models struggle to capture relationships between long-distance events in scripts, leading to insufficient utilization of script information.

Event chains are primarily processed using natural language processing methods. Rudinger et al. [26] reframe script event prediction as a language modeling task and train a discriminative language model for this purpose. Ahrendt and Demberg [27] considered information about participants in events, summarized the roles of participants from the script, and based on this, used skip-grams to collect narrative event statistics and score candidate events. Pichotta and Mooney [28] proposed a recursive neural network model using classical Long Short-Term Memory (LSTM) for statistical script learning to predict subsequent events. Similarly, Pichotta and Mooney [29] employ sentence-level LSTM for probabilistic reasoning on scripts. PairLSTM [30] combines event pair modeling and event chain modeling using LSTM. It utilizes the LSTM hidden states as features for event pair modeling, employs a dynamic memory network to automatically infer weights for existing events, and infers subsequent events accordingly. SAM-Net [31] integrates attention mechanisms at both the event and chain levels. It uses event-level attention to simulate the relationships between subsequent events and individual events. Simultaneously, chain-level attention models the relationships between subsequent events and segments within the event chain. This enables the combination of individual events and event segments within a chain to predict subsequent events effectively. Transformer models, such as Bidirectional Encoder Representations from Transformers (BERT [32]) and Robustly optimized BERT approach (RoBERTa [33]), are also applicable to script event prediction. MCPredictor [34] is a transformer-based model that, at the event level, utilizes rich information from the text to obtain a more comprehensive semantic representation of events. At the script level, it considers multiple event sequences corresponding to different participants for subsequent events. The model integrates deep event-level and script-level information for script event prediction. Lv et al. [35] integrated event knowledge from an external knowledge base, leveraging the interaction between event textual information and external event knowledge to aid in predicting the correct answers. Zhou et al. [36] proposed two self-supervised pre-training tasks: terminal recognition and contrastive scoring. Subsequently, these two tasks were integrated using a multi-task learning framework to jointly train the model. The pre-trained model was then fine-tuned using manually annotated script event prediction training data. Bai et al. [37] designed an event extractor to extract rich events from the text. By using elaborate event descriptions to precisely express events, they employed a comprehensive event encoder to flexibly capture subtle interactions between verbs and any number of arguments, ultimately predicting subsequent events. Zhu et al. [38] introduced a novel event-level cloze-fill strategy as a learning objective to inject event-level knowledge into a pre-trained language model. They then designed a likelihood-based contrastive loss for fine-tuning the generation model. Instead of employing additional prediction layers, they utilized the likelihood of sequences induced by the generation model for predictions. Compared to pair-based models, chain-based models can better model semantic information in scripts but still struggle to model rich relational knowledge between events.

Models based on event graphs conceptualize scripts as graph structures. SGNN [39] extracted narrative event chains from a large corpus of news data, constructs a narrative event evolution graph based on the extracted chains, and learns better event embeddings by modeling interactions among events (actual verbs) on the evolution graph. HeterEvent [40] models words and events as a heterogeneous event graph network, exploring the impact of word-word, word-event, and event-event relationships. Then, through a multi-structure comparison learning framework, it is further expanded to enhance the robustness of graph training. Du et al. [41] employed the BERT model to automatically construct event graphs. They introduced an additional structured variable into BERT to learn event connections during the training process. Subsequently, during the testing process, they utilized the structured variable to predict connections between unseen events. Wang et al. [42] extract relationships among events from an external event knowledge graph, formalize the script as a relational event chain, combine Transformer and Graph Neural Network(GNN) to learn embeddings of events encoding semantic and relational knowledge, and then utilize a relation Transformer to compute the likelihood of different candidate events.

<!-- Meanless: 3-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

### 2.2. Event schema induction

In the early stage of event schema induction, the focus is on identifying the triggering factors and participants of individual events, without considering the complex interactions among atomic events, which collectively form intricate schemas. Chambers et al. [43] seamlessly integrated co-reference chains into the learning process for generating event schemas. Cheung et al. [44] introduced latent themes encompassing frameworks, events, and participants, learning the transformations between frameworks and events to effectively elucidate textual content. Nguyen et al. [45] addressed entity disambiguation issues, providing a flexible framework for further expansion. On the other hand, Sha et al. [20] imposed constraints based on templates and slots within the same sentence. Huang et al. [46] employed a comprehensive approach by merging symbolic semantics and distributed semantics, facilitating the detection and representation of event structures. This was realized through a joint framework that extracted event types and argument roles while discovering event schemas. Yuan et al. [47] proposed a Bayesian non-parametric model to detect events and event types by leveraging slot expressions of entities mentioned in news articles. They further encoded insights using an unsupervised embedding model for schema induction. Subsequently, they extracted slot values for each event based on the expression schemas of slots, thus constructing event profiles. Huang and Ji [48] designed a semi-supervised vector quantized variational autoencoder framework to automatically learn discrete latent type representations for each visible and invisible type. They optimized it using annotations of visible type events, enabling the automatic discovery of a set of unseen types from a given corpus.

Recent work has primarily focused on inducing schemas for event pairs and multiple events. Li et al. [21] introduced a self-regressive language model trained on event-event paths, selecting significant and coherent paths to probabilistically construct these graph schemas. Li et al. [49] introduced the novel concept of "Temporal Complex Event Schema": a graph-based schema representation comprising events, arguments, temporal orders, and argument relationships. Additionally, they proposed a temporal event graph model capable of predicting event instances following temporal complex event schema. Jin et al. [50] proposed an event schema induction framework based on double graph autoencoders. The framework employs a high-level variational graph autoencoder to learn event skeletons and then utilizes a low-level GCN graph autoencoder to reconstruct entity-entity relationships. Dodr et al. [51] utilized large language models to generate source documents. Given advanced event definitions, they predicted specific events, arguments, and their relationships based on these source documents to construct a comprehensive schema describing complex events. Li et al. [52] considered event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). They designed an incremental prompting and validation method, INCSCHEMA, which decomposes the construction of a complex event graph into three stages: event skeleton construction, event expansion, and validation of event-event relationships.

### 2.3. Discussion on current works

In contrast to established event prediction methods, our newly introduced Pred-ID produces event evolutionary pattern at the schema-level. The predictive information at the schema-level will guide the potential evolution direction of the event graph and the types of future events. Contrasted with prevailing techniques for event pattern induction, Pred-ID not only inducts temporal dependencies among events from the original instance event graph but also deduces and expands potential future event types. For any given Instance Event Graph (nodes representing event triggers and event arguments, edges representing dependencies between event triggers, event arguments, and triggers-arguments), Pred-ID can reveal event evolution pattern through event-type graph induction. After that, the graph deduction will be conducted on the induced event evolutionary pattern as the prospect into future, serving for the following event prediction.

## 3. Details on proposed Pred-ID

The overview of our proposed Pred-ID, particularly at each time step ${T}_{i}$ ,is illustrated in Fig. 3. The various concepts depicted in Fig. 3 are defined as follows:

Definition 1 (Instance Event Graph ${G}_{T}^{I}$ ). An instance event graph ${G}_{T}^{I}$ consists of events $E$ and relations $\xi$ . Event is denoted as $E = \left( {{e}^{I},{a}^{I}}\right)$ , where ${e}^{I}$ denotes the event trigger and ${a}^{I}$ represents the event argument. The relation $\xi$ consists of three types: event-event temporal order ${\xi }_{\left( {e}_{i}^{I},{e}_{k}^{I}\right) }$ representing the occurrence of event trigger ${e}_{j}^{I}$ before event trigger ${e}_{k}^{I}$ ,event-argument edge ${\xi }_{\left( {e}_{i}^{I},{a}_{i}^{I}\right) }$ denoting the event trigger ${e}_{i}^{I}$ involves the argument roles ${a}_{j}^{I}$ ,and argument-argument relation edge ${\xi }_{\left( {a}_{i}^{I},r,{a}_{i}^{I}\right) }$ representing the argument roles ${a}_{i}^{I}$ and ${a}_{j}^{I}$ are associated by relationship type $\mathrm{r}$ . The nodes within the instance event graph ${G}_{{T}_{i}}^{I}$ are termed as "event triggers ${e}^{I}$ " and "event arguments ${a}^{I}$ ".

Definition 2 (Event-Type Schema). The event-type schema serves as a classification framework for event triggers. In the schema, the event triggers node in instance event graph ${G}_{{T}_{t}}^{I}$ are abstracted to the types of events and entities (e.g.,Terrorism $\rightarrow$ Attack). The event schema can transform the instance event graph to an event-type graph ${G}_{{T}_{i}}^{S}$ , which can be served for event prediction from event graph induction and deduction.

Definition 3 (Event-Type Graph ${G}_{{T}_{i}}^{S}$ ). The event-type graph ${G}_{{T}_{i}}^{S}$ is obtained through the transformation of the instance event graph ${G}_{{T}_{i}}^{I}$ by the event-type schema. Here,the event-type graph ${G}_{{T}_{i}}^{S}$ consists of event-type node ${e}^{S}$ and temporal orders ${\xi }_{\left( {e}_{i}^{S},{e}_{i}^{S}\right) }$ between events,excluding argument details. The event-type nodes within the event-type graph ${G}_{{T}_{i}}^{S}$ only represents generalized event type of event trigger through the event-type schema. The nodes within the event-type graph ${G}_{{T}_{i}}^{S}$ are termed as "event-type nodes ${e}^{S}$ ".

Definition 4 (Induced Event Graph ${G}_{{T}_{i}}^{SI}$ ). The induced event graph ${G}_{{T}_{i}}^{SI}$ , derived from the event-type graph ${G}_{{T}_{i}}^{S}$ ,preserves solely the event-type nodes ${e}^{S}$ obtained from the event-type graph ${G}_{{T}_{i}}^{S}$ while disregarding the initial temporal orders ${\xi }_{\left( {e}_{i}^{S},{e}_{k}^{S}\right) }$ . It explores the core development process by inducing temporal orders among events, introducing temporal orders ${\xi }_{\left( {e}_{i}^{SI},{e}_{k}^{SI}\right) }$ inferred from event-event successions. The nodes within the induced event graph ${G}_{{T}_{i}}^{SI}$ are termed as "event-type nodes ${e}^{SI}$ ".

Definition 5 (Event Evolutionary Pattern $\left. {G}_{{T}_{i}}^{SID}\right)$ . The event evolutionary pattern ${G}_{{T}_{i}}^{SID}$ represents the ultimate outcome of both inductive event graph generation and deductive event graph expansion. It begins with the induction of event types, followed by deduction processes that expand multiple event-type nodes ${e}^{S}$ and establish temporal orders ${\xi }_{\left( {e}_{j}^{SID},{e}_{k}^{SID}\right) }$ between events. The nodes within the event evolutionary pattern ${G}_{{T}_{i}}^{SID}$ are termed as "event-type nodes ${e}^{SID}$ ".

Definition 6 (Aggregated Event Graph ${G}_{{T}_{i}}^{Agg}$ ). The aggregated event graph ${G}_{{T}_{i}}^{Agg}$ is derived from the instance event graph ${G}_{{T}_{i}}^{I}$ by aggregating event argument information into event triggers ${e}^{I}$ ,discarding event argument ${a}^{I}$ ,and retaining only the event triggers ${e}^{I}$ . The nodes within the aggregated event graph ${G}_{{T}_{i}}^{Agg}$ are termed as "event nodes ${e}^{agg}$ ".

<!-- Meanless: 4-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

<!-- Media -->

<!-- figureText: The Graph Induction, Graph Deduction and Graph Node Prediction at each time step ② Inductive Event Graph Generation ③ Deductive Event Graph Expansion ${G}^{SP}$ Decoder Encoder Decoder Induced Event Graph ${\widehat{H}}^{Sl}$ Event Evolutionary Pattern ④ Graph Fusion ⑤ Event Prediction Candidate Events Event Evolutionary Pattern Event Contexts Fusion Graph Fusion Graph Generation Entity Relation Aggregated Event Graph New Instance Event Graph Input of next timestep T3 time step T2 time step T1 time step Deductive Event Graph Expan: Encoder Event-Type Graph ① Event Generalized - ④ ② Event Evolutionary Pattern Graph Fusion Inductive Event Event Type Graph Event Type Schema Event-Type Graph ① Input to nex Add Event to timestep Instance Instance Event Graph Instance Event Graph The instance event graph sequence across multiple time steps -->

<img src="https://cdn.noedgeai.com/bo_d2u6h13ef24c73b31eqg_4.jpg?x=206&y=150&w=1335&h=600&r=0"/>

Fig. 3. Overview on Pred-ID: The event prediction by event evolutionary pattern induction and deduction at each time step along the given instance event graph sequence.

<!-- Media -->

Definition 7 (Fusion Graph ${G}_{{T}_{i}}^{F}$ ). The fusion graph ${G}_{{T}_{i}}^{F}$ inherits the connectivity structure of the event evolutionary pattern ${G}_{{T}_{i}}^{SID}$ ,while its node features are obtained by aligning and merging the event evolutionary pattern ${G}_{{T}_{i}}^{SID}$ with the aggregated event graph ${G}_{{T}_{i}}^{Agg}$ . The nodes within the fusion graph ${G}_{{T}_{i}}^{F}$ are termed as "event nodes ${e}^{F}$ ".

Based on all mentioned above, in Fig. 3, given a sequence of event graphs $\left\{  {{G}_{{T}_{1}}^{I},{G}_{{T}_{2}}^{I},\ldots ,{G}_{{T}_{i}}^{I},\ldots ,{G}_{{T}_{n}}^{I}}\right\}$ ,an instance event graph ${G}_{{T}_{i}}^{I}$ at time step ${T}_{i}$ containing the event(both event trigger and event arguments) predicted at former ${T}_{i - 1}$ step will be first generalized into event-type graph ${G}_{{T}_{i}}^{S}$ through the pre-defined event-type schema in step 1 . Here, the event-type schema is a children-to-parent concept tree on all the involved event-types. Then,at time step ${T}_{i}$ ,an Inductive Event Graph Generation(step 2) is applied to the event-type graph ${G}_{{T}_{i}}^{S}$ ,depict the backbone of already-occurring event-type nodes, represented as the induced event graph ${G}_{{T}_{i}}^{SI}$ . Subsequently,a Deductive Event Graph Expansion(step 3) is performed on the induced event graph ${G}_{{T}_{i}}^{SI}$ ,adding potential future event-type nodes and establishing temporal orders with event-type nodes within the induced event graph ${G}_{{T}_{i}}^{SI}$ . This procedure yields the event evolutionary pattern ${G}_{{T}_{i}}^{SID}$ ,tracing the progression from past occurrences to potential future developments. Subsequently, to extract the existing event development process, event triggers, along with their temporal orders, are derived from the instance event graph. Aggregating event argument information into the event triggers to derive the aggregated event graph ${G}_{{T}_{i}}^{Agg}$ . Finally,the aggregated event graph ${G}_{{T}_{i}}^{Agg}$ (i.e.,containing existing event development process) and the event evolutionary pattern ${G}_{{T}_{i}}^{SID}$ (i.e.,containing prospect into near future) will be fused into fusion graph ${G}_{{T}_{i}}^{F}$ in step 4,based on which the most possible event (containing event trigger and event arguments) will be predicted in step 5 and added into instance event graph ${G}_{{T}_{i}}^{I}$ for next round of pattern induction,deduction and fusion at time step ${T}_{i + 1}$ . Here,the event predicted at time step ${T}_{i}$ will be added into instance event graph ${G}_{{T}_{i}}^{I}$ as the instance event graph ${G}_{{T}_{i + 1}}^{I}$ at time step ${T}_{i + 1}$ .

Finally, details on our proposed model Pred-ID will be elaborated through Inductive Event Graph Generation (step 2), Deductive Event Graph Expansion (step 3) and Graph Fusion (step 4) for Event Prediction (step 5). Due to the concise description of Event Generalized (Step 1), it is introduced at the start of Inductive Event Graph Generation (step 2).

### 3.1. Inductive event graph generation

To comprehend the event development process at the schema-level from the instance event graph ${G}^{I}$ ,the instance event graph ${G}^{I}$ will be first generalized into an event-type graph ${G}^{S}$ in the step 1 of Fig. 3 with the help of event type schema. This event-type graph ${G}^{S}$ comprises generalized event triggers(i.e., event-type nodes) derived through a pre-defined event-type schema. Then, as illustrated in Fig. 4, an encoding-decoding Inductive Event Graph Generation is applied to the event-type graph ${G}^{S}$ ,to comprehensively explore the core development process of the entire event-type graph ${G}^{S}$ . During the encoding phase (i.e., parts A and B), Pred-ID extracts the event core subgraph of the entire event-type graph ${G}^{S}$ ,revealing the propagation trends within each event-type node. In the decoding phase, as shown in part C of Fig. 4, Pred-ID introduces a Graph Adjacency Decoder (GAD) to estimate the distribution of core events. It enumerates all possible event-event pairs within the event-type graph ${G}^{S}$ (following temporal order) to generate the induced event graph ${G}^{SI}$ which can reveal the event evolution pattern. The generated induced event graph can reveal the implicit evolutionary patterns between events, so as to provide sufficient basis for the deductive event graph expansion.

First, in part A of Fig. 4, the extraction of the event core subgraph ${G}^{SP}$ involves identifying crucial event-type nodes within the event-type graph ${G}^{S}$ and scrutinizing the connections among them. Initially, the type representation ${h}_{i}^{S}$ of each event-type node in the event-type graph ${G}^{S}$ undergoes encoding by Graph Convolutional Networks (GCN) to yield the graph convolutional representation ${h}_{i}^{{S}^{\prime }}$ . To identify the core event-type nodes that influence the development of the event-type graph,the event-type graph ${G}^{S}$ is decomposed into $N$ local subgraphs ${c}_{h}\left( {e}_{i}^{S}\right)$ (for $\mathrm{i} = 1$ to $\mathrm{N}$ ). Here, $\mathrm{N}$ (or $\mathrm{n}$ ) denotes the count of event-type nodes within the event-type graph ${G}^{S}$ ,which is equivalent to the count of event triggers in the instance event graph ${G}^{I}$ . Each local subgraph ${c}_{h}\left( {e}_{i}^{S}\right)$ includes an event-type node ${e}_{i}^{S}$ ,termed the centroid of the local subgraph ${c}_{h}\left( {e}_{i}^{S}\right)$ ,along with its $h$ -hop neighboring event-type nodes. Next, as shown in Eq. (1), to model the impact of each event-type node on the overall representation of the local subgraph,a query vector ${m}_{i}$ is initially formulated to encompass all nodes within the local subgraph ${c}_{h}\left( {e}_{i}^{S}\right)$ . This query vector ${m}_{i}$ employs additive attention to examine all constituent event-type nodes ${e}_{i}^{S} \in  {c}_{h}\left( {e}_{i}^{S}\right)$ . This process yields the representation ${h}_{i}^{c}$ for each local subgraph ${c}_{h}\left( {e}_{i}^{S}\right)$ and an attention matrix $X$ used for Eq. (3),which ${X}_{i,j}$ denotes the impact weight of the $i$ th event-type node within the $j$ th local subgraph, $\left| {{c}_{h}\left( {e}_{i}^{S}\right) }\right|$ represents the count of nodes within the local subgraph ${c}_{h}\left( {e}_{i}^{S}\right) .{\overrightarrow{\omega }}^{T}$ and $W$ denote a learnable vector and matrix. The variable ${\alpha }_{i,j}$ signifies the attention weight allocated by the query vector ${m}_{i}$ to the event-type node ${e}_{j}^{S}$ .

<!-- Meanless: 5-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

$$
\left\{  \begin{array}{l} {h}_{i}^{c} = \mathop{\sum }\limits_{{j = 1}}^{\left| {c}_{h}\left( {e}_{i}^{S}\right) \right| }{\alpha }_{i,j}{h}_{j}^{{S}^{\prime }} \\  {\alpha }_{i,j} = \operatorname{softmax}\left( {{\overrightarrow{\omega }}^{T}\sigma \left( {W{m}_{i}\parallel {h}_{j}^{{S}^{\prime }}}\right) }\right) \\  {m}_{i} = \mathop{\max }\limits_{{{e}_{j}^{S} \in  {c}_{h}\left( {e}_{i}^{S}\right) }}\left( {h}_{j}^{{S}^{\prime }}\right) \\  {X}_{i,j} = {\alpha }_{i,j}{m}_{i} \end{array}\right.  \tag{1}
$$

<!-- Media -->

<!-- figureText: Inductive Graph Generation GAD (Graph Adjacency Decoder Induced Event Graph (B) Graph Propagation ✓ ${\widehat{H}}^{s}$ Input Graph © GAD (Graph Adjacency Decoder) Avg Concat ${G}^{SP}$ Max ${MLP}_{a}$ Cosine MLP Similarity Yes/No Extract Event Core Graph ${G}^{SP}$ Event-Type Graph Graph Propagation ${\widehat{H}}^{S}$ A Extract Event Core Graph GCN Decompose Input Graph Attention Attention Compute each representation Weight ${h}_{1}^{c},\ldots ,{h}_{n}^{c}$ ${X}^{S}\left( { : ,\widehat{i}}\right)$ . ${\phi }_{1},\ldots ,{\phi }_{n} \leftarrow   \odot$ ${\left( {\widehat{X}}^{S}\right) }^{T}{\widehat{A}}^{S}{\widehat{X}}^{S}$ ${\widehat{H}}_{c}^{S}\left( {\widehat{i}, : }\right)$ ${\widehat{H}}_{c}^{S}$ ${H}^{SP}$ Output Graph -->

<img src="https://cdn.noedgeai.com/bo_d2u6h13ef24c73b31eqg_5.jpg?x=206&y=152&w=1338&h=888&r=0"/>

Fig. 4. The structure of inductive event graph generation module.

<!-- Media -->

Moreover, as defined in Eq. (2), To calculate the influence of local subgraphs on adjacent ones,the representation ${h}_{i}^{c}$ explores event-type node ${e}_{i}^{S}$ within the local subgraph ${c}_{h}\left( {e}_{i}^{S}\right)$ ,observes the representation ${h}_{j}^{c}$ of the local subgraph ${c}_{h}\left( {e}_{j}^{S}\right)$ ,and computes the clustering score ${\phi }_{i}$ for ${c}_{h}\left( {e}_{i}^{S}\right)$ across adjacent local subgraphs. The entire clustering score represents as $\Phi  = {\left\lbrack  {\phi }_{1},{\phi }_{2},\ldots ,{\phi }_{N}\right\rbrack  }^{T}$ . where ${W}_{1},{W}_{2}$ and ${W}_{3}$ are learnable matrices, ${A}^{S}$ is the adjacency matrix of the event-type graph ${G}^{S}$ .

$$
{\phi }_{i} = \operatorname{ReLU}\left( {{h}_{i}^{c}{W}_{1} + \mathop{\sum }\limits_{{{e}_{j}^{S} \in  {c}_{h}\left( {e}_{i}^{S}\right) }}{A}_{i,j}^{s}\left( {{h}_{i}^{c}{W}_{2} - {h}_{j}^{c}{W}_{3}}\right) }\right)  \tag{2}
$$

The purpose of the Eq. (3) is to calculate the feature matrix ${H}^{SP}$ and adjacency matrix ${A}^{SP}$ for the event core subgraph ${G}^{SP}$ . The feature matrix ${H}^{SP}$ is derived from ${\widehat{H}}_{c}^{S}$ ,which is updated according to the clustering score $\Phi$ for ${H}_{c}^{S}$ ,wherein ${H}_{c}^{S} = \left\{  {{h}_{1}^{c},{h}_{2}^{c},\ldots ,{h}_{n}^{c}}\right\}$ . The indices $\widehat{i}$ of event-type nodes are determined based on the clustering scores $\Phi$ , selecting the top $\left\lbrack  {kN}\right\rbrack$ scores. Here, $k \in  (0,1\rbrack$ represents the ratio to be extracted from $N$ local subgraphs. The adjacency matrix ${A}^{SP}$ of the event core subgraph ${G}^{SP}$ is established through the integration of the adjacency matrix ${A}^{S}$ of the event-type graph ${G}^{S}$ and the attention matrix $X$ . The purpose of this approach is to model the interrelationships between event-type nodes within each local subgraph ${c}_{h}\left( {e}_{i}^{S}\right)$ (for $\mathrm{i} = 1$ to N),combining the connectivity within the event-type graph ${G}^{S}$ to construct the adjacency matrix ${A}^{SP}$ of the event core subgraph. where ${I}_{t}$ represents the identity matrix within the corresponding dimension, the feature matrix ${H}^{SP}$ and adjacency matrix ${A}^{SP}$ obtained form the event core subgraph ${G}^{SP}$ .

$$
\left\{  \begin{array}{l} {\widehat{H}}_{c}^{S} = \Phi  \odot  {H}_{c}^{S} \\  \widehat{i} = {\operatorname{TOP}}_{k}\left( {\Phi ,\lceil {kN}\rceil }\right) \\  {H}^{SP} = {\widehat{H}}_{c}^{S}\left( {\widehat{i}, : }\right)  \in  {R}^{\lceil {kN}\rceil  \times  d} \\  {A}^{SP} = {\left( \widehat{X}\right) }^{T}{\widehat{A}}^{S}\widehat{X} \\  \widehat{X} = X\left( { : ,\widehat{i}}\right) ,{\widehat{A}}^{S} = {A}^{S} + {I}_{t} \end{array}\right.  \tag{3}
$$

Second, graph propagated representations are obtained in part B of Fig. 4. As shown in Eq. (4), to capture event tendency, the representation of event will propagate along the structure of event-type graph ${G}^{S}$ . During each iteration,the event-type node ${e}_{i}^{S}$ ’s representation ${h}_{i}^{S}$ observes the preceding event-type node ${e}_{j}^{S}$ ’s representation ${h}_{j}^{S}$ , generating attention weight ${\alpha }_{ij}$ that quantifies the connection strength between ${e}_{i}^{S}$ and ${e}_{j}^{S}$ . This process induces trend information from the graph propagated representation ${\widehat{h}}_{j}^{S}$ of ${e}_{j}^{S}$ to derive the graph propagated representations ${\widehat{h}}_{i}^{S}$ of ${e}_{i}^{S}$ . Where, ${W}_{1}$ and ${W}_{2}$ represent learnable matrices.

$$
\left\{  \begin{array}{l} {\widehat{h}}_{i}^{S} = {h}_{i}^{S} + \mathop{\sum }\limits_{{\left( {{e}_{j},{e}_{i}}\right)  \in  {G}^{S}}}{\alpha }_{ij}{\widehat{h}}_{j}^{S} \\  {\alpha }_{ij} = \operatorname{softmax}\left( {{W}_{1}{h}_{i}^{S}{\left( {W}_{2}{h}_{j}^{S}\right) }^{T}}\right)  \end{array}\right.  \tag{4}
$$

Thirdly, in Fig. 4, Part C employs the Graph Adjacency Decoder (GAD) to generate the induced event graph ${G}^{SP}$ . Especially,the feature matrix ${H}^{SI}$ of the induced event graph ${G}^{SI}$ maintains consistency with the feature matrix ${H}^{S}$ of the event-type graph ${G}^{S}$ ,while the adjacency matrix ${A}^{SI}$ is computed based on following equations (5) to (7). As shown in Eq. (5), the Graph Adjacency Decoder (GAD) first computes the average representation $\frac{1}{n} \cdot  \mathop{\sum }\limits_{{i = 1}}^{\left| {G}^{SP}\right| }{h}_{i}^{SP}$ and maximum representation $\mathop{\max }\limits_{{i = 1}}^{\left| {G}^{SP}\right| }\left( {h}_{i}^{SP}\right)$ within the event core subgraph ${G}^{SP}$ . These are then concatenated to form the global representation ${h}_{G}^{SP}$ of the event core subgraph ${G}^{SP}$ . Where, ${h}_{i}^{SP}$ represents the $i$ th representation of feature matrix ${H}^{SP},\left| {G}^{SP}\right|$ indicates the number of event-type nodes within the event core subgraph ${G}^{SP}$ ,and $\parallel$ denotes the concatenation operation.

<!-- Meanless: 6-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

$$
{h}_{G}^{SP} = \left\lbrack  {\frac{1}{n} \cdot  \mathop{\sum }\limits_{{i = 1}}^{\left| {G}^{SP}\right| }{h}_{i}^{SP}\parallel \mathop{\max }\limits_{{i = 1}}^{\left| {G}^{SP}\right| }\left( {h}_{i}^{SP}\right) }\right\rbrack   \tag{5}
$$

Then in Eq. (6),the mean $\mu$ and covariance matrices $\sigma$ are derived by observing the global representation ${h}_{G}^{SP}$ through two MLP layers, yielding the event core distribution $N\left( {\mu ,\sigma }\right)$ [50]. Where,event core representation ${h}_{\varphi }^{SP} \sim  N\left( {\mu ,\sigma }\right)$ signifies the vector representation for the event core distribution $N\left( {\mu ,\sigma }\right)$ .

$$
\left\{  \begin{array}{l} {h}_{\varphi }^{SP} \in  {R}^{d} = \operatorname{reshape}\left( {N\left( {\mu ,\sigma }\right) }\right) \\  \mu  = {ML}{P}_{\mu }\left( {h}_{G}^{SP}\right)  \in  {R}^{d} \\  \sigma  = {ML}{P}_{\sigma }\left( {h}_{G}^{SP}\right)  \in  {R}^{d} \end{array}\right.  \tag{6}
$$

As depicted in Eq. (7), leveraging the event core representation ${h}_{\varphi }^{SP}$ for the event core distribution $N\left( {\mu ,\sigma }\right)$ ,event-event pairs are enumerated within the event-type nodes of the induced event graph ${G}^{SI}$ . This process involves combining event-type nodes within the induced event graph ${G}^{SI}$ to generate multiple event-event pairs $\left( {{e}_{i}^{SI},{e}_{j}^{SI}}\right)$ . It is worth noting that event-event pairs $\left( {{e}_{i}^{SI},{e}_{i}^{SI}}\right)$ where ${e}_{i}^{SI}$ occurs after ${e}_{j}^{SI}$ are not considered. Each event-event pair $\left( {{e}_{i}^{SI},{e}_{j}^{SI}}\right)$ is represented by concatenating the respective graph propagated representations on ${\widehat{h}}_{i}^{S}$ and ${\widehat{h}}_{i}^{S}$ . These concatenated representations are then processed through ${ML}{P}_{m}$ layer to derive an aggregated representation ${h}_{i,j}^{m}$ for each $\left( {{e}_{i}^{SI},{e}_{j}^{SI}}\right)$ pair. Following this,the similarity between the aggregated representation ${h}_{i,j}^{m}$ and the event core distribution representation ${h}_{\varphi }^{SP} \sim$ $N\left( {\mu ,\sigma }\right)$ is evaluated. This evaluation aims to ascertain the probability ${A}_{i,j}^{SI}$ of the existence of temporal order ${\xi }_{\left( {e}_{i}^{SI},{e}_{i}^{SI}\right) }$ ,thereby uncovering the event development process derived from the entire instance event graph ${G}_{{T}_{i}}^{I}$ . Here,if the probability ${A}_{i,j}^{SI}$ of the temporal order ${\xi }_{\left( {e}_{i}^{SI},{e}_{i}^{SI}\right) }$ exceeds the hyperparameter $p$ ,the temporal order ${\xi }_{\left( {e}_{i}^{SI},{e}_{i}^{SI}\right) }$ will be added. The hyperparameter $p$ will be discussed in the ablation study, where its relevance and impact will be explored. The above process leads to the generation of the adjacency matrix ${A}^{SI}$ for the induced event graph ${G}^{SI}$ . And the induced event graph ${G}^{SI}$ is computed from the feature matrix ${H}^{SI}$ and the adjacency matrix ${A}^{SI}$ .

$$
\left\{  \begin{array}{l} {h}_{i,j}^{m} = {ML}{P}_{m}\left( {{\widehat{h}}_{i}^{S}\parallel {\widehat{h}}_{j}^{S}}\right) \\  {A}_{i,j}^{SI} = \frac{1}{2} \cdot  \left( {\frac{{h}_{i,j}^{m} \cdot  {h}_{\varphi }^{SP}}{\begin{Vmatrix}{h}_{i,j}^{m}\end{Vmatrix}\begin{Vmatrix}{h}_{\varphi }^{SP}\end{Vmatrix}} + 1}\right)  \in  \left( {0,1}\right)  \end{array}\right.  \tag{7}
$$

Overall, the Inductive Event Graph Generation adheres to an encoding-decoding paradigm as outlined from Eq. (1) to (7). Initially, the instance event graph ${G}^{I}$ undergoes a generalization process into an event-type graph ${G}^{S}$ ,comprising generalized event triggers (i.e.,event-type nodes) based on a pre-defined event-type schema. During the encoding phase (refer to Eqs. (1) to (4),the event-type graph ${G}^{S}$ is partitioned into $N$ local subgraphs ${c}_{h}\left( {e}_{i}^{S}\right)$ ,computing clustering scores ${\phi }_{i}$ (for $\mathrm{i} = 1$ to $\mathrm{N}$ ). These top-scored subgraphs collectively constitute the event core subgraph ${G}^{SP}$ ,comprising multiple central event-type nodes from the highest-scoring subgraphs(refer to Eqs. (1) to (3). Simultaneously, graph propagation occurs within the event-type graph ${G}^{S}$ ,unveiling tendencies in event development and yielding graph propagated representations ${\widehat{h}}_{i}^{S}$ for each event-type node(refer to Eq. (4)). During the decoding phase,the node feature matrix ${H}^{SI}$ in the induced event graph ${G}^{SI}$ maintains consistency with the node feature matrix ${H}^{S} = \left\{  {{h}_{1}^{S},{h}_{2}^{S},\ldots ,{h}_{n}^{S}}\right\}$ of the event-type graph ${G}^{S}$ . Subsequently,the event core distribution $N\left( {\mu ,\sigma }\right)$ is derive from the event core subgraph ${G}^{SP}$ . Leveraging this event core distribution $N\left( {\mu ,\sigma }\right)$ and graph propagated representations ${\widehat{h}}_{i}^{S}$ for each event-type node,we enumerate all potential event-event pairs $\left( {{e}_{i}^{SI},{e}_{j}^{SI}}\right)$ constituted by the induced event graph's nodes. This enumeration aids in constructing the adjacency matrix ${A}^{SI}$ for the induced event graph. Then an induced event graph ${G}^{SI}$ has been constructed to reflect the event development process entailed in the given instance event graph. Finally, the loss function for Inductive Event Graph Generation (refer to Eq. (8)) seeks to align the developmental trend of core events depicted in the generated induced event graph with that of the ground-truth induced event graph. This alignment assists in approximating the developmental schema of ground-truth induced event graph, facilitating subsequent Deductive Event Graph Expansion. In Eq. (8), ${y}_{i,j}^{\text{ref }}$ represents edge connectivity in the ground-truth induced event graph,while ${A}_{i,j}^{SI}$ signifies the predicted probability of temporal order ${\xi }_{\left( {e}_{i}^{SI},{e}_{j}^{SI}\right) }$ existence in the induced event graph ${G}^{SI}$ .

$$
{L}_{\text{gen }} =  - \mathop{\sum }\limits_{{{e}_{i}^{SI} \in  {G}^{SI}}}\mathop{\sum }\limits_{{{e}_{j}^{SI} \in  {G}^{SI}}}\left( {{y}_{i,j}^{\text{ref }}\log \left( {A}_{i,j}^{SI}\right)  + \left( {1 - {y}_{i,j}^{\text{ref }}}\right) \log \left( {1 - {A}_{i,j}^{SI}}\right) }\right)  \tag{8}
$$

### 3.2. Deductive event graph expansion

As shown in Fig. 5, in order to predict the direction of future events, the proposed Pred-ID can extend the induced event graph ${G}^{SI}$ by extracting the event core subgraph (including various potential event types). This extension aims to explore the trend of event evolution through the results of inductive event graph generation and deductive event graph expansion, so as to provide direction for event prediction.

First, following part A of Fig. 5, event core representation is acquired within the induced event graph ${G}^{SI}$ as defined from Eqs. (1) to (6). Eq. (1) decomposes the induced event graph ${G}^{SI}$ into $N$ local subgraphs, computing the representation for each local subgraph. Here, the number of event-type nodes ${e}^{SI}$ in the induced event graph ${G}^{SI}$ is equal to the number(N)of event-type nodes ${e}^{S}$ in the event-type graph ${G}^{S}$ . Subsequently,according to Eq. (2),the clustering score ${\phi }_{i}$ for each local subgraph is calculated. Event-type nodes within the event core subgraph are selected based on these clustering scores. Following this selection,Eq. (3) is utilized to compute the adjacency matrix ${A}^{SIP}$ for the event core subgraph ${G}^{SIP}$ . Afterward,the computation of the global representation for the event core subgraph ${G}^{SIP}$ is executed using Eq. (4). Subsequently, Eq. (5) is applied to derive the event core distribution $N\left( {\mu ,\sigma }\right)$ ,from which the vector representation ${h}_{\varphi }^{SIP}$ is obtained.

Second, the graph propagated representations are computed in part B of Fig. 5. As shown in Eq. (9), the principle of graph propagation is kept consistent with that in Inductive Event Graph Generation, where the original event-type node representations in induced event graph ${G}^{SI}$ propagate along the graph structure of ${G}^{SI}$ ,getting the graph propagated representations as ${\widehat{h}}_{i}^{SI}$ .

$$
\left\{  \begin{array}{l} {\widehat{h}}_{i}^{SI} = {h}_{i}^{SI} + \mathop{\sum }\limits_{{\left( {{e}_{j},{e}_{i}}\right)  \in  {G}^{SI}}}{\alpha }_{ij}{\widehat{h}}_{j}^{SI} \\  {\alpha }_{ij} = \operatorname{softmax}\left( {{W}_{1}{h}_{i}^{SI}{\left( {W}_{2}{h}_{j}^{SI}\right) }^{T}}\right)  \end{array}\right.  \tag{9}
$$

Third,in part C of Fig. 5, GAD expands the induced event graph ${G}^{SI}$ to deductive expansion,where the combination of induction and deduction forms the event evolutionary pattern ${G}^{SID}$ . To enable the deductive expansion for prospective event types, $\mathrm{M}$ additional event types are introduced into the induced event graph ${G}^{SI}$ . Pred-ID integrates the complete event type library to comprehensively augment the event evolutionary pattern ${G}^{SID}$ ,incorporating essential event types for robust deduction. M represents the total event type count of the entire event type library. Here, ${e}_{i}^{SI}\left( {i \in  \left\lbrack  {1,2,\ldots ,N}\right\rbrack  }\right)$ signifies the event-type nodes within the induced event graph ${G}^{SI}$ ,while ${e}_{j}^{SI}(j \in  \lbrack N +$ $1,\ldots ,N + M\rbrack )$ represents the newly deduced event-type nodes. The event-type representation ${h}_{i}^{SID}$ for the deduced event-type nodes is conducted utilizing the pre-trained BERT [32]. Then, the feature matrix ${H}^{SID}$ characterizing the event evolutionary pattern ${G}^{SID}$ is expressed by amalgamating the event-type representations ${h}_{i}^{SI}$ of nodes ${e}_{i}^{SI}$ within the induced event graph ${G}^{SI}$ and the event-type representations ${h}_{j}^{SID}$ of the newly deduced event-type nodes ${e}_{i}^{SID}$ . As shown in Eq. (10), Pred-ID utilizes GAD to enumerate event pairs composed of ${e}_{i}^{SI}$ and ${e}_{j}^{SID}$ ,merge event-type node representations from these pairs,resulting in an aggregated representation ${h}_{i,j}^{m}$ . Following this,the probability of temporal order existing in ${e}_{i}^{SI}$ and ${e}_{j}^{SI}$ is deduced through cosine similarity. Similar to Inductive Event Graph Generation, the addition of temporal orders depends on the hyperparameter $p$ . Finally,this process combines induction and deduction, expanding potential event types into the future based on the induced event graph ${G}^{SI}$ . It expands the developmental paths within the induced event graph ${G}^{SI}$ to derive the adjacency matrix ${A}^{SID}$ of the event evolutionary pattern ${G}^{SID}$ . It is worth noting that after adding temporal orders, if the newly deduced node remains isolated, it is considered unrelated to the current trend and will be removed.

<!-- Meanless: 7-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

$$
\left\{  \begin{array}{l} {h}_{i,j}^{m} = {ML}{P}_{m}\left( {{\widehat{h}}_{i}^{SI}\parallel {h}_{j}^{SID}}\right) \\  {A}_{i,j}^{SID} = \frac{1}{2} \cdot  \left( {\frac{{h}_{i,j}^{m} \cdot  {h}_{\varphi }^{SP}}{\begin{Vmatrix}{h}_{i,j}^{m}\end{Vmatrix}\begin{Vmatrix}{h}_{\varphi }^{SP}\end{Vmatrix}} + 1}\right)  \in  \left( {0,1}\right)  \end{array}\right.  \tag{10}
$$

<!-- Media -->

<!-- figureText: Deductive Graph Expansion GAD (Graph Adjacency Decoder) No Next? Deducion Result Extract Event Core Graph C Induced Event Graph Graph >>b1 Propagation ${\widehat{H}}^{SI}$ -->

<img src="https://cdn.noedgeai.com/bo_d2u6h13ef24c73b31eqg_7.jpg?x=209&y=154&w=1328&h=344&r=0"/>

Fig. 5. The structure of deductive event graph expansion module.

<!-- Media -->

If the newly deduced event-type nodes establish temporal orders with the induced event graph, these orders are considered expansions of the event development process, necessitating further Deductive Event Graph Expansion to adequately expand future pathways. The newly deduced event-type nodes need to obtain graph propagated representations using Eq. (9), capturing the developmental trend of propagating induced event graph ${G}^{SI}$ for the next iteration of Deductive Event Graph Expansion. The expansion process concludes under two conditions: either when the newly deduced event-type nodes are all isolated, or when the number of event-type nodes in the event evolutionary pattern ${G}^{SID}$ surpasses the upper limit set by the hyperparameter $\varepsilon$ .

As a result,the event evolutionary pattern ${G}^{SID}$ has been finalized, incorporating both Inductive Event Graph Generation and Deductive Event Graph Expansion. The Inductive Event Graph Generation comprehensively investigates the core development process. Meanwhile, the graph deductive expansion module, building upon the inductive generation, further expands the core development process. It introduces prospective future event-type nodes within these core development processes, providing directional guidance at the schema-level for event prediction. This enhancement enables the event evolutionary pattern ${G}^{SID}$ to offer robust assistance for subsequent event prediction.

### 3.3. Graph fusion for event prediction

As shown in Fig. 6,event evolutionary pattern ${G}^{SID}$ can guide the prediction of the future of the instance event graph through the prospective evolution of the future, so as to improve the accuracy of the event prediction. First,the instance event graph ${G}^{I}$ creates the aggregate event graph ${G}^{Agg}$ by aggregating event triggers and their event parameters. The alignment between aggregate event graph and event evolution pattern ${G}^{SID}$ can enhance the relevance of the evolution pattern to the instance graph. After that, we use the event evolutionary pattern ${G}^{SID}$ as a base,it merges the corresponding event node representations from the aggregated event graph ${G}^{Agg}$ ,forming a fusion graph ${G}^{F}$ that amalgamates instance-level and schema-level information. Next, Pred-ID establishes the most relevant context representation for candidate events, comparing them to select the best event. This best event is added to the instance event graph ${G}^{I}$ for generating temporal orders and argument-argument edges, constructing new instance event graph ${G}_{{T}_{i + 1}}^{I}$ as Pred-ID’s input at time step ${T}_{i + 1}$ .

Firstly, aggregate the argument information in the instance event graph ${G}^{I}$ and align it with the event evolutionary pattern ${G}^{SID}$ . As shown in Eq. (11),the event triggers ${E}^{I} = \left\{  {{e}_{1}^{I},\ldots ,{e}_{n}^{I}}\right\}$ in the instance event graph ${G}^{I}$ aggregate their associated event arguments $\left\{  {{h}_{{a}_{1}},{h}_{{a}_{2}},\ldots }\right\}$ into the event trigger representation ${H}^{I} = \left\{  {{h}_{{e}_{1}}^{I},\ldots ,{h}_{{e}_{n}}^{I}}\right\}$ through an Attention mechanism, obtaining an aggregated representation ${H}^{Agg} = \left\{  {{h}_{1}^{Agg},\ldots ,{h}_{n}^{Agg}}\right\}$ ,which form the aggregated event graph ${G}^{Agg}$ . Subsequently,the event evolutionary pattern ${G}^{SID}$ aligns with the aggregated event graph ${G}^{Agg}$ ,aiming for each event-type node in the event evolutionary pattern ${G}^{SID}$ to find a corresponding event node in the aggregated event graph ${G}^{Agg}$ . To address the issue of deductive expansion event-type nodes in the event evolutionary pattern ${G}^{SID}$ lacking corresponding event nodes in the aggregated event graph ${G}^{Agg}$ , virtual event nodes will be added to the aggregated event graph ${G}^{Agg}$ to meet the alignment requirements.

$$
\left\{  \begin{array}{l} \mathrm{Q} = {h}_{{e}_{i}}^{I},K = \left\{  {{h}_{{a}_{1}},{h}_{{a}_{2}},\ldots }\right\}  ,V = \left\{  {{h}_{{a}_{1}},{h}_{{a}_{2}},\ldots }\right\}  \\  {h}_{i}^{Agg} = {h}_{{e}_{i}}^{I} + \operatorname{Softmax}\left( {Q{K}^{T}}\right) V \end{array}\right.  \tag{11}
$$

Secondly,fuse the event evolutionary pattern ${G}^{SID}$ with the aggregated event graph ${G}^{I}$ to create fusion graph ${G}^{F}$ . As described in Eq. (12),using the event evolutionary pattern ${G}^{SID}$ as a base,align and merge it with the aggregated event graph ${G}^{Agg}$ . An event-type node ${e}_{i}^{SID}$ in the event evolutionary pattern ${G}^{SID}$ finds its corresponding event node ${e}_{i}^{Agg}$ in the aggregated event graph ${G}^{Agg}$ . Through a ${ML}{P}_{f}$ , event node ${e}_{i}^{Agg}$ ’s aggregated representation ${h}_{i}^{I}$ and event-type node ${e}_{i}^{SID}$ ’s type representation ${h}_{i}^{SID}$ are aggregated,combining information from both instance-level and schema-level, obtaining the fusion representation ${h}_{i}^{F}$ ,thereby constructing the fusion graph ${G}^{F}$ . To capture the local neighborhood information of event nodes, GCN is used to update event node representations in the fusion graph ${G}^{F}$ .

$$
{h}_{i}^{F} = {ML}{P}_{f}\left( {{h}_{i}^{SID}\parallel {h}_{i}^{I}}\right)  \tag{12}
$$

Thirdly, construct the most relevant context representation and select the best candidate event from $\lambda$ candidate events ${E}^{\delta }$ (each containing event trigger ${e}^{\delta }$ and associated event arguments ${a}^{\delta }$ ) based on the context representation. The representations ${H}^{\delta } = \left\{  {{h}_{1}^{\delta },\ldots ,{h}_{\lambda }^{\delta }}\right\}$ of candidate events ${E}^{\delta }$ are derived by aggregating event argument information through Eq. (11). Then, as demonstrated in Eq. (13), within the fusion graph ${G}^{F}$ that has captured event node neighborhood information,the aggregated representation ${h}_{k}^{\delta }$ of each candidate event ${e}^{\delta }$ observes the representations ${H}^{F} = \left\{  {{h}_{1}^{F},\ldots ,{h}_{\left| {G}^{F}\right| }^{F}}\right\}$ of event nodes in the fusion graph ${G}^{F}$ using an Attention mechanism,aggregating relevant event information to construct the most relevant context representation ${C}^{\delta } = \left\{  {{c}_{1}^{\delta },{c}_{2}^{\delta },\ldots ,{c}_{\lambda }^{\delta }}\right\}$ . Then,by computing the fitness score ${S}^{\delta } = \left\{  {{s}_{1}^{\delta },{s}_{2}^{\delta },\ldots ,{s}_{\lambda }^{\delta }}\right\}$ between the aggregated representation ${h}_{k}^{\delta }$ of the candidate event and the most relevant context representation ${c}_{k}^{\delta }$ using cosine similarity, the candidate event with the highest fitness score is added to the instance event graph ${G}^{I}$ as the best event.

<!-- Meanless: 8-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

$$
\left\{  \begin{array}{l} Q = {H}^{\delta },K = {H}^{F},V = {H}^{F} \\  {C}^{\delta } = \operatorname{softmax}\left( {Q{K}^{T}}\right) V \\  {e}_{i}^{\delta } = {\operatorname{argmax}}_{\lambda }\left( {S}^{\delta }\right) \\  {s}_{k}^{\delta } = \frac{1}{2} \cdot  \left( {\frac{{h}_{k}^{\delta } \cdot  {c}_{k}^{\delta }}{\begin{Vmatrix}{h}_{k}^{\delta }\end{Vmatrix}\begin{Vmatrix}{c}_{k}^{\delta }\end{Vmatrix}} + 1}\right)  \in  \left( {0,1}\right)  \end{array}\right.  \tag{13}
$$

<!-- Media -->

<!-- figureText: Graph Fusion Event Prediction Aggregate ${h}_{2}^{\delta }$ Arguments Candidate Events Q J Attention Event Contexts Cosine Similarity Fusion Graph Temporal Order Generation Entity Relation Generation Event Evolutionary Pattern Fusion Graph Aggregated Event Graph Aggregate Arguments Instance Event Graph -->

<img src="https://cdn.noedgeai.com/bo_d2u6h13ef24c73b31eqg_8.jpg?x=404&y=148&w=942&h=810&r=0"/>

Fig. 6. The structure of graph fusion for event prediction module.

<!-- Media -->

Lastly,construct temporal orders ${\xi }_{\left( {e}^{I},{e}^{\delta }\right) }$ and argument-argument edges ${\xi }_{\left( {a}^{I},r,{a}^{\delta }\right) }$ to obtain a new instance event graph ${G}_{{T}_{i + 1}}^{I}$ . As outlined in Eq. (14), after adding the best event to the instance event graph ${G}^{I}$ ,to establish connections between the best event and the instance event graph ${G}^{I}$ ,temporal orders ${\xi }_{\left( {e}^{I},{e}^{\delta }\right) }$ and argument-argument edges ${\xi }_{\left( {a}^{I},r,{a}^{\delta }\right) }$ will be generated. For generating temporal orders ${\xi }_{\left( {e}^{I},{e}^{\delta }\right) }$ ,a mixed Bernoulli distribution is used to describe the temporal order connection between two events,where $B$ is the number of mixture components. MLP calculates the parameter vectors $\gamma$ for each relationship between events, representing their features. Multiple possible connection scenarios are considered through different probability distributions $\theta$ within the mixed Bernoulli distribution,combining these distributions with certain posterior probabilities ${\widehat{p}}_{\left( {e}_{i}^{I},{e}_{i}^{\delta }\right) }$ to reflect the likelihood of temporal order connection. For generating argument-argument edge ${\xi }_{\left( {a}_{i}^{I},r,{a}_{i}^{\delta }\right) }$ ,the probability ${\widehat{p}}_{\left( {a}_{j},r,{a}_{k}\right) }$ of argument-argument edge ${\xi }_{\left( {a}_{i}^{I},r,{a}_{i}^{\delta }\right) }$ between the argument ${a}_{j}^{\delta }$ of the best event and all arguments ${a}^{I}$ in the instance event graph ${G}^{I}$ is modeled as a classification distribution of relationship types in the event-type schema,where $\mathrm{R}$ represents all relation types of argument-argument edge, O represents no relationship between two arguments, and CO indicates that two arguments are the same. The new instance event graph ${G}^{I}$ ,incorporating temporal orders ${\xi }_{\left( {e}^{I},{e}^{\delta }\right) }$ and argument relationships ${\xi }_{\left( {a}^{I},r,{a}^{\delta }\right) }$ ,will serve as the input ${G}_{{T}_{i + 1}}^{I}$

for Pred-ID in the next time step.

$$
\left\{  \begin{array}{l} {\widehat{p}}_{\left( {e}_{j}^{I},{e}_{i}^{\delta }\right) } = \mathop{\sum }\limits_{b}{\gamma }_{b}{\theta }_{b,j,i} \\  {\gamma }_{1},\cdots ,{\gamma }_{B} = \operatorname{Softmax}\left( {\mathop{\sum }\limits_{{i,j}}\operatorname{MLP}\left( {{h}_{j}^{I} - {h}_{i}^{\delta }}\right) }\right) \\  {\theta }_{1,j,i},\cdots ,{\theta }_{B,j,i} = \operatorname{ReLu}\left( {{\operatorname{MLP}}_{\theta }\left( {{h}_{j}^{I} - {h}_{i}^{\delta }}\right) }\right) \\  {\widehat{p}}_{\left( {e}_{i}^{I},r,{e}_{j}^{\delta }\right) } = \frac{\exp \left( {{\operatorname{MLP}}_{r}\left( {{h}_{i}^{I} - {h}_{i}^{\delta }}\right) }\right) }{\mathop{\sum }\limits_{{{e}^{\prime } = {e}_{i} \cup  C,C}}\exp \left( {{\operatorname{MLP}}_{r}\left( {{h}_{i}^{I} - {h}_{i}^{\delta }}\right) }\right) } \end{array}\right.  \tag{14}
$$

In Eq. (15), the loss function for event prediction aims to maximize the gap between real and non-real candidate events, adjusting model parameters to maximize the fitness score corresponding to the real candidate event and consequently enhance the model's ability to predict events. Here, $y$ denotes the real event,margin is the hyperparameter controlling the gap between the score of the real event and non-real candidate event.

$$
{L}_{\text{pred }} = \mathop{\sum }\limits_{{j = 1}}^{\lambda }\left( {\max \left( {0,\text{ margin } - {s}_{y}^{\delta } + {\widehat{s}}_{j}}\right) }\right)  \tag{15}
$$

As shown in Eq. (16), the first half of the loss function optimizes the generation of temporal orders ${\xi }_{\left( {e}^{I},{e}^{\delta }\right) }$ by minimizing cross-entropy. Similarly, the second half optimizes the generation of argument-argument edges ${\xi }_{\left( {a}^{I},r,{a}^{\delta }\right) }$ through cross-entropy minimization. Where ${p}_{\left( {e}_{j}^{I},{e}_{i}^{\delta }\right) }^{ref}$ and ${p}_{\left( {a}_{i}^{I},r,{a}_{i}^{\delta }\right) }^{ref}$ denote the ground-truth labels for ${\xi }_{\left( {e}_{j}^{I},{e}_{i}^{\delta }\right) }$ and ${\xi }_{\left( {a}_{i}^{I},r,{a}_{j}^{\delta }\right) }$ .

$$
{L}_{\text{edge }} =  - {p}_{\left( {e}_{j}^{I},{e}_{i}^{\delta }\right) }^{ref}\log \left( {\widehat{p}}_{\left( {e}_{j}^{I},{e}_{i}^{\delta }\right) }\right)  + \left( {1 - {p}_{\left( {e}_{j}^{I},{e}_{i}^{\delta }\right) }^{ref}}\right) \log \left( {1 - {\widehat{p}}_{\left( {e}_{j}^{I},{e}_{i}^{\delta }\right) }}\right)  \tag{16}
$$

$$
 - {p}_{\left( {a}_{i}^{I},r,{a}_{j}^{\delta }\right) }^{ref}\log \left( {\widehat{p}}_{\left( {a}_{i}^{I},r,{a}_{j}^{\delta }\right) }\right)  + \left( {1 - {p}_{\left( {a}_{i}^{I},r,{a}_{j}^{\delta }\right) }^{ref}}\right) \log \left( {1 - {\widehat{p}}_{\left( {a}_{i}^{I},r,{a}_{j}^{\delta }\right) }}\right) 
$$

The overall loss function, as shown in Eq. (17), operates in various stages within the model. Firstly, during the Inductive Event Graph Generation,the loss function ${L}_{\text{gen }}$ (Eq. (8)) aims to align the event development process in the induced event graph ${G}^{SI}$ with the provided induced event graph by dataset. This alignment serves as a foundation for subsequent Deductive Event Graph Expansion. Then, the Deductive Event Graph Expansion utilizes the induction results, delving deeper into the core developmental process, and incorporating potential future event types to provide schema-level guidance for event prediction. Subsequently, in the event prediction phase, the loss function ${L}_{\text{pred }}$ (Eq. (15)) measures the fitness score ${S}^{\delta }$ between the aggregated representation ${H}^{\delta }$ of candidate events ${E}^{\delta }$ and the most relevant context representation ${C}^{\delta }$ . This score ${S}^{\delta }$ reflects the similarity between candidate events and their context, adjusting model parameters to maximize the fitness score of real event, thus enhancing the capability for event prediction. These prediction outcomes impact the training during the Deductive Event Graph Expansion, collectively contributing to accurate schema-level predictions. Finally, during the generation of temporal orders ${\xi }_{\left( {e}^{I},{e}^{\delta }\right) }$ and argument-argument edges ${\xi }_{\left( {a}^{I},r,{a}^{\delta }\right) }$ ,the loss function ${L}_{\text{edge }}$ (Eq. (16)) evaluates the accuracy of the model in generating connections between the best event and the instance event graph ${G}_{{T}_{i}}^{I}$ . Throughout the optimization process,event prediction results guide Deductive Event Graph Expansion, subsequently influencing Inductive Event Graph Generation, forming a closed-loop cyclic optimization mechanism.

<!-- Meanless: 9-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

$$
\left\{  \begin{array}{l} {L}_{\text{all }} = {\eta }_{1}{L}_{\text{gen }} + {\eta }_{2}{L}_{\text{pred }} + {\eta }_{3}{L}_{\text{edge }} \\  {\eta }_{1},{\eta }_{2},{\eta }_{3} \in  \left( {0,1}\right)  \end{array}\right.  \tag{17}
$$

## 4. Analysis of experimental results

In this section, we conducted comprehensive experiments using benchmark datasets and compared our proposed model Pred-ID with state-of-the-art models to demonstrate its effectiveness. Before delving into the discussion of experimental results, we will provide a brief overview of the general settings.

### 4.1. Experimental setting

We will introduce the data set used in the experiment in detail, and list the comparison methods needed to prove the validity of the proposed model. Finally, in order to ensure the reproducibility of the experiment, we explain the experimental evaluation indexes and model parameters in detail.

#### 4.1.1. Data preparation

The datasets we utilized are based on Li [49] and primarily consist of two corpora: the General Schema Learning Corpus and the Improvised Explosive Device (IED) Schema Learning Corpus. The IED Schema Learning Corpus is further divided into four categories: General IED, Drone Strikes IED, Car-bombing IED, and Suicide IED. However, due to insufficient data volume for Drone Strikes IED, this category has been excluded from consideration. Since our task involves both event prediction and event schema induction, the original datasets do not entirely fulfill our requirements. Therefore, we implemented the following enhancements to the original datasets:

As shown in Fig. 7, we divide the original instance event graph G into two parts, denoted as G1 and G2, following a certain ratio, for example, $G \rightarrow  {G1}\left( {1 - {12}}\right)  + {G2}\left( {{13} - {24}}\right)$ . Subsequently, ${G1}$ is used as the input instance event graph, and the ground truth candidate event is selected from G2. The chosen event is the first node in G2 that has a temporal order with G1, along with its arguments, such as event trigger 13 and its event arguments. Disturbance options for candidate events are randomly selected from the entire dataset. Typically, there are five candidate events for the event prediction task [28,35,39,41,42]. To further enhance the model's potential, we expanded the number of candidate events from 5 to 10 . The ground truth temporal orders are connected to node 13 in the instance event graph, while the ground truth argument-argument edges are those connecting to the event arguments of node 13. To increase the diversity of training samples, we introduced various splitting strategies, especially for larger instance event graphs. Multiple cuts can be applied to larger instance event graphs to ensure comprehensive coverage of variations in instance event graph sizes. This strategy allows us to flexibly handle instance event graphs of different sizes, enhancing training diversity and generalization. Table 1 summarizes the statistical data for our improved datasets.

When conducting multi-step predictions, we progressively adjust the combination of G1 and G2. We incrementally increase the number of nodes in G1 and decrease the number of nodes in G2 until reaching the 10th step, for example, G1(1-21) + G2(22-24). This multi-step prediction setting aims to explore the model's performance on complex sequence prediction tasks under gradually changing input conditions.

To induce event pattern at the schema-level, we employed the event-type schema from the DARPA KAIROS ontology ${}^{1}$ during the generalization of instance event graph. The DARPA KAIROS ontology is a fine-grained ontology used for schema learning. Generalization involves utilizing OneIE (a information extraction system) to identify the most corresponding event types from the DARPA KAIROS ontology based on specific event triggers. Furthermore, even though Li [49] manually created corresponding ground-truth schema for each IED type using the schema management tool [53], there are still differences between instance event graphs. Therefore, we also use the schema management tool to create corresponding ground-truth induced event graph for each instance event graph, aiming to guide the effectiveness of Inductive Event Graph Generation. The ultimately generated event evolutionary pattern is still compared with the ground-truth schema.

#### 4.1.2. Baselines

We consider the baseline model from the perspective of event prediction and event evolutionary pattern generation. The baseline models are as follows:

- TEGM [49] ${}^{2}$ (Temporal Event Graph Model): TEGM generates event schema graphs iteratively in an autoregressive manner. It gradually generates events and edges between newly generated events and existing events. Greedy decoding is then employed to obtain an event schema graph.

- DGCRN [54] ${}^{3}$ : DGCRN extracts dynamic features with time steps from predefined graph node attributes, and designs filtering parameters to affect the changes of dynamic features of each node to construct an event schema graph.

- DASTGN [55]: DASTGN uses predefined adjacency and node features to construct temporal and spatial features of data flow, and realizes spatio-temporal graph fusion by graph fusion method, so as to obtain event schema graph.

- DoubleGAE [50] ${}^{4}$ (Double Graph Autoencoder): DoubleGAE employs a high-level variational graph autoencoder to learn event skeletons. It then uses a low-level GCN graph autoencoder to reconstruct entity-entity relationships, creating an event schema graph through a two-stage process.

These schema induction methods aim to compare Pred-ID with existing schema induction methods in terms of generating event evolutionary pattern. The goal is to demonstrate the effectiveness of inductive generation of existing graphs, deductive expansion for future graphs, and the reverse guidance of inductive and deductive effects on future event predictions.

- PGBIG ${}^{5}$ [56]: PGBIG is designed to make predictions about the future from a predefined explicit graph.

---

<!-- Footnote -->

1 https://github.com/NextCenturyCorporation/kairos-pub/tree/master/ data-format/ontology

2 https://github.com/limanling/temporal-graph-schema

${}^{3}$ https://github.com/tsinghua-fib-lab/Traffic-Benchmark

4 https://github.com/tracyjin/DoubleGAE.git

5 https://github.com/705062791/PGBIG

<!-- Footnote -->

---

<!-- Meanless: 10-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

<!-- Media -->

<!-- figureText: G1 G2 -->

<img src="https://cdn.noedgeai.com/bo_d2u6h13ef24c73b31eqg_10.jpg?x=330&y=150&w=1084&h=441&r=0"/>

Fig. 7. Segmentation strategy for the original dataset.

Table 1

Statistics of the datasets we used, "e" and "a" short for "event" and "argument", respectively.

<table><tr><td>Datasets</td><td>General</td><td>IED</td><td>General-IED</td><td>Car-IED</td><td>Suicide-IED</td></tr><tr><td>#train/val/test instances</td><td>1208/170/170</td><td>3381/456/456</td><td>924/95/159</td><td>771/58/53</td><td>1686/276/244</td></tr><tr><td>Avg # e/a nodes per graph</td><td>31.0/35.4</td><td>45.5/65.7</td><td>43.0/59.5</td><td>46.8/65.2</td><td>46.2/69.3</td></tr><tr><td>Avg # ee/ea/aa links per graph</td><td>72.6/56.1/21.3</td><td>83.9/110.1/39.8</td><td>86.2/98.5/34.6</td><td>${88.0}/{109.0}/{35.8}$</td><td>80.2/116.6/44.3</td></tr></table>

<!-- Media -->

- SGNN [39] ${}^{6}$ (Scaled Graph Neural Network): SGNN constructs narrative event evolution graphs based on event chains. It models event interactions to learn better event representations for selecting the correct subsequent events.

- siMLPe [57]: siMLPe first uses the general association between GCN and redesign discrete cosine transform to revealing the change process of events over time, so as to build an event schema graph.

- EventComp [23]: EventComp simultaneously learns embeddings for event triggers and arguments, combines embeddings into event representations, and predicts the strength of association between two events using a coherence function.

These event prediction methods aim to compare Pred-ID with existing event prediction methods in terms of predicting events. The goal is to demonstrate the effectiveness of extracting event evolutionary pattern and coherently predicting future events by merging information at both the instance-level and schema-level.

- Pred-ID-DVAE (Pred-ID with DVAE [58]7): It replaces the induction and deduction method of generating event evolutionary pattern in Pred-ID with DVAE (DAG variational autoencoder), which uses an asynchronous message-passing mechanism to encode directed acyclic graphs (DAGs) respecting their partial order.

- Pred-ID-CONDGEN (Pred-ID with CONDGEN [59]8): Akin to Pred-ID-DVAE, it replaces the induction and deduction method of generating event evolutionary pattern in Pred-ID with COND-GEN, which combines the capabilities of graph convolutional networks (GCN), variational autoencoders (VAE), and generative adversarial networks (GAN) to generate event evolutionary pattern.

- SDGL [60] ${}^{9}$ (Dynamic Event Graph Model): SDGL uses Temporal Convolutional Network to mine the temporal relationships of data, thereby building a dynamic graph, using the spatial relationships between nodes to build a static graph, and finally integrating the dynamic and static graphs to achieve node prediction.

These variants aim to refine the comparison by considering the effectiveness of Pred-ID's inductive and deductive schema generation methods against existing methods. The goal is to demonstrate that Pred-ID's inductive and deductive approach can capture richer event development processes and expand them deductively to acquire more effective future information, leading to the generation of more effective event evolutionary pattern.

#### 4.1.3. Implementation details

Several metrics are used to evaluate the performance of event prediction, including Accuracy, F1, HITS@k, and MRR (Mean Reciprocal Rank). Accuracy is the ratio of correctly predicted events to the total number of predictions, reflecting the overall correctness of the model. F1 combines the precision and recall of the model, representing the harmonic mean of precision and recall. HITS@k indicates the proportion of correctly predicted events within the top $\mathrm{k}$ predictions, measuring the model’s performance in the top $\mathrm{k}$ predictions. ${MRR}$ is the reciprocal of the average rank of correct events in all predictions, used to assess the quality of the model's ranking of correct events. These metrics comprehensively consider different aspects of performance, contributing to a comprehensive evaluation of the effectiveness of event prediction models.

To assess the quality of the pattern, we use Event type match, Event sequence match (l), and MCS (maximum common subgraph) metrics to compare the generated pattern with ground-truth schema. Event type match measures the similarity between the event types in the generated pattern and the ground-truth schema by calculating the F1 score, providing a comprehensive evaluation of the overlap and accuracy between the two sets. Event sequence match evaluates the similarity by calculating the F1 score between all event sequences of length $l$ in the generated pattern and the ground-truth schema, providing an assessment of precision and recall for the collected event sequences. MCS calculates the maximum common subgraph between the pattern and the ground-truth schema, revealing the similarity in global structure by determining the maximum number of event nodes and temporal orders in this common subgraph.

Regarding hyperparameters, in the Inductive Event Graph Generation,the hyperparameter $h$ ,representing the value for decomposing the local subgraph ${c}_{h}\left( {e}_{i}\right)$ ,is set to 1 . The hyperparameter $k$ ,indicating the selection of the Top $\lceil {kN}\rceil$ local subgraphs,is set to 0.5 . The values for $h$ and $k$ are referenced from the configuration in [61]. In the Graph Fusion for Event Prediction, the hyperparameter margin in the loss function is set to 0.015 , following the configuration in [39]. The number $B$ of mixture component is set to 2 following [49]. The value of the hyperparameter $\varepsilon$ representing the upper limit is set to the maximum number of instance event graphs in the training set. The dimension of the node hidden state is set to 128 , the dimension of the Event Core Distribution is set to 64, and the batch size is 16 . The learning rate is $1\mathrm{e} - 5$ ,and the number of training epochs is 400 . Both our model and the baselines are trained on an RTX 3090 GPU with 24 GB DRAM.

---

<!-- Footnote -->

6 https://github.com/eecrazy/ConstructingNEEG_IJCAI_2018

7 https://github.com/muhanzhang/D-VAE

8 https://github.com/KelestZ/CondGen

9 https://github.com/ZhuoLinLi-shu/SDGL

<!-- Footnote -->

---

<!-- Meanless: 11-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

<!-- Media -->

Table 2

Event prediction result of hyperparameter $p$ .

<table><tr><td rowspan="2">Dataset</td><td rowspan="2">Model</td><td rowspan="2">Event Match</td><td colspan="2">Event Sequence Match</td><td colspan="2">MCS</td><td rowspan="2">Accuracy</td><td rowspan="2">F1</td><td rowspan="2">Efficiency</td></tr><tr><td>$1 = 2$</td><td>$1 = 3$</td><td>#nodes</td><td>#edges</td></tr><tr><td rowspan="4">General-IED</td><td>Pred-ID $\left( {p = {0.9}}\right)$</td><td>0.9141</td><td>0.3875</td><td>0.0941</td><td>12.58</td><td>13.34</td><td>0.8418</td><td>0.7087</td><td>0.287 s</td></tr><tr><td>Pred-ID $\left( {p = {0.7}}\right)$</td><td>0.9158</td><td>0.4782</td><td>0.1284</td><td>14.97</td><td>17.01</td><td>0.8504</td><td>0.7191</td><td>0.398 s</td></tr><tr><td>Pred-ID $\left( {p = {0.5}}\right)$</td><td>0.9160</td><td>0.5592</td><td>0.1673</td><td>17.00</td><td>20.47</td><td>0.8690</td><td>0.7312</td><td>0.542 s</td></tr><tr><td>Pred-ID $\left( {p = {0.3}}\right)$</td><td>0.9160</td><td>0.5749</td><td>0.2038</td><td>17.23</td><td>21.08</td><td>0.8333</td><td>0.7275</td><td>0.610 s</td></tr><tr><td rowspan="4">Car-IED</td><td>Pred-ID $\left( {p = {0.9}}\right)$</td><td>0.9310</td><td>0.3949</td><td>0.0834</td><td>11.71</td><td>11.89</td><td>0.7770</td><td>0.7348</td><td>0.355 s</td></tr><tr><td>Pred-ID $\left( {p = {0.7}}\right)$</td><td>0.9318</td><td>0.4965</td><td>0.1261</td><td>13.76</td><td>15.18</td><td>0.8598</td><td>0.7377</td><td>0.471 s</td></tr><tr><td>Pred-ID $\left( {p = {0.5}}\right)$</td><td>0.9318</td><td>0.5900</td><td>0.1753</td><td>15.74</td><td>18.67</td><td>0.8726</td><td>0.7406</td><td>0.675 s</td></tr><tr><td>Pred-ID $\left( {p = {0.3}}\right)$</td><td>0.9318</td><td>0.6067</td><td>0.1980</td><td>16.07</td><td>19.32</td><td>0.8535</td><td>0.6786</td><td>0.739 s</td></tr><tr><td rowspan="4">Suicide-IED</td><td>Pred-ID $\left( {p = {0.9}}\right)$</td><td>0.9100</td><td>0.3139</td><td>0.0719</td><td>10.33</td><td>10.46</td><td>0.6379</td><td>0.5695</td><td>0.244 s</td></tr><tr><td>Pred-ID $\left( {p = {0.7}}\right)$</td><td>0.9150</td><td>0.4457</td><td>0.1291</td><td>13.80</td><td>15.57</td><td>0.6559</td><td>0.5752</td><td>0.364 s</td></tr><tr><td>Pred-ID $\left( {p = {0.5}}\right)$</td><td>0.9150</td><td>0.5480</td><td>0.1920</td><td>16.68</td><td>20.06</td><td>0.6594</td><td>0.5803</td><td>${0.566}\mathrm{\;s}$</td></tr><tr><td>Pred-ID $\left( {p = {0.3}}\right)$</td><td>0.9150</td><td>0.5821</td><td>0.2308</td><td>17.46</td><td>21.51</td><td>0.6551</td><td>0.5727</td><td>0.778 s</td></tr></table>

<!-- Media -->

### 4.2. Parameter sensitivity on Pred-ID

In this section, we discuss the impact of different values for the hy-perparameter $p$ on the generation of inductive and deductive event evolution pattern and event prediction. The hyperparameter $p$ is present in Eqs. (7) and (10) and is used to determine whether to add temporal orders.

As shown in Table 2, it is evident that the best performance for event evolutionary pattern generation occurs when $p = {0.3}$ ,while the optimal event prediction is achieved when $p = {0.5}$ . The reason for this is that when the hyperparameter $p$ is set lower,many temporal orders are judged to exist during Inductive Event Graph Generation, and the same applies to Deductive Event Graph Expansion. This results in the best Event Sequence Match and MCS, while Event Match shows little change. However, an excess of edges leads to a decline in event prediction performance. As $p$ decreases from 0.9 to 0.5,event prediction performance continues to improve. When $p$ decreases further to 0.3 , event prediction performance begins to decline. This suggests that when $p$ is very high,the inductive and deductive processes may exclude many potential paths that should exist, possibly including originally existing temporal orders. This hinders the event evolutionary pattern from providing effective guidance for future directions for instance event graphs. On the other hand,when $p$ is very low,many unnecessary paths are added, introducing noise into the event evolutionary pattern and creating chaotic guidance for instance event graph. Additionally, the Efficiency column shows a continuous increase as $p$ decreases. This is because,with a lower $p$ ,more temporal orders are added,and this process of adding temporal orders increases the runtime of Pred-ID for each iteration. After comprehensive consideration, we ultimately choose 0.5 as the value for $p$ .

### 4.3. Ablation study on Pred-ID

To assess the impact of relational components in Pred-ID, we conducted an ablation study by examining six variants:

- I(w/o ex): This variant employs the basic Inductive Event Graph Generation module without extracting the event core subgraph.

- I: The basic Inductive Event Graph Generation module.

- I+D: It includes the Inductive Event Graph Generation module with the addition of the Deductive Event Graph Expansion.

- I+D+P(w/o agg & f): Event prediction is performed based on the inductive and deductive processes without aggregating event arguments to form the aggregated event graph ${G}^{Agg}$ and subsequent graph fusion. Events are predicted independently using instance event graph ${G}^{I}$ and event evolutionary pattern ${G}^{SID}$ ,and these probabilities are added to predict final events.

- I+D+P(w/o f): Aggregation of event arguments to form the aggregated event graph ${G}^{Agg}$ is done based on the inductive and deductive processes, but graph fusion is omitted. Events are predicted independently using the aggregated event graph ${G}^{Agg}$ and event evolutionary pattern ${G}^{SID}$ ,and these probabilities are added to predict final events.

- I+D+P(w/o agg): Event prediction is performed based on the inductive and deductive processes without aggregating event arguments. During graph fusion, only the fusion of event trigger information is considered to form the fusion graph ${G}^{F}$ ,and predictions are made using this fusion graph ${G}^{F}$ .

- I+D+P(+ agg & f): Event prediction is performed based on the inductive and deductive processes, aggregating event arguments to form the aggregated event graph ${G}^{Agg}$ . Subsequently,the fusion of the aggregated event graph ${G}^{Agg}$ with the event evolutionary pattern ${G}^{SID}$ is employed for subsequent event prediction.

These variants aim to investigate the contributions of different components in Pred-ID, including Inductive Event Graph Generation, Deductive Event Graph Expansion, and Graph Fusion for Event Prediction, as well as the effects of aggregation and fusion in the model.

The results of the ablation study are presented in Table 3, and the observations are as follows: Firstly, the I model outperforms I(w/o ex) in terms of event evolutionary pattern generation. This is because the event core subgraph ${G}^{SP}$ in Eqs. (1) to (3) and the event core distribution $N\left( {\mu ,\sigma }\right)$ in Eqs. (5) to (6) provides valuable guidance for constructing the event evolutionary pattern ${G}^{SID}$ . Secondly,the I and I(w/o ex) models perform less effectively in event evolutionary pattern generation compared to I+D. The reason is that I and I(w/o ex) include only the event types in the induced event graph ${G}^{SI}$ ,while the ground-truth schema includes not only all event types from the induced event graph ${G}^{SI}$ but also many other event types. Pure inductive methods cannot cover all event types in the ground-truth schema well, and the same applies to the temporal orders between event types. Thirdly, the I+D+P(w/o agg & f) model exhibits lower performance in event prediction compared to $\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {\mathrm{w}/\mathrm{o}\mathrm{f}}\right)$ . This is because argument aggregation (see Eq. (11)) can better capture and reflect the complex relationships among events, thereby improving the accuracy of event prediction. Fourthly, the I+D+P(w/o agg & f) model shows lower event prediction performance than $\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {\mathrm{w}/\mathrm{o}\text{agg}}\right)$ . This indicates that alignment and fusion (see Eq. (12)) can integrate existing and future trends in the event evolutionary pattern into the instance event graph, guiding subsequent event prediction. Finally, the I+D+P(+ agg & f) models consistently outperform I+D+P(w/o agg) and I+D+P(w/o f). This suggests that the combination of argument aggregation and alignment fusion can better assist event prediction, allowing the model to not only identify triggers in the instance event graph but also discover corresponding arguments, obtaining more information.

<!-- Meanless: 12-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

<!-- Media -->

Table 3

The ablation results of Pred-ID.

<table><tr><td rowspan="2">Dataset</td><td rowspan="2">Model</td><td rowspan="2">Event Match</td><td colspan="2">Event Sequence Match</td><td colspan="2">MCS</td><td rowspan="2">Accuracy</td><td rowspan="2">F1</td><td rowspan="2">Efficiency</td></tr><tr><td>$1 = 2$</td><td>$1 = 3$</td><td>#nodes</td><td>#edges</td></tr><tr><td rowspan="7">General-IED</td><td>I(w/o ex)</td><td>0.2325</td><td>0.0585</td><td>0.0161</td><td>3.04</td><td>2.16</td><td>-</td><td>-</td><td>0.098 s</td></tr><tr><td>I</td><td>0.2325</td><td>0.0593</td><td>0.0170</td><td>3.42</td><td>2.55</td><td>-</td><td>-</td><td>0.104 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D}$</td><td>0.9153</td><td>0.5567</td><td>0.1801</td><td>16.99</td><td>20.46</td><td>_____</td><td>_____</td><td>0.316 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {\mathrm{w}/\mathrm{o}\text{agg}\mathrm{{\& f}}}\right)$</td><td>0.9122</td><td>0.5463</td><td>0.1481</td><td>16.12</td><td>19.64</td><td>0.7234</td><td>0.6145</td><td>0.338 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {\mathrm{w}/\mathrm{o}\mathrm{f}}\right)$</td><td>0.9136</td><td>0.5494</td><td>0.1505</td><td>16.20</td><td>19.97</td><td>0.7564</td><td>0.6697</td><td>0.453 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {\mathrm{w}/\mathrm{o}\text{agg}}\right)$</td><td>0.9091</td><td>0.5541</td><td>0.1560</td><td>16.13</td><td>19.70</td><td>0.8375</td><td>0.7182</td><td>0.502 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {+\text{agg }\& \mathrm{f}}\right)$</td><td>0.9160</td><td>0.5592</td><td>0.1673</td><td>17.00</td><td>20.47</td><td>0.8690</td><td>0.7312</td><td>0.542 s</td></tr><tr><td rowspan="7">Car-IED</td><td>I(w/o ex)</td><td>0.3763</td><td>0.0718</td><td>0.0093</td><td>3.61</td><td>2.51</td><td>-</td><td>-</td><td>${0.102}\mathrm{\;s}$</td></tr><tr><td>I</td><td>0.3763</td><td>0.0737</td><td>0.0101</td><td>3.73</td><td>2.73</td><td>-</td><td>-</td><td>0.113 s</td></tr><tr><td>I+D</td><td>0.9327</td><td>0.5949</td><td>0.1786</td><td>15.78</td><td>18.78</td><td>_____</td><td>-</td><td>0.472 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {\mathrm{w}/\mathrm{o}\text{agg}\mathrm{\& }\mathrm{f}}\right)$</td><td>0.9145</td><td>0.5779</td><td>0.1612</td><td>15.26</td><td>18.10</td><td>0.7443</td><td>0.5975</td><td>0.508 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {\mathrm{w}/\mathrm{o}\mathrm{f}}\right)$</td><td>0.9276</td><td>0.5820</td><td>0.1677</td><td>15.48</td><td>18.65</td><td>0.7760</td><td>0.6384</td><td>0.546 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {\mathrm{w}/\mathrm{o}\text{agg}}\right)$</td><td>0.9298</td><td>0.5824</td><td>0.1675</td><td>15.59</td><td>18.40</td><td>0.8662</td><td>0.7346</td><td>0.632 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {+\text{agg }\& \mathrm{f}}\right)$</td><td>0.9318</td><td>0.5900</td><td>0.1753</td><td>15.74</td><td>18.67</td><td>0.8726</td><td>0.7406</td><td>0.675 s</td></tr><tr><td rowspan="7">Suicide-IED</td><td>I(w/o ex)</td><td>0.2288</td><td>0.0525</td><td>0.0140</td><td>3.10</td><td>2.28</td><td>-</td><td>-</td><td>0.085 s</td></tr><tr><td>I</td><td>0.2288</td><td>0.0603</td><td>0.0158</td><td>3.29</td><td>2.46</td><td>-</td><td>-</td><td>0.097 s</td></tr><tr><td>I+D</td><td>0.9163</td><td>0.5805</td><td>0.2130</td><td>17.46</td><td>21.40</td><td>_____</td><td>-</td><td>0.399 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {\mathrm{w}/\mathrm{o}\text{agg}\mathrm{\& }\mathrm{f}}\right)$</td><td>0.9005</td><td>0.5345</td><td>0.1789</td><td>16.40</td><td>19.53</td><td>0.4854</td><td>0.4523</td><td>0.458 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {\mathrm{w}/\mathrm{o}\mathrm{f}}\right)$</td><td>0.9087</td><td>0.5420</td><td>0.1879</td><td>16.48</td><td>19.59</td><td>0.5296</td><td>0.4944</td><td>0.477 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {\mathrm{w}/\mathrm{o}\text{agg}}\right)$</td><td>0.9067</td><td>0.5403</td><td>0.1888</td><td>16.33</td><td>20.07</td><td>0.6502</td><td>0.5728</td><td>0.545 s</td></tr><tr><td>$\mathrm{I} + \mathrm{D} + \mathrm{P}\left( {+\text{agg }\& \mathrm{f}}\right)$</td><td>0.9150</td><td>0.5480</td><td>0.1920</td><td>16.68</td><td>20.06</td><td>0.6594</td><td>0.5803</td><td>0.566 s</td></tr></table>

<!-- Media -->

### 4.4. Comparison with other methods

Firstly, in terms of event evolutionary pattern generation, we compare the event evolutionary pattern ${G}^{SID}$ generated by Pred-ID with those generated by other comparative methods and the ground-truth schema, as shown in Table 4. According to the Event Match, Event Sequence Match, and MCS metrics, our proposed Pred-ID model achieves the best performance in event type matching, event sequence matching, and maximum common subgraph matching. This is attributed to the Inductive Event Graph Generation module of Pred-ID, which extracts the event core subgraph of in the event-type graph ${G}^{S}$ . This enables Pred-ID to generate event information that is close to the core subgraph during generation (see Eqs. (1) to (3)). This mechanism ensures that the MCS metric achieves the best performance among all models. In the Inductive Event Graph Generation module, Pred-ID also captures the trend information of event development through graph propagation (see Eq. (4)). This mechanism leads the model to better capture the sequential relationships in event development when generating an induced event graph ${G}^{SI}$ ,achieving optimal results in the Event Sequence Match metric. The deductive expansion of the induced event graph ${G}^{SI}$ allows the generation of event evolutionary pattern ${G}^{SID}$ not limited to the event types in the original event-type graph ${G}^{S}$ . The addition of multiple additional event types expands more event development processes in the future direction of the event evolutionary pattern ${G}^{SID}$ (see Eq. (10)). This mechanism ensures that Pred-ID achieves the best results in Event Match and Event Sequence Match.

Furthermore, the performance of Pred-ID-DVAE and Pred-ID-CondGen, which replace the kernel of event evolutionary pattern generation, is lower than DoubleGAE and our model. One reason is that Pred-ID-DVAE and Pred-ID-CondGen are trained to generate event evolutionary pattern ${G}^{SID}$ and use them for event prediction. In contrast, the baseline model that only performs event pattern induction is trained to generate event evolutionary pattern ${G}^{SID}$ without considering event prediction. This leads Pred-ID-DVAE and Pred-ID-CondGen to generate evolutionary pattern ${G}^{SID}$ that are helpful for event prediction and discard some unrelated event types and their temporal orders, resulting in a decrease in various metrics. In contrast, our inductive and deductive generation model preserves the event-type nodes of the current event-type graph ${G}^{S}$ during induction and expands future event types during deduction, ensuring alignment with the instance event graph ${G}^{I}$ and predicting future event types.

Secondly, in terms of event prediction, we compare the event prediction performance of Pred-ID with other baseline models, as shown in Table 5. Pred-ID outperforms other comparative models in terms of Accuracy, HIT@k, and MRR metrics, achieving the best results. This is attributed to our Pred-ID model aggregating argument information from the instance event graph ${G}^{I}$ to construct the aggregated event graph ${G}^{Agg}$ (see Eq. (11)). This mechanism refines event representations in the instance event graph ${G}^{I}$ ,making it easier for the event evolutionary pattern ${G}^{SID}$ to alignment and fusion. Unlike baseline models that only use instance-level information, Pred-ID aligns the aggregated event graph ${G}^{Agg}$ with the event evolutionary pattern ${G}^{SID}$ . The corresponding event-type nodes ${e}^{SID}$ and event nodes ${e}^{Agg}$ are fused, combining instance-level and schema-level information. This mechanism allows Pred-ID to better understand the current event development at schema-level, analyze future event types in the event evolutionary pattern ${G}^{SID}$ ,and select the best event from the candidate events.

Moreover, Pred-ID-DVAE and Pred-ID-CondGen, which replace the kernel of event evolutionary pattern generation, show lower event prediction performance in terms of Accuracy, F1, HITS@k, and MRR metrics compared to Pred-ID. This is because our Inductive Event Graph Generation module retains all event types in the event-type graph ${G}^{S}$ ,making it easy to correspond to the corresponding event triggers in the instance event graph ${G}^{I}$ during subsequent alignment. The Deductive Event Graph Expansion module expands multiple future event types and expands the main process of event development, providing valuable assistance in future event prediction. The event evolutionary pattern ${G}^{SID}$ constructed by the inductive generation and deductive expansion effectively summarizes the current event context and expands it in the future direction.

Thirdly, in terms of multi-step prediction, we compare the multistep prediction performance of Pred-ID with that of other comparison methods, with RNN introduced as the most basic multi-step prediction baseline. As shown in Fig. 8, Pred-ID outperforms other comparison models in terms of Accuracy (see (a)), F1 (see (b)), MRR (see (c)), and HITS@k (see (d), (e) and (f)). This is attributed to the fact that our Pred-ID model, with the guidance of the generated event evolutionary pattern ${G}^{SID}$ through both induction and deduction,performs well in multi-step event prediction. The induction module of Pred-ID comprehensively summarizes the event types in the instance event graph ${G}^{I}$ , while the Deductive Event Graph Expansion module expands the future event types in the future direction to obtain the event evolutionary pattern ${G}^{SID}$ . The alignment and fusion of the event evolution pattern ${G}^{SID}$ with the aggregated event graph ${G}^{Agg}$ enable the model to distinguish between events that have occurred and predicted event types. This allows Pred-ID to identify key points that differ from the previous step at each step of multi-step prediction, resulting in different prediction outcomes.

<!-- Meanless: 13-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

<!-- Media -->

Table 4

The statistics of event evolutionary pattern generation.

<table><tr><td rowspan="2">Dataset</td><td rowspan="2">Model</td><td rowspan="2">Event Match</td><td colspan="2">Event Sequence Match</td><td colspan="2">MCS</td><td rowspan="2">Efficiency</td><td rowspan="2">Parameter size</td></tr><tr><td>$1 = 2$</td><td>$1 = 3$</td><td>#nodes</td><td>#edges</td></tr><tr><td rowspan="7">General-IED</td><td>TEGM</td><td>0.6672</td><td>0.1761</td><td>0.0630</td><td>6.32</td><td>5.41</td><td>4.354 s</td><td>1.72M</td></tr><tr><td>DGCRN</td><td>0.9055</td><td>0.4441</td><td>0.1317</td><td>11.62</td><td>15.63</td><td>0.425 s</td><td>3.4M</td></tr><tr><td>DASTGN</td><td>0.8954</td><td>0.5396</td><td>0.1125</td><td>14.12</td><td>14.56</td><td>0.144 s</td><td>2.8M</td></tr><tr><td>DoubleGAE</td><td>0.6953</td><td>0.2634</td><td>0.1248</td><td>15.98</td><td>15.03</td><td>0.628 s</td><td>2.11M</td></tr><tr><td>Pred-ID-DVAE</td><td>0.6753</td><td>0.1835</td><td>0.0826</td><td>7.62</td><td>6.91</td><td>0.144 s</td><td>5.53M</td></tr><tr><td>Pred-ID-CondGen</td><td>0.2401</td><td>0.1309</td><td>0.0497</td><td>4.06</td><td>4.58</td><td>0.091 s</td><td>3.35M</td></tr><tr><td>Pred-ID(Ours)</td><td>0.9160</td><td>0.5592</td><td>0.1673</td><td>17.00</td><td>20.47</td><td>0.542 s</td><td>4.69M</td></tr><tr><td rowspan="7">Car-IED</td><td>TEGM</td><td>0.5641</td><td>0.1574</td><td>0.0417</td><td>5.57</td><td>4.68</td><td>${3.511}\mathrm{\;s}$</td><td>1.72M</td></tr><tr><td>DGCRN</td><td>0.9314</td><td>0.5725</td><td>0.1347</td><td>14.71</td><td>17.67</td><td>0.434 s</td><td>3.4M</td></tr><tr><td>DASTGN</td><td>0.9162</td><td>0.5834</td><td>0.1439</td><td>13.54</td><td>12.97</td><td>0.136 s</td><td>2.8M</td></tr><tr><td>DoubleGAE</td><td>0.6653</td><td>0.2514</td><td>0.0812</td><td>10.85</td><td>10.03</td><td>0.604 s</td><td>2.11M</td></tr><tr><td>Pred-ID-DVAE</td><td>0.5993</td><td>0.1655</td><td>0.0488</td><td>6.99</td><td>6.31</td><td>0.186 s</td><td>5.53M</td></tr><tr><td>Pred-ID-CondGen</td><td>0.3607</td><td>0.1243</td><td>0.0354</td><td>4.05</td><td>5.14</td><td>0.099 s</td><td>3.35M</td></tr><tr><td>Pred-ID(Ours)</td><td>0.9318</td><td>0.6067</td><td>0.1980</td><td>16.07</td><td>19.32</td><td>0.675 s</td><td>4.69M</td></tr><tr><td rowspan="7">Suicide-IED</td><td>TEGM</td><td>0.6022</td><td>0.1771</td><td>0.0428</td><td>5.88</td><td>4.96</td><td>${3.502}\mathrm{\;s}$</td><td>1.72M</td></tr><tr><td>DGCRN</td><td>0.9049</td><td>0.5594</td><td>0.1613</td><td>14.62</td><td>17.71</td><td>0.414 s</td><td>3.4M</td></tr><tr><td>DASTGN</td><td>0.8736</td><td>0.5524</td><td>0.1834</td><td>14.37</td><td>16.18</td><td>0.101 s</td><td>2.8M</td></tr><tr><td>DoubleGAE</td><td>0.7125</td><td>0.2952</td><td>0.0973</td><td>6.19</td><td>5.34</td><td>0.498 s</td><td>2.11M</td></tr><tr><td>Pred-ID-DVAE</td><td>0.6445</td><td>0.1930</td><td>0.0503</td><td>5.98</td><td>5.55</td><td>0.155 s</td><td>5.53M</td></tr><tr><td>Pred-ID-CondGen</td><td>0.2406</td><td>0.1031</td><td>0.0335</td><td>3.56</td><td>3.96</td><td>0.084 s</td><td>3.35M</td></tr><tr><td>Pred-ID(Ours)</td><td>0.9150</td><td>0.5821</td><td>0.2308</td><td>17.46</td><td>21.51</td><td>0.566 s</td><td>4.69M</td></tr></table>

Table 5

The result of event prediction in one-step.

<table><tr><td rowspan="2">Datasets</td><td rowspan="2">Methods</td><td rowspan="2">Accuracy</td><td rowspan="2">F1</td><td colspan="2">@HITS</td><td rowspan="2">@5</td><td rowspan="2">MRR</td><td rowspan="2">Efficiency</td><td rowspan="2">Parameter size</td></tr><tr><td>@1</td><td>@3</td></tr><tr><td rowspan="8">IED</td><td>PGBIG</td><td>0.4611</td><td>0.4726</td><td>0.4657</td><td>0.7538</td><td>0.8834</td><td>0.5954</td><td>0.024 s</td><td>3.41M</td></tr><tr><td>EventComp</td><td>0.4133</td><td>0.4397</td><td>0.4156</td><td>0.5949</td><td>0.7253</td><td>0.5567</td><td>0.029 s</td><td>0.87M</td></tr><tr><td>SDGL</td><td>0.6314</td><td>0.5411</td><td>0.6032</td><td>0.8739</td><td>0.9215</td><td>0.7286</td><td>0.021 s</td><td>5.00M</td></tr><tr><td>siMLPe</td><td>0.5431</td><td>0.5014</td><td>0.5183</td><td>0.8362</td><td>0.9044</td><td>0.6917</td><td>0.091 s</td><td>3.31M</td></tr><tr><td>SGNN</td><td>0.5227</td><td>0.2965</td><td>0.5227</td><td>0.8906</td><td>0.9814</td><td>0.7094</td><td>0.029 s</td><td>1.06M</td></tr><tr><td>Pred-ID-DVAE</td><td>0.4633</td><td>0.4599</td><td>0.4587</td><td>0.7241</td><td>0.8091</td><td>0.6244</td><td>0.167 s</td><td>5.53M</td></tr><tr><td>Pred-ID-CondGen</td><td>0.4801</td><td>0.4298</td><td>0.4790</td><td>0.8042</td><td>0.9172</td><td>0.6587</td><td>0.075 s</td><td>3.35M</td></tr><tr><td>Pred-ID(Ours)</td><td>0.6507</td><td>0.5598</td><td>0.6438</td><td>0.9243</td><td>0.9860</td><td>0.7883</td><td>0.103 s</td><td>4.69M</td></tr><tr><td rowspan="8">General</td><td>PGBIG</td><td>0.6024</td><td>0.6191</td><td>0.5869</td><td>0.9125</td><td>0.9267</td><td>0.7148</td><td>0.046 s</td><td>3.41M</td></tr><tr><td>EventComp</td><td>0.5226</td><td>0.5339</td><td>0.5226</td><td>0.6710</td><td>0.7871</td><td>0.6393</td><td>0.023 s</td><td>0.87M</td></tr><tr><td>SDGL</td><td>0.6812</td><td>0.6049</td><td>0.6487</td><td>0.9545</td><td>0.9758</td><td>0.7962</td><td>0.117 s</td><td>5.00M</td></tr><tr><td>siMLPe</td><td>0.6031</td><td>0.5524</td><td>0.6158</td><td>0.9247</td><td>0.9636</td><td>0.7729</td><td>0.107 s</td><td>3.31M</td></tr><tr><td>SGNN</td><td>0.6290</td><td>0.3527</td><td>0.6290</td><td>0.9419</td><td>0.9803</td><td>0.7874</td><td>0.026 s</td><td>1.06M</td></tr><tr><td>Pred-ID-DVAE</td><td>0.5993</td><td>0.6009</td><td>0.6026</td><td>0.8940</td><td>0.9305</td><td>0.7564</td><td>0.183 s</td><td>5.53M</td></tr><tr><td>Pred-ID-CondGen</td><td>0.6254</td><td>0.6316</td><td>0.6321</td><td>0.9465</td><td>0.9833</td><td>0.7864</td><td>0.054 s</td><td>3.35M</td></tr><tr><td>Pred-ID(Ours)</td><td>0.7052</td><td>0.6110</td><td>0.7119</td><td>0.9669</td><td>0.9868</td><td>0.8352</td><td>0.130 s</td><td>4.69M</td></tr></table>

<!-- Media -->

Furthermore, our Pred-ID model, along with other comparison models, exhibits the best prediction performance in the first step, and as the time steps expand, the prediction performance shows a certain degree of decline compared to the first step. This is because these models are trained based on single-step prediction, and the performance may suffer when applied to multi-step prediction, especially in the second step where there is only one node difference from the previous instance event graph, making it challenging for most models to make correct predictions. As the time steps increase, the differences in instance event graph ${G}^{I}$ become more pronounced,and models find it easier to make correct predictions for subsequent events.

## 5. Discussion on evolutionary schema mining

In this section, we have selected the most typical event evolutionary pattern ${G}^{SID}$ and the powerful sub-pattern for a case study to discussion the effectiveness of Pred-ID.

#### 5.1.The discussion of powerful sub-pattern visualization

The powerful sub-pattern is extracted from the event evolutionary pattern by the candidate events with high fitness scores in the Graph Fusion for Event Prediction stage. Specifically, based on the candidate events with high fitness scores, we reverse observe the fusion graph to identify the event nodes assigned high attention weights. Using the event nodes, we locate the corresponding event-type nodes in the event evolutionary pattern and extract the powerful sub-pattern of the event evolutionary pattern.

The graph of the instance, event evolutionary pattern and powerful sub-pattern are shown in Fig. 9. Thanks to the induction and deduction of the Event Graph, the pattern (Fig. 9(b)) includes not only current event types, but also future event types. In addition, the construction of the powerful sub-pattern (Fig. 9(c)) further refines the development direction of future events, so as to enhance the effectiveness of event prediction. In contrast, the event pattern generated by Pred-ID-DVAE (Fig. 9(e)) contains some event types in the event-type graph ${G}^{S}$ ,but the mining of event dependence is rather chaotic. Moreover, the powerful sub-pattern (Fig. 9(f)) cannot extract key information. The event evolution pattern generated by Pred-ID-CondGen (Fig. 9(h)) successfully extracted important event types from the event-type graph ${G}^{S}$ ,but could not distinguish the better event sequence. In addition, the less powerful sub-pattern(Fig. 9(i)) cannot effectively guide event prediction.

<!-- Meanless: 14-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

<!-- Media -->

<!-- figureText: 0.6 EventComp EventComp EventComp SDGL SDGI siMLPe siMLPe SGNN SGNN Pred-ID-DVA Pred-ID-DVAE Pred-ID-CondGen Pred-ID-CondGen Pred-ID(Ours) 0.4 0.3 (b) F1 (c) MRR PGBIG PGBIG EventComp EventComp SDGL 0.9 SDGL siMLPe siMLPe Pred-ID-DVAI 0.8 Pred-ID-DVAI Pred-ID-CondGer Pred-ID-CondGen Pred-ID(Ours) - Pred-ID(Ours) 0.7 0.6 8 Time Steps Time Steps (e) HITS@3 (f) HITS @ 5 siMLPe 0.5 SGNN Pred-ID-DVAE Pred-ID-CondGen 0.4 0.4 Pred-ID(Ours) 0.3 0.3 0.2 0.1 10 (a) Accuracy PGBIG 0.6 EventComp siMLPe 0.8 0.5 Pred-ID-DVAE Pred-ID-CondGer HITS1 Pred-ID(Ours) HITS3 0.3 0.4 0.2 0.3 0.1 10 2 Time Steps (d) HITS @ 1 -->

<img src="https://cdn.noedgeai.com/bo_d2u6h13ef24c73b31eqg_14.jpg?x=205&y=148&w=1340&h=754&r=0"/>

Fig. 8. The result of event prediction in multi-step.

<!-- figureText: LOC Attack Injure DetonateExplode Attack ThreatenCoerce Promotion Attack Promotion (c) Powerful sub-pattern by Pred-ID Transportation Contact broadcas broadcast Attack Die (f) Powerful sub-pattern by Pred-ID-DVAE Demonstrate DetonateExplode (i) Powerful sub-pattern by Pred-ID-CONDGEN Broadcast targeted Demonstrate (a) Instance for Pred-ID (b) pattern by Pred-ID Attack LOC Die Contact Transportation Transportation throw targeted (d) Instance for Pred-ID-DVAE (e) pattern by Pred-ID-DVAE attack insurgency throw DetonateExplode (g) Instance for Pred-ID-CONDGEN (h) pattern by Pred-ID-CONDGEN -->

<img src="https://cdn.noedgeai.com/bo_d2u6h13ef24c73b31eqg_14.jpg?x=203&y=979&w=1343&h=889&r=0"/>

Fig. 9. The graph of the instance, event evolutionary pattern and powerful sub-pattern.

<!-- Media -->

#### 5.2.The discussion of powerful sub-pattern

For the powerful sub-pattern indicator in Table 6, we use the average, maximum and minimum values of the number of nodes (N), the number of connections (E) and the occupancy ratio on the evolution pattern observe the characteristics of the powerful sub-pattern extracted by each method. It can be found that Pred-ID reaches the maximum in all indicators, and the powerful sub-pattern can prove that the induced pattern structure and features more effectively, each node and edge of the induced pattern are of great useful to event prediction. However, the powerful sub-patterns observed by other comparison methods cannot cover more comprehensive information in the induction graph, which proves that other methods cannot comprehensively analyze the induction graph in event prediction. This phenomenon shows that the quality of the induced graph generated by the comparison method is not high, and it cannot serve the event prediction effectively. In terms of the effectively of the event prediction, Pred-ID achieves the best performance in event prediction metrics, including Accuracy, HITS@k, and MRR. The superior performance of Pred-ID can be attributed to its first inducts the current event-type graph ${G}^{S}$ ,mining the developmental processes within it, and then deduces the inducted event graph ${G}^{SI}$ ,expanding the primary process of event development. The combined approach of induction and deduction preserves the original information in the event-type graph ${G}^{S}$ and further expands future event types based on the discovered pattern, offering guidance for event prediction in the future direction.

<!-- Meanless: 15-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

<!-- Media -->

Table 6

Link schema replay accuracy on IED dataset.

<table><tr><td>Dataset</td><td>Model</td><td>Avg(N)</td><td>Max(N)</td><td>Min(N)</td><td>Avg(E)</td><td>Max(E)</td><td>Min(E)</td><td>Occupy</td><td>Accuracy</td><td>F1</td><td>HITS@1</td><td>MRR</td></tr><tr><td rowspan="4">IED</td><td>SGDL</td><td>13.57</td><td>16</td><td>6</td><td>30.19</td><td>37</td><td>3</td><td>0.426</td><td>0.6314</td><td>0.5411</td><td>0.6032</td><td>0.7286</td></tr><tr><td>Pred-ID-DVAE</td><td>17.47</td><td>23</td><td>2</td><td>23.55</td><td>43</td><td>1</td><td>0.372</td><td>0.4633</td><td>0.4599</td><td>0.4587</td><td>0.6244</td></tr><tr><td>Pred-ID-CondGen</td><td>8.48</td><td>11</td><td>5</td><td>28.54</td><td>40</td><td>5</td><td>0.401</td><td>0.4801</td><td>0.4298</td><td>0.4790</td><td>0.6587</td></tr><tr><td>Pred-ID(Ours)</td><td>36.14</td><td>55</td><td>12</td><td>46.55</td><td>94</td><td>9</td><td>0.585</td><td>0.6507</td><td>0.5598</td><td>0.6438</td><td>0.7883</td></tr><tr><td rowspan="4">General</td><td>SGDL</td><td>10.47</td><td>18</td><td>5</td><td>23.34</td><td>46</td><td>2</td><td>0.414</td><td>0.6812</td><td>0.6049</td><td>0.6487</td><td>0.7962</td></tr><tr><td>Pred-ID-DVAE</td><td>12.52</td><td>23</td><td>2</td><td>15.76</td><td>56</td><td>1</td><td>0.352</td><td>0.5993</td><td>0.6009</td><td>0.6026</td><td>0.7564</td></tr><tr><td>Pred-ID-CondGen</td><td>6.98</td><td>11</td><td>5</td><td>19.13</td><td>38</td><td>5</td><td>0.443</td><td>0.6254</td><td>0.6316</td><td>0.6321</td><td>0.7864</td></tr><tr><td>Pred-ID(Ours)</td><td>33.40</td><td>46</td><td>7</td><td>54.10</td><td>124</td><td>6</td><td>0.542</td><td>0.7052</td><td>0.6110</td><td>0.7119</td><td>0.8352</td></tr></table>

<!-- Media -->

## 6. Conclusion

The core task of this study is to effectively address the analysis of event evolutionary pattern and future event prediction in the field of information management. In contrast to conventional methods that concentrate on individual events or defining event-event types, we propose a predictive method called Pred-ID, which includes the inductive event graph generation, deductive event graph expansion and graph fusion for event prediction. Pred-ID, in the inductive event graph generation phase, learns the developmental process at the internal schema-level of the instance event graph, generating the developmental process between events. In the deductive event graph expansion phase, it expands future event types, prolongs the main process of event development, and constructs event evolutionary pattern. In the graph fusion for event prediction phase, it aligns the instance event graph with the event evolutionary pattern, collaboratively performing event prediction. Experimental results demonstrate that compared to existing methods, our model achieves optimal results in two key aspects. Firstly, our model excels in the task of event evolutionary pattern generation. Secondly, the model also demonstrates optimal performance in event prediction, improving prediction accuracy. Through experimental analysis, We observe that the generation of the event graph during induction can extract crucial information about event development from the event-type graph. when applied to the Deductive Event Graph Expansion, can expand multiple future event types based on the current development trend, expanding the event development process and constructing event evolutionary pattern. Graph Fusion for Event Prediction combines information at the instance-level and schema-level, allowing the instance event graph to understand not only the developmental trends at the schema-level but also the future event types, further enhancing the effectiveness of event prediction. For future work, we plan to incorporate external knowledge into the model to enable it to gain further insights into the developmental trends of the current instance event graph through external common knowledge. Additionally, we aim to further optimize the model, especially enhancing efficiency when dealing with large-scale graph data.

## CRediT authorship contribution statement

Huan Rong: Validation, Supervision, Software, Resources, Project administration, Methodology, Investigation, Funding acquisition, Formal analysis, Data curation, Conceptualization. Zhongfeng Chen: Methodology, Investigation, Funding acquisition, Formal analysis, Data curation, Conceptualization. Zhenyu Lu: Investigation, Formal analysis, Data curation, Conceptualization. Xiao-ke Xu: Writing - review & editing, Validation, Methodology, Investigation, Formal analysis, Conceptualization. Kai Huang: Resources. Victor S. Sheng: Writing - review & editing, Validation, Project administration, Formal analysis, Conceptualization.

## Declaration of competing interest

The authors declare the following financial interests/personal relationships which may be considered as potential competing interests: Huan Rong reports financial support was provided by Nanjing University of Information Science and Technology. None If there are other authors, they declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## Acknowledgments

This work is supported by the National Natural Science Foundation of China (NO. 62102187), the Natural Science Foundation of Jiangsu Province (Basic Research Program), China (NO. BK20210639), the National Natural Science Foundation of China (NO. 62173065, NO. 62476045), the National Natural Science Foundation of China (Key Program) (NO. U20B2061).

## Data availability

Data will be made available on request. References

[1] Saiping Guan, Xueqi Cheng, Long Bai, Fujun Zhang, Zixuan Li, Yutao Zeng, Xiaolong Jin, Jiafeng Guo, What is event knowledge graph: A survey, IEEE Trans. Knowl. Data Eng. (2022).

[2] Le Sun, Wenzhang Dai, Ghulam Muhammad, Multi-level graph memory network cluster convolutional recurrent network for traffic forecasting, Inf. Fusion 105 (2024) 102214.

[3] Ga Xiang, Yangsen Zhang, Jianlong Tan, Zihan Ran, En Shi, Research on the construction of event corpus with document-level causal relations for social security, Inf. Process. Manage. 60 (6) (2023) 103515.

[4] David Berdik, Safa Otoum, Nikolas Schmidt, Dylan Porter, Yaser Jararweh, A survey on blockchain for information systems management and security, Inf. Process. Manage. 58 (1) (2021) 102397.

[5] Jinghua Zhao, Huihong He, Xiaohua Zhao, Jie Lin, Modeling and simulation of microblog-based public health emergency-associated public opinion communication, Inf. Process. Manage. 59 (2) (2022) 102846.

[6] Marek R. Ogiela, Wenny Rahayu, Isaac Woungang, Predictive intelligence in secure data processing, management, and forecasting, Inf. Process. Manage. 59 (3) (2022) 102941.

[7] Szu-Yin Lin, Yun-Ching Kung, Fang-Yie Leu, Predictive intelligence in harmful news identification by BERT-based ensemble learning model with text sentiment analysis, Inf. Process. Manage. 59 (2) (2022) 102872.

[8] Yuan Liu, Ibrahim R. Alzahrani, Refed Adnan Jaleel, Saleh Al Sulaie, An efficient smart data mining framework based cloud internet of things for developing artificial intelligence of marketing information analysis, Inf. Process. Manage. 60 (1) (2023) 103121.

<!-- Meanless: 16-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

[9] ZongKe Bao, Kamarul Faizal Hashim, Alaa Omran Almagrabi, Haslina binti Hashim, Business intelligence impact on management accounting development given the role of mediation decision type and environment, Inf. Process. Manage. 60 (4) (2023) 103380.

[10] Mariana A. Souza, Robert Sabourin, George D.C. Cavalcanti, Rafael M.O. Cruz, A dynamic multiple classifier system using graph neural network for high dimensional overlapped data, Inf. Fusion 103 (2024) 102145.

[11] Imad Afyouni, Zaher Al Aghbari, Reshma Abdul Razack, Multi-feature, multimodal, and multi-source social event detection: A comprehensive survey, Inf. Fusion 79 (2022) 279-308.

[12] Da Lei, Min Xu, Shuaian Wang, A deep multimodal network for multi-task trajectory prediction, Inf. Fusion 113 (2025) 102597.

[13] Xiao Liu, Heyan Huang, Yue Zhang, End-to-end event factuality prediction using directional labeled graph recurrent network, Inf. Process. Manage. 59 (2) (2022) 102836.

[14] Linmei Hu, Juanzi Li, Liqiang Nie, Xiao-Li Li, Chao Shao, What happens next? Future subevent prediction using contextual hierarchical LSTM, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 31, No. 1, 2017.

[15] Snigdha Chaturvedi, Haoruo Peng, Dan Roth, Story comprehension for predicting what happens next, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2017, pp. 1603-1614.

[16] Shengdong Zhang, Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott, Mohak Shah, Deep learning on symbolic representations for large-scale heterogeneous time-series event prediction, in: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, IEEE, 2017, pp. 5970-5974.

[17] Ying Lin, Heng Ji, Fei Huang, Lingfei Wu, A joint neural model for information extraction with global features, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 7999-8009.

[18] Ashutosh Modi, Event embeddings for semantic script modeling, in: Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, 2016, pp. 75-83.

[19] Nan Xu, Hongming Zhang, Jianshu Chen, CEO: Corpus-based open-domain event ontology induction, 2023, arXiv preprint arXiv:2305.13521.

[20] Lei Sha, Sujian Li, Baobao Chang, Zhifang Sui, Joint learning templates and slots for event schema induction, 2016, arXiv preprint arXiv:1603.01333.

[21] Manling Li, Qi Zeng, Ying Lin, Kyunghyun Cho, Heng Ji, Jonathan May, Nathanael Chambers, Clare Voss, Connecting the dots: Event graph schema induction with path language modeling, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP, 2020, pp. ${684} - {695}$ .

[22] Nathanael Chambers, Dan Jurafsky, Unsupervised learning of narrative event chains, in: Proceedings of ACL-08: HLT, 2008, pp. 789-797.

[23] Mark Granroth-Wilding, Stephen Clark, What happens next? event prediction using a compositional neural network model, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 30, No. 1, 2016.

[24] I.-Ta Lee, Dan Goldwasser, Multi-relational script learning for discourse relations, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 4214-4226.

[25] Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, Efficient estimation of word representations in vector space, 2013, arXiv preprint arXiv:1301.3781.

[26] Rachel Rudinger, Pushpendre Rastogi, Francis Ferraro, Benjamin Van Durme, Script induction as language modeling, in: Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1681-1686.

[27] Simon Ahrendt, Vera Demberg, Improving event prediction by representing script participants, in: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016, pp. 546-551.

[28] Karl Pichotta, Raymond Mooney, Learning statistical scripts with LSTM recurrent neural networks, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 30, No. 1, 2016.

[29] Karl Pichotta, Raymond J. Mooney, Using sentence-level LSTM language models for script inference, 2016, arXiv preprint arXiv:1604.02993.

[30] Zhongqing Wang, Yue Zhang, Ching Yun Chang, Integrating order information and event relation for script event prediction, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2017, pp. 57-67.

[31] Shangwen Lv, Wanhui Qian, Longtao Huang, Jizhong Han, Songlin Hu, Sam-net: Integrating event-level and chain-level attentions to predict what happens next, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33, No. 01, 2019, pp. 6802-6809.

[32] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pretraining of deep bidirectional transformers for language understanding, 2018, arXiv preprint arXiv:1810.04805.

[33] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta: A robustly optimized bert pretraining approach, 2019, arXiv preprint arXiv:1907.11692.

[34] Long Bai, Saiping Guan, Jiafeng Guo, Zixuan Li, Xiaolong Jin, Xueqi Cheng, Integrating deep event-level and script-level information for script event prediction, 2021, arXiv preprint arXiv:2110.15706.

[35] Shangwen Lv, Fuqing Zhu, Songlin Hu, Integrating external event knowledge for script learning, in: Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 306-315.

[36] Bo Zhou, Yubo Chen, Kang Liu, Jun Zhao, Jiexin Xu, Xiaojian Jiang, Jinlong Li, Multi-task self-supervised learning for script event prediction, in: Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 3662-3666.

[37] Long Bai, Saiping Guan, Zixuan Li, Jiafeng Guo, Xiaolong Jin, Xueqi Cheng, Rich event modeling for script event prediction, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37, No. 11, 2023, pp. 12553-12561.

[38] Fangqi Zhu, Jun Gao, Changlong Yu, Wei Wang, Chen Xu, Xin Mu, Min Yang, Ruifeng Xu, A generative approach for script event prediction via contrastive fine-tuning, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37, No. 11, 2023, pp. 14056-14064.

[39] Zhongyang Li, Xiao Ding, Ting Liu, Constructing narrative event evolutionary graph for script event prediction, 2018, arXiv preprint arXiv:1805.05081.

[40] Jianming Zheng, Fei Cai, Yanxiang Ling, Honghui Chen, Heterogeneous graph neural networks to predict what happen next, in: Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 328-338.

[41] Li Du, Xiao Ding, Yue Zhang, Kai Xiong, Ting Liu, Bing Qin, A graph enhanced BERT model for event prediction, 2022, arXiv preprint arXiv:2205.10822.

[42] Zikang Wang, Linjing Li, Daniel Zeng, Integrating relational knowledge with text sequences for script event prediction, IEEE Trans. Neural Netw. Learn. Syst. (2023).

[43] Nathanael Chambers, Event schema induction with a probabilistic entity-driven model, in: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013, pp. 1797-1807.

[44] Jackie Chi Kit Cheung, Hoifung Poon, Lucy Vanderwende, Probabilistic frame induction, 2013, arXiv preprint arXiv:1302.4813.

[45] Kiem-Hieu Nguyen, Xavier Tannier, Olivier Ferret, Romaric Besançon, Generative event schema induction with entity disambiguation, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2015, pp. 188-197.

[46] Lifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng Ji, Clare Voss, Jiawei Han, Avirup Sil, Liberal event extraction and event schema induction, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2016, pp. 258-268.

[47] Quan Yuan, Xiang Ren, Wenqi He, Chao Zhang, Xinhe Geng, Lifu Huang, Heng Ji, Chin-Yew Lin, Jiawei Han, Open-schema event profiling for massive news corpora, in: Proceedings of the 27th ACM International Conference on Information and Knowledge Management, 2018, pp. 587-596.

[48] Lifu Huang, Heng Ji, Semi-supervised new event type induction and event detection, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP, 2020, pp. 718-724.

[49] Manling Li, Sha Li, Zhenhailong Wang, Lifu Huang, Kyunghyun Cho, Heng Ji, Jiawei Han, Clare Voss, The future is not one-dimensional: Complex event schema induction by graph modeling for event prediction, in: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 5203-5215.

[50] Xiaomeng Jin, Manling Li, Heng Ji, Event schema induction with double graph autoencoders, in: Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022, pp. 2013-2025.

[51] Rotem Dror, Haoyu Wang, Dan Roth, Zero-shot on-the-fly event schema induction, 2022, arXiv preprint arXiv:2210.06254.

[52] Sha Li, Ruining Zhao, Manling Li, Heng Ji, Chris Callison-Burch, Jiawei Han, Opendomain hierarchical event schema induction by incremental prompting and verification, in: Proc. the 61st Annual Meeting of the Association for Computational Linguistics, ACL2023, 2023.

[53] Piyush Mishra, Akanksha Malhotra, Susan Windisch Brown, Martha Palmer, Ghazaleh Kazeminejad, A graphical interface for curating schemas, in: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, 2021, pp. 159-166.

[54] Fuxian Li, Jie Feng, Huan Yan, Guangyin Jin, Fan Yang, Funing Sun, Depeng Jin, Yong Li, Dynamic graph convolutional recurrent network for traffic prediction: Benchmark and solution, ACM Trans. Knowl. Discov. Data 17 (1) (2023) 1-21.

[55] Ziheng Gao, Zhuolin Li, Lingyu Xu, Jie Yu, Dynamic adaptive spatio-temporal graph neural network for multi-node offshore wind speed forecasting, Appl. Soft Comput. 141 (2023) 110294.

[56] Tiezheng Ma, Yongwei Nie, Chengjiang Long, Qing Zhang, Guiqing Li, Progressively generating better initial guesses towards next stages for high-quality human motion prediction, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 6437-6446.

[57] Wen Guo, Yuming Du, Xi Shen, Vincent Lepetit, Xavier Alameda-Pineda, Francesc Moreno-Noguer, Back to mlp: A simple baseline for human motion prediction, in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023, pp. 4809-4819.

<!-- Meanless: 17-->




<!-- Meanless: H. Rong et al. Information Fusion 117 (2025) 102819-->

[58] Muhan Zhang, Shali Jiang, Zhicheng Cui, Roman Garnett, Yixin Chen, D-vae: A variational autoencoder for directed acyclic graphs, Adv. Neural Inf. Process. Syst. 32 (2019).

[59] Carl Yang, Peiye Zhuang, Wenhan Shi, Alan Luu, Pan Li, Conditional structure generation through graph variational generative adversarial nets, Adv. Neural Inf. Process. Syst. 32 (2019).

[60] Zhuo Lin Li, Gao Wei Zhang, Jie Yu, Ling Yu Xu, Dynamic graph structure learning for multivariate time series forecasting, Pattern Recognit. 138 (2023) 109423.

[61] Ekagra Ranjan, Soumya Sanyal, Partha Talukdar, Asap: Adaptive structure aware pooling for learning hierarchical graph representations, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, No. 04, 2020, pp. 5470-5477.

<!-- Meanless: 18-->

