

<!-- Meanless: Transportation Research Part C 174 (2025) 105081 Contents lists available at ScienceDirect Transportation Research Part C ELSEVIER journal homepage: www.elsevier.com/locate/trc updates-->

# TSGDiff: Traffic state generative diffusion model using multi-source information fusion

Huipeng Zhang ${}^{1}$ , Honghui Dong ${}^{1}$ , Zhiqiang Yang

School of Traffic and Transportation, Beijing Jiaotong University, No. 3 Shangyuan Village, Haidian District, 100044, Beijing, China

Beijing Research Center of Urban Traffic Information Sensing and Service Technologies, Beijing Jiaotong University, No. 3 Shangyuan Village, Haidian District, 100044, Beijing, China

## ARTICLEINFO

Keywords:

Generative model

Traffic state generation

Multi-source information

Conditional diffusion model

Speed prediction

## A B S T R A C T

Accurate analysis and prediction of traffic states are fundamental and crucial for intelligent transportation systems, playing a significant role in enhancing the efficiency and safety of traffic systems. Advances in deep learning have promoted the development of traffic prediction. However, some traditional prediction methods primarily rely on historical traffic data to sequentially predict future traffic trends. While some also incorporate one or more influencing factors, such as weather and day of the week, as covariates, they often lack a unified fusion approach to model the impact of these covariates on future traffic states, and they are prone to error accumulation in long-term predictions. To address these challenges, we propose TSGDiff, a novel traffic state generative diffusion model using multi-source information fusion. The proposed method leverages an innovative diffusion model framework and integrates various sources of information, such as traffic data, weather, and weekdays, to enhance the accuracy of traffic state prediction. TSGDiff transforms historical spatiotemporal information and future environment information into feature representations using an attention-based spatiotemporal extraction module and a traffic semantic encoding module, respectively. These feature representations serve as guiding conditions for the diffusion model to generate traffic states. By incorporating the prediction horizon as an input parameter, TSGDiff directly generates future traffic states point-to-point, thereby avoiding error accumulation inherent in iterative prediction methods. To adapt the diffusion model to graph structure road network data, we introduce a Graph Attention U-Net (GAUNet) to capture the spatial correlations in traffic data. Experiments on real-world Beijing traffic datasets demonstrate that TSGDiff significantly outperforms baseline models for long-term predictions and performs comparably for short-term predictions.

## 1. Introduction

Traffic state refers to the operation status of vehicle flow in a road network over a certain time and spatial scope. Generally, traffic state characterizes the degree of congestion, vehicle speed, and the fluidity of the road network. Accurate prediction of future traffic states is a core function of intelligent transportation systems. Traffic state predictions can assist traffic managers in implementing effective traffic control measures, such as adjusting traffic signal timings and providing traffic guidance, to reduce congestion and improve road efficiency. Additionally, travelers can use predicted traffic states to choose the optimal travel time and route.

---

<!-- Footnote -->

* Corresponding author at: School of Traffic and Transportation, Beijing Jiaotong University, No. 3 Shangyuan Village, Haidian District, 100044, Beijing, China. E-mail addresses: 20114022@bjtu.edu.cn (H. Zhang), hhdong@bjtu.edu.cn (H. Dong), 23111233@bjtu.edu.cn (Z. Yang).

<!-- Footnote -->

---

<!-- Meanless: https://doi.org/10.1016/j.trc.2025.105081 Received 26 July 2024; Received in revised form 24 February 2025; Accepted 26 February 2025 Available online 9 March 2025 0968-090X/(C) 2025 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

<!-- figureText: Truth Speed $\left( {\mathrm{{km}}/\mathrm{h}}\right)$ Truth Prediction prediction 14 - Error 8 12 prediction Large Environment ${t}_{2}$ ... History -30 . -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_1.jpg?x=259&y=155&w=1185&h=425&r=0"/>

Fig. 1. Challenges in traditional traffic prediction.

<!-- Media -->

Most traffic state prediction methods (Lu et al., 2020; Huo et al., 2023) rely on historical data to predict future traffic states, assuming that historical and future data share the same modality. Although these methods have shown promising results in short-term traffic prediction, they face several significant challenges, as shown in Fig. 1. Firstly, they take insufficient account of the multimodal factors influencing traffic states. Traditional methods usually focus on deterministic prediction based on historical data and ignore the randomness and uncertainty of the traffic system. Traffic systems are complex and influenced by a variety of elements, making the accuracy of traffic predictions dependent not only on historical data but also on other uncertain factors (Li et al., 2024b), such as weather conditions, traveler behavior, special events, and traffic control measures. Therefore, accurately modeling these uncertainties becomes crucial. Secondly, these methods rarely consider the long-term characteristics of traffic flow, leading to error accumulation and lower accuracy in long-term traffic predictions (typically beyond ${30}\mathrm{\;{min}}$ ). Peak hours,volatility,daily variations, and seasonal patterns of traffic flow significantly affect the accuracy of prediction models, and limited historical data cannot capture these characteristics adequately. For instance, during peak hours, a sudden heavy rain can cause abrupt changes in traffic conditions. In such scenarios, it is essential to consider both weather conditions and the commuting patterns of traffic flow to address the sudden changes in traffic states. Traditional prediction methods often fail to anticipate the traffic states following such abrupt events. Therefore, developing a traffic state generative model that integrates multiple sources of information is crucial for enhancing the model's adaptability to complex traffic scenarios. In this study, we use "generation" to describe our task, as it goes beyond traditional time-series forecasting. We leverage multi-source information as conditions to guide the diffusion model in generating a range of possible future traffic states, rather than merely extending historical data. Compared to simple time-series forecasting, generative models better capture the randomness and complexity of traffic states, making generation a more fitting term to reflect the core and innovations of our approach.

Several researchers have proposed solutions to the above issues. For instance, Zhang et al. (2023) constructed a dataset that uses textual descriptions to represent traffic states and introduced ChatTraffic, a model that generates traffic states from these textual descriptions. This dataset encompasses various traffic environment information, including weather, time, and unforeseen events. However, the ChatTraffic model does not consider the spatiotemporal features of historical traffic and transforms traffic data into image structures within the model, treating graph-structured road network data as image data. Furthermore, it relies on large language models to extract features from the text, which increases the training cost of the model and makes the quality of traffic generation highly dependent on the performance of the language model. In contrast, we explore a method to generate traffic states by integrating multi-source traffic information and term it as the task of traffic state generation from multi-source traffic information (TI2TS). In the TI2TS task, we use multi-source information that affects future traffic states as inputs to the model, including the spatiotemporal information of historical traffic states and future environment information. The model then outputs the traffic states at future time points. Compared to traditional traffic prediction methods, TI2TS generates road network traffic data from multiple modalities of information.

To transform data of different modalities, we encode multi-source traffic data into a unified representation and decode this representation into the traffic network data space. Generative models offer a promising approach to this decoding process. By using the encoded representation of multi-source information as guiding conditions, we can direct the generative model to produce the results that we expect.

Currently, generative models have achieved tremendous success across various domains. Initially, generative models demonstrated exceptional performance in natural language processing, image processing, and text-to-image translation (Ramesh et al., 2022), and the results often surpass human capabilities. Generally, generative tasks involve creating new data instances based on existing data. These tasks typically require understanding the data distribution and generating similar samples to the training data. Generative tasks manifest differently across various application domains. In image generation, for example, given a set of images, the model generates new images. Generative Adversarial Networks (GANs) (Goodfellow et al., 2014; Patashnik et al., 2021) are known for producing highly realistic images. In text generation, given a set of textual content, the model generates new text, so it can be applied in dialogue systems and article writing. In audio generation, given audio samples, the model generates new audio samples. In time series generation, the model can generate future data, which is particularly useful for traffic flow prediction. Additionally, in multimodal tasks, generative models can produce corresponding images, audio, or graph structures based on the given content.

<!-- Meanless: 2-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

However, the generative tasks face two key challenges. The first is the challenge of generating high-quality realistic data. The generated data should be nearly indistinguishable from real data. For example, in image generation, the generated images should possess high fidelity and detailed characteristics. The second challenge is to ensure both diversity and consistency in the generated results. Diversity and consistency are often opposite goals. In tasks like image or text generation, diversity is usually prioritized to produce a wide range of outputs. However, in generating traffic data, consistency is more important. This is because traffic data generated under the same conditions should be consistent. Addressing these challenges requires generative models to learn the high-level distributional features of the data and incorporate new variations while maintaining these features. This ensures that the generated data is both high-quality and appropriately diverse or consistent, depending on the requirements of the specific task.

Diffusion models are one of the most robust generative models. They not only are easy to train but also exhibit excellent generative capabilities. Furthermore, diffusion models, supported by comprehensive mathematical theories, offer strong interpretability. They can adopt various architectures, including those based on Convolutional Neural Networks (CNNs), Transformers (Vaswani et al., 2017), and attention mechanisms. Generative tasks based on diffusion models (Ho et al., 2020; Rombach et al., 2022) have also been demonstrated to be successful, particularly in multimodal tasks (Saharia et al., 2022). Inspired by these advancements, we propose TSGDiff, a novel traffic state generative diffusion model using multi-source information fusion.

The proposed TSGDiff model generates probabilistic distributions of future traffic states using a diffusion model, rather than producing a single deterministic prediction. The uniqueness of the diffusion model lies in its ability to capture the inherent randomness in traffic states. By inputting multi-source information as a condition, it generates diverse distributions of future traffic states. This approach not only enhances prediction accuracy but also better addresses uncertainty, particularly in long-term prediction. The core role of the diffusion model in this study is to reflect the complexity and diversity of traffic states through probabilistic modeling, which is especially beneficial in long-term predictions with greater uncertainty. Other components of our framework are also crucial. We utilize an attention-based spatiotemporal neural network module to extract the spatial and temporal dependencies from historical traffic data, enabling the model to understand the patterns in past traffic states. The environment semantic encoding module captures the influence of different traffic environment factors on traffic states. The prediction step embedding module inputs the prediction horizon information point by point into the model, mitigating the issue of error accumulation in long-term prediction. By combining these modules with the probabilistic nature of the diffusion model, the overall framework significantly improves the accuracy of long-term predictions. We employ a graph attention UNet (GAUNet) for diffusion denoising, ensuring that the topological information of the road network is preserved throughout the diffusion process. Additionally, we explore the generative principles of diffusion models on traffic networks and the roles of each module in TSGDiff, to enhance the model's interpretability. The diffusion process in TSGDiff models the uncertainty of traffic states, enabling better generalization and more robust predictions. This provides strong support for traffic management and planning.

In summary, the contributions of this paper are as follows:

1. TSGDiff Model: We propose the TSGDiff model, a diffusion model based on multi-source information for generating traffic states. This model addresses the issues of insufficient consideration of environment information and the failure of long-term traffic predictions. The model uses multi-source traffic information as guiding conditions to generate a probability distribution of future traffic states and directly inputs the prediction horizon as a parameter input to mitigate error accumulation.

2. Multi-Source Information Integration:Our method integrates multi-source information. Historical traffic information features are extracted using an attention spatiotemporal module (AST-Module), and the importance of different environment attributes is measured using a multi-head linear attention mechanism.

3. GAUNet Module: TSGDiff combines graph attention networks (Velickovic et al., 2017) with UNet (Ronneberger et al., 2015) to form the GAUNet. GAUNet retains the complete graph structure information and the uncertainty of traffic states within the diffusion model, enhancing the robustness of traffic state generation.

4. Experiment Validation: We conduct experiments on real-world traffic datasets, and specify the task of traffic state generation as speed prediction. TSGDiff achieves comparable performance to traditional models in short-term predictions and significantly better results in long-term predictions. In addition, we conduct detailed experiments to verify the functions of each module in TSGDiff.

The structure of the remainder of this paper is as follows: Section 2 reviews related work on traffic state generation. Section 3 introduces relevant preliminaries and the theoretical framework of the traffic conditional diffusion model. Section 4 presents the proposed model TSGDiff. Section 5 describes the experiments and evaluations. Section 6 discusses the ablation study, case study, and data selection. Section 7 provides conclusions.

## 2. Related work

### 2.1. Generative model

Currently, the field of generative models is advancing. The goal of generative models is to learn the probability distribution of a given dataset to generate new data samples. The main types of generative models include Variational Autoencoders (VAEs) (Kingma and Welling, 2013), Flow-based Models (Dinh et al., 2016; Kingma and Dhariwal, 2018), Generative Adversarial Networks (GANs) (Patashnik et al., 2021; Wu et al., 2020), and Diffusion Models (Ho et al., 2020; Rombach et al., 2022; Nichol and Dhariwal, 2021).

<!-- Meanless: 3-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

VAEs are self-supervised neural networks designed to learn efficient representations of original data. VAEs mainly work to learn an identity decoding function $x \approx  f\left( x\right)  = {D}_{\theta }\left( {{E}_{\phi }\left( x\right) }\right)$ ,which consists of two parts: an encoder ${E}_{\phi }$ and a decoder ${D}_{\theta }$ . The encoder maps the real data to a latent space, and the decoder reconstructs the data from the latent space. This allows for data compression, reconstruction as well as generation of new data. VAEs usually assume that the data follows a specific distribution, which may limit its modeling ability for complex traffic data.

Flow-based models use probabilistic flows to describe data distribution and require all model transformations to be reversible, which restricts the development of flow-based models. These models typically comprise two components: a transformation network $F$ that maps input data to the latent space and an inverse transformation network ${F}^{-1}$ that maps samples from the latent space back to the data space. Both networks are optimized simultaneously during training to minimize the difference between the generated and real data.

GANs introduce an adversarial training framework involving a generator and a discriminator to fit the real data distribution. The generator creates sample data, while the discriminator assesses the authenticity of these samples. Both components are engaged in adversarial training to enable the generator to produce high-quality data. However, GANs usually require careful adjustment of hyperparameters and often encounter issues of training instability and mode collapse. In contrast, VAEs provide more stable training but typically produce lower-quality outputs with less detail than GANs.

Diffusion models can better capture the diversity and complexity in traffic states through their step-by-step reverse denoising generation process. The models can generate multiple possible future state distributions and effectively handle uncertainties. During training, the models receive noisy observed data and generate samples that match the real data distribution through multi-step diffusion. Compared to VAEs, flow-based models, and GANs, diffusion models exhibit stronger generative capabilities, more stable training (Dhariwal and Nichol, 2021), and can generate multi-modal probability distributions more stably, making them the most robust generative models available today. Additionally, diffusion models offer good interpretability, allowing for a deeper understanding of the sample generation process. Diffusion models have been promising in various tasks, including image generation (Rombach et al., 2022; Saharia et al., 2022), video generation (Luo et al., 2023; Ho et al., 2022), text generation (Gong et al., 2022; Li et al., 2022), speech synthesis (Huang et al., 2022; Shen et al., 2023), graph generation (Vignac et al., 2022), and spatiotemporal prediction (Wen et al., 2023; Yuan et al., 2023).

These generative models offer a promising approach for generating traffic states. Inspired by multimodal generative tasks, we propose a diffusion model-based method that leverages multi-source information to correlate traffic states. This generative approach incorporates various traffic environment factors with historical traffic features, making it a more robust model for long-term traffic prediction.

### 2.2. Traffic prediction

Traffic prediction is crucial in urban planning and management. It plays a significant role in alleviating congestion and enhancing road usage efficiency. Traffic prediction methodologies can be categorized into traditional statistical learning methods, machine learning approaches, and deep learning models.

Traditional statistical learning methods include the historical average method (HA) (Liu et al., 2019), autoregressive integrated moving average (ARIMA) models (Duan et al., 2016), and linear regression models (Rice and Van Zwet, 2004). However, since traffic data is inherently nonlinear, these statistical learning methods, which are primarily based on prior knowledge and simple mathematical statistics, face significant challenges in accurately predicting traffic states. Machine learning approaches alleviate some of the issues inherent in traditional statistical methods. Approaches such as gradient boosting decision trees (GBDT), support vector machines (SVM) (Hong, 2011), hidden Markov models (HMM) (Jiang and Fei, 2016), Markov chains (Shin and Sunwoo, 2018), and Kalman filter-based models (Van Hinsbergen et al., 2011) are capable of capturing the nonlinear relationships in traffic data. Nevertheless, traffic data exhibits both temporal and spatial characteristics, and the aforementioned methods only focus on temporal features, and fail to model spatial dependencies.

In contrast, deep learning models address the spatiotemporal modeling of traffic data, integrating both temporal and spatial features (Chen et al., 2020; Ke et al., 2017; Zhao et al., 2019). Typically, recurrent neural networks (RNNs) are employed to extract temporal features, while convolutional neural networks (CNNs) and graph neural networks (GNNs) are utilized to capture spatial features (Jia et al., 2021). For instance, to predict lane-level short-term traffic speed, Lu et al. (2020) proposed a mixed deep learning model (MDL) that integrates convolutional long short-term memory layers (Conv-LSTM), convolutional layers, and fully connected layers. Yang et al. (2021) introduced a path-based speed prediction neural network (PSPNN), which consists of CNNs and bidirectional LSTMs (Bi-LSTM) to extract the spatiotemporal correlations of historical data. Guo et al. (2019b) developed spatiotemporal 3D convolutional neural networks (ST-3DNet), which incorporate 3D convolutions to capture the spatiotemporal features in traffic data. However, these methods utilize CNNs to extract spatial correlations, which are suited for Euclidean space and thus may not effectively handle non-Euclidean space. Recent research has extended CNNs to GNNs to process data with arbitrary graph structures. Spatio-temporal graph convolutional networks (STGCN) (Yu et al., 2017) and diffusion convolutional recurrent neural networks (DCRNN) (Li et al., 2017) were among the first traffic flow prediction frameworks based on graph convolutional networks (GCN). Subsequent works such as T-GCN (Zhao et al., 2019), STSGCN (Song et al., 2020a), ASTGCN (Guo et al., 2019a), Graph WaveNet (GWN) (Wu et al., 2019), ST-GRAT (Park et al., 2020), ADSTGCN (Zhao et al., 2022), GMAN (Zheng et al., 2020), GMHANN (Wang et al., 2023), MVSTG (Wei et al., 2024), PGCN (Shin and Yoon, 2024), and SFGCN (Li et al., 2024a) have further explored spatiotemporal relationships from various perspectives. Feng et al. (2023) proposed a macro-micro spatiotemporal neural network (MMSTNet) that captures the correlations between different regions of the road network. However, the restricted parallel processing capacity of RNNs hinders their effectiveness in learning long sequences. In traffic flow prediction tasks, Cai et al. (2020) applied Google's Transformer architecture, initially designed for machine translation, to traffic flow prediction, because it can capture the continuity and periodicity of time series. Zou et al. (2023) proposed a spatiotemporal generative inference network (STGIN) that avoids the propagation of long-term prediction errors across both temporal and spatial dimensions. Lei et al. (2024) introduced a conditional diffusion framework with a spatiotemporal estimator (CDSTE) to address the issue of traffic state estimation at sensor-free locations. Yan et al. (2024) introduced a progressive space-time self-attention model (ProSTformer) to capture the spatiotemporal correlations in traffic prediction.

<!-- Meanless: 4-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

In general time series prediction, Daiya and Lin (2021) proposed a multimodal deep learning prediction architecture using dilated causal convolutions and Transformer blocks for feature extraction from both data sources. Wu et al. (2020) introduced a GAN-based time series forecasting model that adopts a sparse Transformer as the generator to learn sparse attention maps and uses a discriminator to improve the prediction performance at a sequence level. Liu et al. (2024) proposed iTransformer, a model that innovatively adjusts the Transformer architecture to address issues related to performance, computation, and representation learning in traditional Transformers for time series prediction. Additionally, a series of Transformer-based variants, such as Scaleformer (Shabani et al., 2023), Pathformer (Chen et al., 2024), and MTST (Zhang et al., 2024), have been developed. Wang et al. (2024) introduced TimeMixer, which offers a novel perspective based on multiscale mixing. It utilizes decomposed multiscale sequences for information extraction and prediction, and achieves state-of-the-art performance in both long- and short-term forecasting tasks while maintaining good operational efficiency.

Although sequential models like Transformer perform well in capturing long-term dependencies, they usually generate a single deterministic prediction, and cannot naturally produce diverse future traffic states. In contrast, diffusion models are more flexible and efficient in conditional modeling. By utilizing conditional generation, diffusion models can create diverse distributions of future traffic states under various conditions, better handling potential variations and addressing real-world uncertainties more effectively.

Deep learning-based models effectively capture the spatiotemporal dependencies in traffic data and have demonstrated commendable performance in short-term traffic prediction. However, long-term prediction remains a significant challenge. Additionally, these methods predominantly rely on traffic data to predict traffic states, often overlooking the impact of multi-source information on traffic data. This paper aims to leverage multi-source information to generate future traffic states, thereby enhancing the model's capability for long-term prediction.

## 3. Methodology

### 3.1. Preliminaries

#### 3.1.1. Traffic state

Traffic states on a road network are typically characterized by parameters such as flow, speed, density, and occupancy rate, which reflect the level of congestion and the efficiency of vehicle movement. Traffic states can be classified into four categories: (1)Free flow, where there are few vehicles and they travel at desired speeds, showing low traffic flow and no congestion. (2)Stable flow, where traffic flow is moderate, vehicles maintain large distances from each other, with minimal traffic delay, and vehicle speeds are close to those in the free flow state. (3)Moderate congestion occurs when the number of vehicles exceeds road capacity, leading to decreased traffic flow, lower vehicle speeds, and stop-and-go conditions. (4)Severe congestion, where the number of vehicles reaches the maximum capacity of the road segment, distances between vehicles become minimal, vehicle speeds are extremely low, and movement is nearly impossible, potentially causing traffic gridlock.

In this paper, we generate traffic states by modeling vehicle speeds on the road. The corresponding relationship between different traffic states and vehicle speeds $v\left( {\mathrm{\;{km}}/\mathrm{h}}\right)$ are as follows: Free flow: ${60} \leq  v$ ; Stable flow: ${40} \leq  v < {60}$ ; Moderate congestion: ${20} \leq  v < {40}$ ; Severe congestion: $0 \leq  v < {20}$ .

#### 3.1.2. Traffic network

In this paper,we define the traffic network as a graph $G = \left( {V,E,X,A}\right)$ ,where $v \in  V$ represents the nodes and $e \in  E$ represents the edges. We define the road segments in the actual traffic network as the nodes of $G$ ,and the intersections in the network as the edges of $G$ . The number of road segments is denoted by $N = \left| V\right|$ . The matrix $X \in  {\mathbf{R}}^{N \times  d}$ represents the traffic state attributes of the nodes,with $d$ being the dimension of these attributes. $G$ is a directed graph and ${e}_{l,j}$ indicates a directed edge connecting the ordered pair of nodes $\left( {{v}_{i},{v}_{j}}\right)$ . The adjacency matrix of graph $G$ is represented by $A \in  {\mathbf{R}}^{N \times  N}$ . If nodes ${v}_{i}$ and ${v}_{j}$ are connected, ${e}_{i,j} \in  E$ and ${A}_{i,j} = 1$ . Otherwise, ${e}_{i,j} \notin  E$ and ${A}_{i,j} = 0$ .

#### 3.1.3. Traffic environment information

We define the environment information as the factors that influence traffic states. These factors are derived from residents' travel behaviors and include time, weather conditions, peak hours, day of the week, workday and holiday, and traffic control measures. We divide the time of day into 288 five-minute intervals. Weather conditions are categorized into sunny, rainy, snowy, cloudy, etc. Peak hours are segmented according to travel time. The day of the week is classified into seven categories, from Monday to Sunday. For traffic control measures,we consider the odd-even license plate policy in Beijing. For example,at 8 AM on Thursday,21 March 2024, in Beijing, a weekday, the weather is sunny, and it is during the morning peak hour. This day, vehicles with license plates ending in 4 and 9 are restricted. The environment information used in this paper is summarized in Table 1.

<!-- Meanless: 5-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

Table 1

Traffic environment information.

<table><tr><td>Environment information</td><td>Description</td></tr><tr><td>Day of the week</td><td>Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday</td></tr><tr><td>Workday</td><td>Workday, holiday</td></tr><tr><td>Peak hour</td><td>Free flow, morning peak, off-peak, evening peak</td></tr><tr><td>Time</td><td>288 five-minute intervals in one day</td></tr><tr><td>Weather</td><td>Sunny, rainy, snowy, cloudy, hazy, ...</td></tr><tr><td>Traffic control measures</td><td>The odd-even license plate ban policy in Beijing</td></tr></table>

<!-- figureText: Add Noise ${G}_{t}$ ... ${G}_{T}$ Reverse: $\overline{{p}_{\theta }}\left( {{\bar{G}}_{t - 1} \mid  {\bar{G}}_{t};\bar{c}}\right)$ 300 400 500 ${G}_{\bar{0}}$ $G$ Denoise with Condition 100 80 60 40 20 Actual Data 100 200 -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_5.jpg?x=332&y=467&w=1037&h=769&r=0"/>

Fig. 2. The diagram of traffic conditional denoising diffusion framework. The upper section depicts the diffusion and the conditional reverse diffusion process on the graph $G$ . The lower section visualizes the diffusion process of the three traffic states and shows the diffusion results at 100,200,300,400,and 500 time steps.

<!-- Media -->

#### 3.1.4. Problem formulation

In this paper, we define traffic state generation as generating future traffic speeds on a road network, given certain historical traffic flow information and traffic environment information. The traffic state generation problem can be expressed with the following formula:

$$
{\mathbf{T}}_{t + Q} \rightarrow  {Y}_{t + Q} = f\left( {{X}_{\left( {t - P}\right)  : t};{\mathbf{E}}_{t + Q};Q}\right)  \tag{1}
$$

where, ${X}_{\left( {t - P}\right)  : t} = \left\{  {{X}_{t - P},{X}_{t - P + 1},\ldots ,{X}_{t}}\right\}   \in  {\mathbf{R}}^{N \times  C \times  P}$ denotes the historical traffic flow sequence observed over a duration $P$ on the road network $G$ . The traffic flow at time $t$ is represented by ${X}_{t} \in  {\mathbf{R}}^{N \times  C}$ ,with a feature dimension of $C$ for $N$ nodes. In this study, $C = 1$ indicates the speed of the nodes. ${\mathbf{E}}_{t + Q}$ represents the traffic environment factors at time $t + Q$ ,where $Q$ is the prediction horizon. ${Y}_{t + Q} \in  {\mathbf{R}}^{N \times  C}$ and ${\mathbf{T}}_{t + Q}$ denote the generated traffic speed and the traffic state at time $t + Q$ ,respectively. Our objective is to train a function $f\left( \cdot \right)$ to integrate multi-source information to predict ${Y}_{t + Q}$ . We employ a diffusion model to facilitate this function.

### 3.2. Traffic conditional denoising diffusion framework

The initial diffusion models were designed to unconditionally generate images from white noise, which did not align with our task. Our objective is to generate future traffic states based on existing traffic information. Therefore, we modified the original diffusion model by including conditional guidance. Fig. 2 outlines the process of the traffic conditional denoising diffusion framework. It consists of two Markov chains: the forward diffusion process and the conditional reverse diffusion process. Throughout both processes, the topology of the graph G remains unchanged.

- Forward Diffusion Process: This process transitions ${G}_{0}$ to ${G}_{T}$ ,where ${G}_{0}$ represents the true traffic state of the road network, and ${G}_{T} \sim  \mathcal{N}\left( {0,I}\right)$ represents an independent Gaussian distribution (completely disordered state). The process ${G}_{0 : T}$ involves progressively adding noise to the original data.

<!-- Meanless: 6-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

- Conditional Reverse Diffusion Process: Under the guidance of traffic information condition $c$ ,this process transitions ${G}_{T}$ to ${G}_{0}$ . This represents the transformation from a completely disordered state back to an ordered state,essentially recovering the original data distribution from the Gaussian distribution. During inference, only the conditional reverse diffusion process is used.

We employ a diffusion model to capture the probability distribution of future traffic states. Unlike traditional deterministic predictions, the diffusion model generates multiple possible future traffic states by gradually reversing the noise-to-data generation process. This probability-based generative approach allows the model not only to produce a single prediction result but also to generate diverse traffic states under varying conditions, thereby better reflecting the randomness and uncertainty present in real-world scenarios.

#### 3.2.1. Forward diffusion process

Given an initial traffic state distribution ${G}_{0} \sim  q\left( G\right)$ ,Gaussian noise is incrementally added to the distribution over a total of $T$ steps,and a noisy sample sequence ${G}_{1},\ldots {G}_{t},\ldots ,{G}_{T}$ is generated. The variance of the Gaussian noise is determined by a fixed value ${\beta }_{t}\left( \left\{  {{\beta }_{t} \in  \left( {0,1}\right) ,t = 1,\ldots ,T}\right\}  \right)$ ,and the mean is determined by both ${\beta }_{t}$ and the data ${G}_{T}$ at the current time step $t$ . As $t$ increases, ${\beta }_{t}$ also increases,denoted as ${\beta }_{1} < {\beta }_{2} < \cdots  < {\beta }_{T}$ . Based on the Markov chain,the following formulas can be derived:

$$
q\left( {{G}_{t} \mid  {G}_{t - 1}}\right)  = \mathcal{N}\left( {{G}_{t};\sqrt{1 - {\beta }_{t}}{G}_{t - 1},{\beta }_{t}\mathbf{I}}\right)  \tag{2a}
$$

$$
q\left( {{G}_{1 : T} \mid  {G}_{0}}\right)  = \mathop{\prod }\limits_{{t = 1}}^{T}q\left( {{G}_{t} \mid  {G}_{t - 1}}\right)  \tag{2b}
$$

Eq. (2) expresses the distribution of ${G}_{t}$ given ${G}_{t - 1}$ ,where $\mathcal{N}$ denotes a Gaussian distribution with mean $\sqrt{1 - {\beta }_{t}}{G}_{t - 1}$ and variance ${\beta }_{t}\mathbf{I}$ . As $t$ continues to increase,the final network state distribution ${G}_{T}$ approximates an isotropic Gaussian distribution. Unlike VAEs, the posterior distribution $q\left( {{G}_{1 : T} \mid  {G}_{0}}\right)$ in diffusion probabilistic models does not require training but is instead fixed in a Gaussian transition process.

Eq. (2a) is time-consuming for calculating the ${G}_{t}$ at $t$ . However,by leveraging the reparameterization trick of Gaussian distributions (Kingma and Welling,2013), $q\left( {G}_{i}\right)$ can be derived entirely based on ${G}_{0}$ and ${\beta }_{i}$ ,eliminating the need for a step-by-step forward iteration process. The formula is derived as follows:

$$
{G}_{t} = \sqrt{{\bar{\alpha }}_{t}}{G}_{0} + \sqrt{1 - {\bar{\alpha }}_{t}}\bar{z} \tag{3}
$$

where, ${\alpha }_{t} = 1 - {\beta }_{t},{\bar{\alpha }}_{t} = \mathop{\prod }\limits_{{i = 1}}^{T}{\alpha }_{i}$ ,and $\bar{z} \sim  \mathcal{N}\left( {0,1}\right)$ . With the reparameterization trick,it can be obtained that $q\left( {{G}_{t} \mid  {G}_{0}}\right)  \sim$ $\mathcal{N}\left( {\sqrt{{\bar{\alpha }}_{t}}{G}_{0},\left( \sqrt{1 - {\bar{\alpha }}_{t}}\right) \mathbf{I}}\right)$ .

#### 3.2.2. Posterior diffusion conditional probability

The posterior conditional probability $q\left( {{G}_{t - 1} \mid  {G}_{t},{G}_{0}}\right)$ of the Gaussian distribution in the forward diffusion process is as follows:

$$
q\left( {{G}_{t - 1} \mid  {G}_{t},{G}_{0}}\right)  = \mathcal{N}\left( {{G}_{t - 1};\widetilde{\mu }\left( {{G}_{t},{G}_{0}}\right) ,{\widetilde{\beta }}_{t}\mathbf{I}}\right) 
$$

$$
 \propto  \exp \left( {-\frac{1}{2}\left( {\left( {\frac{{\alpha }_{t}}{1 - {\alpha }_{t}} + \frac{1}{1 - {\bar{\alpha }}_{t - 1}}}\right) {G}_{t - 1}^{2} - \left( {\frac{2\sqrt{{\alpha }_{t}}}{1 - {\alpha }_{t}}{x}_{t} + \frac{2\sqrt{{\bar{\alpha }}_{t - 1}}}{1 - {\bar{\alpha }}_{t - 1}}{x}_{0}}\right) {G}_{t - 1} + C\left( {{G}_{t},{G}_{0}}\right) }\right) }\right)  \tag{4}
$$

where, $C\left( {{G}_{t},{G}_{0}}\right)$ is independent of ${G}_{t - 1}$ and can be considered a constant,thus not taken into account in this context.

Based on Eq. (3),we can derive the following estimations for the variance ${\widetilde{\beta }}_{t}$ and mean ${\widetilde{\mu }}_{t}$ :

$$
{\widetilde{\beta }}_{t} = \frac{1 - {\bar{\alpha }}_{t - 1}}{1 - {\bar{\alpha }}_{t}}{\beta }_{t} \tag{5a}
$$

$$
{\widetilde{\mu }}_{t}\left( {{G}_{t},{G}_{0}}\right)  = \frac{1}{\sqrt{{\alpha }_{t}}}\left( {{G}_{t} - \frac{{\beta }_{t}}{\sqrt{1 - {\bar{\alpha }}_{t}}}{z}_{t}}\right)  \tag{5b}
$$

From Eq. (5),we can deduce that the variance ${\widetilde{\rho }}_{t}$ of $q\left( {{G}_{t - 1} \mid  {G}_{t},{G}_{0}}\right)$ is a constant. The estimation of the mean ${\widetilde{\mu }}_{t}$ does not contain ${G}_{0}$ but includes an additional noise ${z}_{t}$ . This implies that,given ${G}_{0}$ ,the mean of the posterior diffusion conditional Gaussian distribution depends only on ${G}_{t}$ and ${z}_{t}$ . This forms the basis for designing neural networks to fit the distribution of diffusion models.

#### 3.2.3. Conditional reverse diffusion process

The conditional reverse diffusion process involves recovering the original road network state ${G}_{0}$ from ${G}_{T} \sim  \mathcal{N}\left( {0,I}\right)$ using the traffic information $c$ . We assume that each sampling process follows a Gaussian distribution $p\left( {{G}_{i - 1} \mid  {G}_{i};c}\right)$ . However,it is impractical to fit this distribution step by step because it would require sampling over the entire dataset. Therefore, we need to construct a parameterized distribution ${p}_{\theta }$ for estimation. The reverse diffusion process remains a Markov process,as described by Eq. (6).

$$
{p}_{\theta }\left( {{G}_{0 : T};c}\right)  = p\left( {{G}_{T};c}\right) \mathop{\prod }\limits_{{t = 1}}^{T}{P}_{\theta }\left( {{G}_{t - 1} \mid  {G}_{t};c}\right)  \tag{6a}
$$

$$
{p}_{\theta }\left( {{G}_{t - 1} \mid  {G}_{t};c}\right)  = \mathcal{N}\left( {{G}_{t - 1};{\mu }_{\theta }\left( {{G}_{t},t,c}\right) ,{\sigma }_{\theta }\left( {{G}_{t},t,c}\right) }\right)  \tag{6b}
$$

where, ${\mu }_{\theta }$ and ${\sigma }_{\theta }$ are both related to $\theta$ ,with their inputs being $\left( {{G}_{t},t,c}\right)$ .

<!-- Meanless: 7-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

#### 3.2.4. Likelihood function for the target data distribution

To ensure that the reverse diffusion process ${p}_{\theta }\left( {{G}_{t - 1} \mid  {G}_{t};c}\right)$ aligns with the true distribution,TSGDiff employs maximum likelihood estimation to estimate $\theta$ . By adding a KL divergence term to the negative log-likelihood,we establish an upper bound for the negative log-likelihood, as shown in Eq. (7).

$$
 - \log {p}_{\theta }\left( {{G}_{0};c}\right)  \leq   - \log {p}_{\theta }\left( {{G}_{0};c}\right)  + {D}_{KL}\left( {q\left( {{G}_{1 : T} \mid  {G}_{0}}\right) \parallel {p}_{\theta }\left( {{G}_{1 : T} \mid  {G}_{0};c}\right) }\right)  \tag{7}
$$

By taking the expectation with respect to $q\left( {G}_{0}\right)$ on both sides of Eq. (7),we obtain:

$$
 - {\mathbf{E}}_{q\left( {G}_{0}\right) }\log {p}_{\theta }\left( {{G}_{0};c}\right)  \leq  {\mathbf{E}}_{q\left( {G}_{0 : T}\right) }\left( {\log \frac{q\left( {{G}_{1 : T} \mid  {G}_{0}}\right) }{{p}_{\theta }\left( {{G}_{0 : T};c}\right) }}\right) 
$$

$$
 = {\mathbf{E}}_{q}\left( \underset{{L}_{T}}{\underbrace{{D}_{KL}\left( {q\left( {{G}_{T} \mid  {G}_{0}}\right) \parallel \log {p}_{\theta }\left( {{G}_{T} : c}\right) }\right) }}\right)  + \mathop{\sum }\limits_{{t = 2}}^{T}\underset{{L}_{t - 1}}{\underbrace{{D}_{KL}\left( {q\left( {{G}_{t - 1} \mid  {G}_{t},{G}_{0}}\right) \parallel {p}_{\theta }\left( {{G}_{t - 1} \mid  {G}_{t};c}\right) }\right) }} - \underset{{L}_{0}}{\underbrace{\log {p}_{\theta }\left( {{G}_{0} \mid  {G}_{1};c}\right) }} \tag{8}
$$

where, ${L}_{T}$ is a constant and does not contain trainable parameters. We set the variance of ${p}_{\theta }\left( {{G}_{t - 1} \mid  {G}_{t};c}\right)$ to a constant related to ${\beta }_{t}$ , and we can derive the following equation in combination with Eqs. (4)-(6):

$$
{L}_{t - 1} - C = {\mathbf{E}}_{t,{G}_{0},\varepsilon }\left( {\frac{1}{2{\sigma }_{t}^{2}}{\begin{Vmatrix}\frac{1}{\sqrt{{\bar{\alpha }}_{t}}}\left( {G}_{t} - \frac{{\beta }_{t}}{\sqrt{1 - {\bar{\alpha }}_{t}}}\varepsilon \right)  - {\mu }_{\theta }\left( {G}_{t},t,c\right) \end{Vmatrix}}^{2}}\right)  \tag{9}
$$

where, $C$ is a constant,and we let the output of the prediction network equal $\varepsilon$ . In practice,we can omit the parameter terms to stabilize training (Ho et al., 2020), so Eq. (9) can be written in the following loss function:

$$
\mathop{\min }\limits_{\theta }\mathcal{L} = \mathop{\min }\limits_{\theta }{\mathbf{E}}_{t,{G}_{0},\varepsilon }\left( {\begin{Vmatrix}\varepsilon  - {\varepsilon }_{\theta }\left( \sqrt{{\bar{\alpha }}_{t}}{G}_{0} + \sqrt{1 - {\bar{\alpha }}_{t}}\varepsilon ,t,c\right) \end{Vmatrix}}^{2}\right)  \tag{10}
$$

Once training is complete,we can generate ${G}_{0}$ from Gaussian noise $\mathcal{N}\left( {0,I}\right)$ through an iterative sampling process based on Eq. (6). We first sample from the most noise-perturbed distribution and then gradually reduce the noise scale until we reach the final noise-free sample.

#### 3.2.5. Training and generating

The training and generating processes of TSGDiff are detailed in Algorithm 1 and Algorithm 2, respectively. In the traffic state generation task, for the generated results to describe the uncertainty of traffic states, we average the results after multiple samplings.

<!-- Media -->

## Algorithm 1 TSGDiff Training

---

Input: Denoising model ${\varepsilon }_{\theta }$ ,Diffusion step $T$ ,Traffic state data distribution $q\left( {G}_{0}\right)$
Output: The trained ${\varepsilon }_{\theta }$
		repeat
			${G}_{0} \sim  q\left( {G}_{0}\right)$
			The multi-source information guiding condition $c$ corresponding to ${G}_{0}$
			Randomly sample a $t,t \sim  \operatorname{Uniform}\{ 1,\cdots ,T\}$
			$\varepsilon  \sim  \mathcal{N}\left( {0,I}\right)$
			Compute the error $\mathcal{L} = {\mathbf{E}}_{t,{G}_{0},\varepsilon }\left( {\begin{Vmatrix}\varepsilon  - {\varepsilon }_{\theta }\left( \sqrt{{\bar{\alpha }}_{t}}{G}_{0} + \sqrt{1 - {\bar{\alpha }}_{t}}\varepsilon ,t,c\right) )\end{Vmatrix}}^{2}\right)$
			Update the gradients ${\nabla }_{\theta }\mathcal{L}$
		until ${\varepsilon }_{\theta }$ converged
		return ${\varepsilon }_{\theta }$

---

<!-- Media -->

## 4. Traffic state generative diffusion model

Below, we provide a detailed description of the carefully designed model structure of TSGDiff.

### 4.1. Overview of TSGDiff

Given the historical spatiotemporal traffic information, traffic environment information, and the prediction step as guiding conditions $c$ ,TSGDiff performs a step-by-step denoising process on ${G}_{T}$ ,predicts the noise $\varepsilon$ at each step and ultimately obtains the noise-free ${G}_{0}$ . We provide an overview of TSGDiff in Fig. 3. It consists of two parts. The upper part of the figure is the multi-source information fusion module, which integrates multi-source information into guiding conditions for the reverse diffusion process. The lower part is the conditional denoising network,the core of which is GAUNet to predict the noise $\varepsilon$ .

In the multi-source information fusion module, we use an attention-based spatiotemporal network (AST-Module), time embedding, and semantic encoding to extract the spatiotemporal features of the traffic state, prediction horizon, and traffic environment factors, respectively. TSGDiff effectively maintains the spatiotemporal relationships in the data and directly generates future traffic states at time $Q$ point-to-point. The following subsections provide detailed explanations of these stages.

<!-- Meanless: 8-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

Algorithm 2 TSGDiff Generating

---

Input: Denoising model ${\varepsilon }_{\theta }$ ,Traffic information condition $c$ ,Diffusion step $T$ ,Initial sample ${G}_{T} \sim  \mathcal{N}\left( {0,I}\right)$ ,Number of sampling $N$
Output: Future traffic state ${G}_{0}$
		for $n = 1$ to $N$ do
			for $t = T$ to 1 do
				if $t > 1$ then
					$z \sim  \mathcal{N}\left( {0,I}\right)$
				else
					$z = 0$
				end if
				${G}_{n,t - 1} = \frac{1}{\sqrt{{\alpha }_{t}}}\left( {{G}_{n,t} - \frac{{\beta }_{t}}{\sqrt{1 - {\bar{\alpha }}_{t}}}{\varepsilon }_{\theta }\left( {{G}_{n,t},t,c}\right) }\right)  + z$
			end for
		end for
		${G}_{0} = \frac{1}{N}\mathop{\sum }\limits_{{n = 1}}^{N}{G}_{n,0}$
		return

---

<!-- figureText: History Traffic Environment: Prediction Step Thursday Workay Morning Peak $8 : {00}\mathrm{{am}}$ ${e}_{5}$ Sunny Ban 4&9 TimeEmbedding TE Semantic Encoding ${SE}$ MultiHead Linear Attention & Group Norm GAUNet ${G}_{t - 1}$ Noise $\varepsilon$ Future Grop Norm [ SiLU ${X}_{t - P}{X}_{t - P + 1}$ Time AST-Module ${ST}$ Condition MultiHead ${\widetilde{G}}_{t}$ Linear Attention ResGAT Denoising Time Gaussian Step Embeding Noise TE ResGAT GAT (GAT -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_8.jpg?x=259&y=756&w=1187&h=880&r=0"/>

Fig. 3. Overview of the TSGDiff framework. The upper part is the multi-source information fusion module, and the lower part is the conditional denoising network.

<!-- Media -->

### 4.2. Multi-source information fusion

##### 4.2.1.AST module

To capture the spatiotemporal dependencies in historical traffic data, we designed an attention-based spatiotemporal feature extraction module (AST-Module), as illustrated in Fig. 4a. Firstly, we use ResGAT to extract spatial features at different time points. Subsequently, we employ LSTM to capture temporal features, incorporating skip connections within the LSTM. Finally, a multi-head linear attention mechanism (Shen et al., 2021) is utilized to fuse the features from different time points.

To ensure that TSGDiff maintains the topological structure of the road network, we design a Residual Graph Attention (ResGAT) layer for extracting spatial features. This layer consists of two Graph Attention Networks (GAT) layers (Velickovic et al., 2017), Group Normalization (Wu and He, 2018), and SiLU activation function. ResGAT effectively captures the non-uniform dependencies between road segments. Traffic patterns in a segment are often influenced by neighboring segments, but the strength of these influences can vary significantly. The attention mechanism in ResGAT dynamically assigns importance to these dependencies, making ResGAT an ideal choice for modeling spatial interactions within a road network. The multi-head attention mechanism allows ResGAT to focus on various patterns between road segments. Additionally, the residual connections in ResGAT ensure better gradient flow during training, helping to avoid the vanishing gradients common in deep networks. This feature is particularly useful for extracting long-range spatial dependencies without compromising performance. Group Normalization avoids the influence of batch size on the model and addresses the issue of Internal Covariate Shift. Given the graph data $H \in  {\mathbf{R}}^{N \times  {in}}$ ,the calculation process of ResGAT is as follows:

<!-- Meanless: 9-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

$$
\left. \begin{array}{l} {e}_{ij} = \operatorname{leakyReLU}\left( {{a}^{T}\operatorname{concat}\left( {W{h}_{i},W{h}_{j}}\right) }\right) \\  {\alpha }_{ij} = \frac{\exp \left( {e}_{ij}\right) }{\mathop{\sum }\limits_{{k \in  {N}_{i}}}\exp \left( {e}_{ik}\right) } \\  {\operatorname{head}}_{m} = \operatorname{ELU}\left( {\mathop{\sum }\limits_{{j \in  {N}_{i}}}{\alpha }_{ij}{W}^{\left( m\right) }{h}_{j} + {W}^{\left( m\right) }{h}_{i}}\right) \\  {H}^{\left( 1\right) } = \operatorname{concat}\left( {{h}_{i}{e}_{ij},\ldots ,{\operatorname{head}}_{m}{d}_{j}}\right) {W}^{\left( h\right) } \end{array}\right\}  \operatorname{GAT}\left( H\right)  \tag{11}
$$

$$
\operatorname{ResGAT} = \operatorname{SiLU}\left\{  {\mathrm{{GN}}\left\{  {{\mathrm{{GAT}}}_{2}\left( {{\mathrm{{GAT}}}_{1}\left( H\right) }\right) }\right\}  }\right\}   + H \tag{12}
$$

<!-- Media -->

<!-- figureText: FeedForward & Group Norm FeedForward & Group Norm Concat MultiHead Linear Attention MLP MLP ... MLP ${e}_{1}$ ${e}_{2}$ ... ${e}_{6}$ (b) Semantic Encoding MultiHead Linear Attention LSTM LSTM LSTM ... MLP MLP ResGAT ResGAT ResGAT ${X}_{{t}_{1}}$ ${X}_{{t}_{2}}$ ${X}_{{t}_{p}}$ (a) AST-Module -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_9.jpg?x=263&y=159&w=1180&h=558&r=0"/>

Fig. 4. Multi-source information extraction framework. (a) Attention-based spatiotemporal feature extraction module. (b) Traffic environment semantic encoding.

<!-- Media -->

where, ${\alpha }_{ij}$ represents the attention weights between graph nodes, $m$ denotes the number of attention heads, $h$ is the node feature, and GN stands for Group Normalization. Both $W$ and $a$ are learnable parameters.

After processing through ResGAT,the historical traffic flow ${X}_{t} \in  {\mathbf{R}}^{N \times  C}$ is transformed into the road network feature ${H}_{t} \in  {\mathbf{R}}^{N \times  d}$ . The features in ${H}_{t}$ are then aggregated across all nodes using a fully connected layer to obtain ${F}_{t} \in  {\mathbf{R}}^{n}$ . The road network of each time step is represented by an encoding ${F}_{t}$ of dimension $n$ . Next, ${F}_{\left( {t - P}\right)  : t} = \left\{  {{F}_{t - P},{F}_{t - P + 1},\ldots ,{F}_{t}}\right\}   \in  {\mathbf{R}}^{n \times  P}$ is input into an LSTM network with skip connections to obtain $s{t}_{\left( {t - P}\right)  : t} \in  {\mathbf{R}}^{n \times  P}$ . The calculation is shown in Eq. (13):

$$
s{t}_{\left( {t - P}\right)  : t} = \operatorname{LSTM}\left( {F}_{\left( {t - P}\right)  : t}\right)  + {F}_{\left( {t - P}\right)  : t} \tag{13}
$$

Then,we use the multi-head linear attention mechanism to fuse the spatiotemporal features $s{t}_{t}$ at different time steps and obtain the historical spatiotemporal feature representation ${ST}$ . The calculation process is shown in Eq. (14):

$$
\mathcal{A}\left( {{Q}_{r},{K}_{r},{V}_{r}}\right)  = {\operatorname{softmax}}_{2}\left( \frac{{Q}_{r}}{\sqrt{d}}\right) \left( {{\operatorname{softmax}}_{1}{\left( {K}_{r}\right) }^{T}{V}_{r}}\right)  \tag{14a}
$$

$$
S{T}_{t} = \operatorname{Concat}\left( {\mathcal{A}\left( {{Q}_{1},{K}_{1},{V}_{1}}\right) ,\ldots ,\mathcal{A}\left( {{Q}_{h},{K}_{h},{V}_{h}}\right) }\right) {W}^{o} \tag{14b}
$$

$$
{ST} = \operatorname{MLP}\left( {\operatorname{Concat}\left( {S}_{\left( {t - P}\right)  : t}\right) }\right)  \tag{14c}
$$

where ${Q}_{r} = {W}_{O}^{r}s{t}_{i} \in  {\mathbf{R}}^{n \times  {d}_{k}},{K}_{r} = {W}_{K}^{r}s{t}_{i} \in  {\mathbf{R}}^{n \times  {d}_{k}}$ ,and ${V}_{r} = {W}_{V}^{r}s{t}_{i} \in  {\mathbf{R}}^{n \times  {d}_{v}}$ . softmax ${}_{1}$ and softmax ${}_{2}$ represent the softmax operations performed on the first dimension of ${K}_{r}$ and the second dimension of ${Q}_{r}$ respectively. This means we perform softmax separately on ${K}_{r}$ and ${Q}_{r}$ ,instead of after the ${Q}_{r}{K}_{r}^{T}$ multiplication. Such operation reduces computational complexity and significantly lowers memory and computation costs. ${W}_{Q}^{r},{W}_{K}^{r},{W}_{V}^{r}$ ,and ${W}^{o}$ are all learnable parameters.

#### 4.2.2. Environment semantic encoding module

To extract the features of traffic environment information, we employ an embedding encoding method, which we refer to as traffic environment semantic encoding, as shown in Fig. 4b. First, the environment information is converted into discrete data. The discretized data is then mapped into the embedding space through word embedding to obtain ${z}_{i} =$ Embedding $\left( {e}_{i}\right)$ (where ${e}_{i}$ represents different environment factors). Next,we use multi-head linear attention to fuse the different environment features, considering the interrelationships among them. Subsequently, the environment features are passed through a fully connected layer to obtain the final environment semantic representation ${SE}$ .

<!-- Meanless: 10-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

<!-- figureText: Conditional Representation $c$ LA(ST) LA(SE) LA(TE) MLP MLP MLP Sum Sum   Softmax Softmax Softmax Softmax nax| \\oftmax Linear Attention FeedForward & Group Norm + Concat  MultiHead Linear Attention  $\mathrm{H}$ QRT + ${ST}$ SE ${TE}$ Conditional representation module -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_10.jpg?x=331&y=155&w=1039&h=478&r=0"/>

Fig. 5. The structure of the conditional representation module.

<!-- Media -->

#### 4.2.3. Prediction step embedding module

TSGDiff takes the prediction step $Q$ as a parameter of the model. Due to the higher correlation between closer time steps compared to that between more distant ones, it is essential to consider the relativity between different time steps. We consider it by converting time into embeddings using position encoding based on sine and cosine functions (Vaswani et al., 2017). This function is shown in Eq. (15):

$$
{TE} = \left\lbrack  {\ldots ,\sin \left( \frac{t}{{10000}^{i/{2d}}}\right) ,\ldots ,\cos \left( \frac{t}{{10000}^{j/{2d}}}\right) ,\ldots }\right\rbrack   \tag{15}
$$

where, $t$ represents the time step and ${2d}$ is the dimension of the time embedding ${TE},i = 1,2,\ldots ,d,j = d + 1,d + 2,\ldots ,{2d}$ .

#### 4.2.4. Conditional representation module

TSGDiff extracts various types of traffic information representations through the aforementioned three modules. They are denoted as ${ST},{SE}$ ,and ${TE}$ ,respectively. We set the dimensions of all the three representations to $D$ (in our experiments, $D = {64}$ ). This uniform dimensionality ensures compatibility among these representations within the attention mechanism and facilitates their fusion. These representations are then fed into a multi-head linear attention layer to be fused into a conditional representation. The computation process is as follows:

$$
\operatorname{MHLA}\left( \alpha \right)  = \operatorname{Concat}\left( {\mathcal{A}\left( {{W}_{Q}^{1}\alpha ,{W}_{K}^{1}\alpha ,{W}_{V}^{1}\alpha }\right) ,\ldots ,\mathcal{A}\left( {{W}_{Q}^{h}\alpha ,{W}_{K}^{h}\alpha ,{W}_{V}^{h}\alpha }\right) }\right) {W}^{o} \tag{16a}
$$

$$
c = \mathrm{{GN}}\{ \operatorname{MLP}\{ \operatorname{Concat}\left( {\operatorname{MHLA}\left( {ST}\right) ,\operatorname{MHLA}\left( {SE}\right) ,\operatorname{MHLA}\left( {TE}\right) }\right) \} \}  \tag{16b}
$$

where, $\alpha$ represents one of ${ST},{SE}$ ,and ${TE}$ . MHLA denotes the multi-head linear attention fusion mechanism. The computation process for $\mathcal{A}$ can be seen in Eq. (14a). All $W$ are learnable parameters. We illustrate the structure of the conditional representation module in Fig. 5. The right half of Fig. 5 depicts the computation process for single-head linear attention. Multi-head attention extends this process by concatenating the results of multiple single-head attentions, followed by a linear transformation, as described in Eq. (16a).

### 4.3. GAUNet: Conditional denoising network

To predict the noise of each node in the graph, we design a Graph Attention U-Net (GAUNet) inspired by the U-Net (Ronneberger et al., 2015) architecture used in image semantic segmentation. In the original U-Net for images, the structure information of graphs cannot be considered. Therefore, we propose GAUNet to model spatial relationships. GAUNet incorporates the multi-head linear attention mechanism to enhance the model's focus on global features. This adaptation allows GAUNet to better handle the topological structure inherent in traffic networks. Fig. 6 illustrates the structure of GAUNet.

In the reverse diffusion process, TSGDiff needs to know the current reverse diffusion time step. So we also use a position encoding based on sine and cosine functions to embed the time information. This time embedding TE is then integrated into the model as a feature scaling weight, as shown in Eq. (17):

---

$$
s = \operatorname{SiLU}\left( {W * {TE} + b}\right) 
$$

---

<!-- Meanless: (17a) 11-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

<!-- figureText: FeedForward & Group Norm ${\widetilde{G}}_{D}^{1}$ Up Sample 1 FeedForward & Group Norm Up Sample 2 MultiHead Linear Attention Up Sample 3 ResGAT ResGAT Scale Scaling Down Sample 3 ResGAT Down Sample 2 Scale Scaling Down Sample 1 ${G}_{t}$ TE TE MultiHead Linear Attention ResGAT Scale Scaling ResGAT L   Scale Scaling Concat ${\widetilde{G}}_{D}^{2}$ ${\widetilde{G}}_{U}^{3}$ TE -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_11.jpg?x=260&y=160&w=1183&h=619&r=0"/>

Fig. 6. The structure of the conditional denoising network GAUNet.

<!-- Media -->

$$
s \rightarrow  \text{[scale,shift]} \tag{17b}
$$

$$
Y = \operatorname{SiLU}\left( {X * \left( {\text{ sacle } + 1}\right)  + \text{ shift }}\right)  \tag{17c}
$$

where,the time embedding ${TE} \in  {\mathbf{R}}^{N \times  d}$ is transformed into a vector $s \in  {\mathbf{R}}^{N \times  {2d}}$ to match the required dimensions. This vector $s$ is then split into two separate vectors,scale $\in  {\mathbf{R}}^{N \times  d}$ and shift $\in  {\mathbf{R}}^{N \times  d}$ . Finally,these vectors are used to scale the feature $X \in  {\mathbf{R}}^{N \times  d}$ , producing the result $Y \in  {\mathbf{R}}^{N \times  d}$ .

As illustrated in Fig. 6, GAUNet is composed of two main parts: a down sample and an up sample. Each down sample layer includes two ResGAT layers, two scaling layers, a multi-head linear attention layer, and a feedforward layer. With each down sample layer, the dimensions of node features are halved. After completing the down sample process, the network transitions to the up sample layers. The structure of the up sample layers is similar to that of the down sample layers, but the addition of a concatenation layer at the first layer ensures that both low-level and high-level features are utilized. With each up sample layer, the dimensions of node features are doubled. Ultimately, GAUNet outputs the predicted noise for each node.

## 5. Experiments and evaluations

### 5.1. Datasets

We constructed a real-world multi-source information and traffic speed dataset for our experiments. This dataset covers all expressways and major roads within the Fifth Ring Road in Beijing, comprising a total of 250 roads, with an average length of 3.06 $\mathrm{{km}}$ ,a minimum of ${0.759}\mathrm{\;{km}}$ ,and a maximum of ${7.47}\mathrm{\;{km}}$ . Approximately ${60}\%$ of the road segments fall within $0 - 3\mathrm{\;{km}}$ ,while about ${80}\%$ are within $0 - {4.4}\mathrm{\;{km}}$ . This spatial granularity strikes a balance between computational efficiency and data availability. The data spans from 7 December 2019 to 17 January 2020,with a sampling interval of $5\mathrm{\;{min}}$ ,resulting in 12,096 time points in total. Additionally, we obtained the corresponding traffic environment information for this period, including weather, workdays, holidays, and traffic control measures. Compared to public datasets like PEMS-Bay, the Beijing dataset better aligns with the goal of this study, which is to evaluate the performance of the diffusion model in complex, real-world urban traffic environments. While PEMS-Bay are commonly used for traffic prediction tasks, they represent relatively simple traffic conditions and lack multi-source environment factors. As such, these public datasets are insufficient to support the multi-source information modeling requirements central to our research.

The experiment strictly follows the protocols widely adopted in previous works. Approximately 90% of the data is used for training, while 10% is reserved for testing. To extensively test the traffic state generation capability on different dates, we selected 11 January 2020, 13 January 2020, 15 January 2020, and 17 January 2020, as the test set. The remaining data was used as the training and validation sets. Furthermore, during the training process, the Max-Min method was used to normalize the training set to the $\left\lbrack  {0,1}\right\rbrack$ range. During the evaluation,the results were scaled back to the normal range and compared with the ground truth.

### 5.2. Evaluation metrics

To evaluate the performance of our model, we used three standard metrics to assess the accuracy of deterministic predictions: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) to quantify the prediction accuracy:

<!-- Meanless: 12-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

$$
\mathrm{{MAE}} = \frac{1}{n}\mathop{\sum }\limits_{{i = 1}}^{n}\left| {{y}_{i} - {\widehat{y}}_{i}}\right| ,\;\mathrm{{RMSE}} = \sqrt{\frac{1}{n}\mathop{\sum }\limits_{{i = 1}}^{n}{\left( {y}_{i} - {\widehat{y}}_{i}\right) }^{2}},\;\mathrm{{MAPE}} = \frac{1}{n}\mathop{\sum }\limits_{{i = 1}}^{n}\left| \frac{{y}_{i} - {\widehat{y}}_{i}}{{y}_{i}}\right|  \times  {100}\%  \tag{18}
$$

where, ${y}_{i}$ denotes the true values, ${\widehat{y}}_{i}$ denotes the predicted values,and $n$ represents the number of predicted results.

Additionally, we employed the Continuous Ranked Probability Score (CRPS) (Matheson and Winkler, 1976) as a metric to evaluate the performance of probabilistic predictions. CRPS measures the compatibility of the estimated probability distribution $F$ with the observed value $Y$ ,as shown in Eq. (19).

$$
\operatorname{CRPS}\left( {F,Y}\right)  = \frac{1}{n}\mathop{\sum }\limits_{{i = 1}}^{n}\int {\left\lbrack  {F}_{i}\left( z\right)  - {1}_{\left\{  z \geq  {y}_{i}\right\}  }\right\rbrack  }^{2}{dz} \tag{19}
$$

Smaller values for the above evaluation metrics indicate better performance.

### 5.3. Implementation details

We employed a diffusion step size $N$ of 500 and adopted a sigmoid schedule for the growth of noise ${\beta }_{t}$ . We clipped ${\beta }_{t}$ to the range $\left\lbrack  {{0.0001},{0.9999}}\right\rbrack$ to prevent singularities near $t = N$ at the end of the diffusion process. The model was trained using the Adam optimizer with a base learning rate of 0.0001 . The batch size was 32 and the epoch was 100 . To prevent overfitting during training, we employed an early stopping mechanism with a patience of 10 epochs. The validation loss is measured by RMSE as the metric for early stopping, which focuses on improving the model's deterministic prediction accuracy. Compared to MAE, which measures the average error, and CRPS, which focuses on probabilistic predictions, RMSE is more sensitive to large prediction errors due to its squared calculation and is capable of clearer reflection the dynamic changes in significant deviations during training. Training was terminated if the validation loss did not improve for 10 consecutive epochs. Besides, we utilized dropout with a dropout rate of 0.2 . All experiments were conducted on two NVIDIA 2080Ti GPUs.

Given that the diffusion model is a stochastic model predicting over distributions, each generated result may vary. To minimize randomness in generation, we took the average of 10 generated results as the final prediction.

### 5.4. Baselines

To demonstrate the feasibility and effectiveness of our proposed model, we conducted experiments on the traffic prediction task, to compare our method with 13 traffic prediction baseline methods. These methods include (1) HA (Liu and Guan, 2004), (2) SVR (Smola and Schlkopf, 2004), (3) GRU (Chung et al., 2014), (4) LSTM (Hochreiter and Schmidhuber, 1997), (5) TCN (Bai et al., 2018), (6) TGCN (Zhao et al., 2019), (7) STGCN (Yu et al., 2017), (8) ASTGCN (Guo et al., 2019a), (9) DCRNN (Li et al., 2017), (10) GWN (Wu et al., 2019), (11) STGIN (Zou et al., 2023), (12) iTransformer (Liu et al., 2024), and (13) TimeMixer (Wang et al., 2024). Among these, (1) and (2) are classical mathematical models, (3)-(5) and (12)-(13) are time series deep learning models, and (6)-(11) are spatiotemporal neural network models. All models' inputs included historical traffic speed data for one hour, and the spatiotemporal neural networks also incorporated the spatial topology of the road network. For a fair comparison, we embedded traffic environment factors as multiple channels of spatiotemporal graph node features into the spatiotemporal neural network models (Baseline (6)-(10)). STGIN has a separate embedding statement, so it does not require additional graph channels for environment factor integration. It is worth noting that the baselines predict future sequence data, while TSGDiff directly predicts the traffic state at a specific future time point.

We ensured that all baselines were evaluated under the same conditions as much as possible. All baselines used the same data normalization method and dataset partitioning with that of TSGDiff as described in 5.1 to ensure data consistency. The input of each baseline consisted of historical data for one hour (historical sequence length of 12), and the output was the data for the next three hours (future sequence length of 36). The parameters for each baseline were tuned using cross-validation. We first fairly built the models on the configurations provided in the original papers or official codes for each one, and then employed grid search to explore the best hyperparameter combinations for each model. Generally, we selected three values for common hyperparameters. For example, when choosing the hidden layer dimensions, we considered several common dimensions: 32, 64, and 128. The parameters that performed best on the validation dataset were then selected as the final results. The maximum number of epochs for training all models was set to 100 , with a batch size of 32 . The same as TSGDiff, the baselines also employed an early stopping mechanism with a patience of 10 epochs, using the validation RMSE as the metric. With the exception of STGIN, which was built using TensorFlow, all other deep learning baselines were implemented using PyTorch. All models were tested under the same hardware environment as TSGDiff, ensuring consistency in running conditions.

### 5.5. Results

#### 5.5.1. Performance comparisons

We compared TSGDiff with several state-of-the-art traffic prediction methods over different prediction horizons. The longest horizon was $3\mathrm{\;h}$ . The results are presented in Table 2,where the best-performing method is highlighted in bold,and the optimal model among the baselines is underlined. Baseline methods typically perform well only in short-term predictions; their accuracy significantly decreases beyond ${30}\mathrm{\;{min}}$ . As shown in the table,TSGDiff not only matches the performance of baselines in the short term but also maintains its performance in the long term. For the 15- and 30-min prediction RMSE, TSGDiff improves the error by 2.43 and 0.52 respectively compared to the optimal baseline. For prediction horizons of 45,60,75,90,120,150,and 180 min, TSGDiff shows percentage decreases in RMSE of 13.56%, 13.90%, 22.41%, 35.77%, 34.82%, 34.35%, and 32.22%, respectively, compared to the optimal baseline. As the prediction horizon increases, the advantage of TSGDiff becomes more apparent. This evidence confirms that TSGDiff's point-to-point generation strategy is minimally affected by the prediction horizon. In other words, TSGDiff effectively alleviates the performance limitations of current traffic prediction methods for long-term predictions.

<!-- Meanless: 13-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

Table 2

Comparison of TSGDiff and traditional traffic prediction methods over different prediction horizons.

<table><tr><td/><td>Metric</td><td>HA</td><td>SVR</td><td>GRU</td><td>LSTM</td><td>TCN</td><td>TGCN</td><td>ST-GCN</td><td>AST-GCN</td><td>DC-RNN</td><td>GWN</td><td>STGIN</td><td>iTrans-former</td><td>Time-Mixer</td><td>TSG-Diff</td></tr><tr><td rowspan="3">15 min</td><td>RMSE</td><td>12.77</td><td>6.50</td><td>5.29</td><td>5.28</td><td>8.13</td><td>6.31</td><td>6.26</td><td>4.22</td><td>9.96</td><td>3.37</td><td>3.20</td><td>3.33</td><td>4.83</td><td>5.80</td></tr><tr><td>MAE</td><td>9.47</td><td>5.47</td><td>3.62</td><td>3.57</td><td>4.95</td><td>4.26</td><td>4.11</td><td>2.67</td><td>7.51</td><td>2.45</td><td>2.49</td><td>1.83</td><td>2.65</td><td>4.12</td></tr><tr><td>MAPE</td><td>26.40</td><td>10.96</td><td>9.01</td><td>9.29</td><td>13.20</td><td>12.82</td><td>12.14</td><td>6.96</td><td>22.40</td><td>5.55</td><td>$\underline{\mathbf{{5.37}}}$</td><td>5.38</td><td>7.41</td><td>9.29</td></tr><tr><td rowspan="3">30 min</td><td>RMSE</td><td>13.75</td><td>8.38</td><td>6.19</td><td>6.12</td><td>8.82</td><td>7.38</td><td>6.90</td><td>6.34</td><td>15.81</td><td>6.23</td><td>6.03</td><td>5.29</td><td>6.29</td><td>5.81</td></tr><tr><td>MAE</td><td>10.23</td><td>7.18</td><td>4.25</td><td>4.43</td><td>5.14</td><td>4.89</td><td>4.84</td><td>4.10</td><td>11.79</td><td>4.03</td><td>3.98</td><td>2.89</td><td>3.51</td><td>4.14</td></tr><tr><td>MAPE</td><td>27.76</td><td>14.34</td><td>10.91</td><td>11.61</td><td>13.90</td><td>13.34</td><td>13.33</td><td>11.80</td><td>29.80</td><td>11.06</td><td>11.19</td><td>8.35</td><td>10.37</td><td>9.38</td></tr><tr><td rowspan="3">45 min</td><td>RMSE</td><td>13.90</td><td>9.51</td><td>6.97</td><td>7.34</td><td>8.47</td><td>8.79</td><td>7.77</td><td>7.52</td><td>16.01</td><td>6.71</td><td>6.80</td><td>6.65</td><td>7.04</td><td>5.75</td></tr><tr><td>MAE</td><td>10.28</td><td>8.19</td><td>4.46</td><td>5.09</td><td>5.04</td><td>5.71</td><td>5.07</td><td>4.59</td><td>12.37</td><td>4.48</td><td>4.60</td><td>3.96</td><td>3.97</td><td>4.08</td></tr><tr><td>MAPE</td><td>28.53</td><td>17.01</td><td>12.21</td><td>13.50</td><td>14.13</td><td>16.16</td><td>15.65</td><td>14.84</td><td>30.82</td><td>12.59</td><td>12.88</td><td>$\underline{11.49}$</td><td>12.05</td><td>9.27</td></tr><tr><td rowspan="3">60 min</td><td>RMSE</td><td>13.93</td><td>10.70</td><td>8.72</td><td>8.50</td><td>8.14</td><td>8.90</td><td>8.58</td><td>7.86</td><td>16.38</td><td>7.81</td><td>6.70</td><td>7.62</td><td>7.70</td><td>5.77</td></tr><tr><td>MAE</td><td>10.32</td><td>8.64</td><td>5.64</td><td>5.91</td><td>4.97</td><td>5.83</td><td>5.77</td><td>5.63</td><td>13.32</td><td>5.33</td><td>4.83</td><td>4.41</td><td>4.38</td><td>4.11</td></tr><tr><td>MAPE</td><td>29.71</td><td>18.35</td><td>15.49</td><td>14.91</td><td>14.18</td><td>16.72</td><td>15.72</td><td>16.69</td><td>30.94</td><td>15.99</td><td>14.54</td><td>13.71</td><td>$\underline{12.96}$</td><td>9.30</td></tr><tr><td rowspan="3">75 min</td><td>RMSE</td><td>14.53</td><td>11.76</td><td>9.38</td><td>9.72</td><td>9.36</td><td>8.98</td><td>9.53</td><td>9.14</td><td>17.26</td><td>8.26</td><td>7.46</td><td>8.34</td><td>8.49</td><td>5.79</td></tr><tr><td>MAE</td><td>10.99</td><td>9.24</td><td>6.80</td><td>6.18</td><td>5.34</td><td>6.78</td><td>7.22</td><td>5.76</td><td>13.63</td><td>5.57</td><td>5.20</td><td>4.91</td><td>4.84</td><td>4.11</td></tr><tr><td>MAPE</td><td>30.70</td><td>21.48</td><td>18.96</td><td>16.81</td><td>14.60</td><td>18.71</td><td>18.10</td><td>16.95</td><td>30.70</td><td>18.56</td><td>15.49</td><td>15.47</td><td>14.55</td><td>9.31</td></tr><tr><td rowspan="3">90 min</td><td>RMSE</td><td>14.78</td><td>12.54</td><td>9.91</td><td>10.04</td><td>9.91</td><td>9.06</td><td>9.84</td><td>9.51</td><td>17.34</td><td>10.50</td><td>9.12</td><td>8.98</td><td>9.04</td><td>5.77</td></tr><tr><td>MAE</td><td>11.68</td><td>9.50</td><td>7.22</td><td>7.01</td><td>5.60</td><td>6.86</td><td>7.61</td><td>6.48</td><td>13.90</td><td>7.29</td><td>6.01</td><td>5.44</td><td>5.11</td><td>4.08</td></tr><tr><td>MAPE</td><td>33.41</td><td>22.50</td><td>19.74</td><td>18.63</td><td>17.38</td><td>20.37</td><td>21.97</td><td>19.15</td><td>29.66</td><td>22.02</td><td>16.63</td><td>17.00</td><td>$\underline{15.49}$</td><td>9.24</td></tr><tr><td rowspan="3">120 min</td><td>RMSE</td><td>16.22</td><td>13.52</td><td>11.38</td><td>11.15</td><td>9.43</td><td>8.92</td><td>11.40</td><td>10.51</td><td>17.84</td><td>11.19</td><td>8.88</td><td>10.22</td><td>8.95</td><td>5.79</td></tr><tr><td>MAE</td><td>12.11</td><td>10.82</td><td>7.97</td><td>7.77</td><td>5.82</td><td>6.60</td><td>7.75</td><td>7.41</td><td>14.15</td><td>8.38</td><td>6.43</td><td>6.32</td><td>5.09</td><td>4.09</td></tr><tr><td>MAPE</td><td>32.96</td><td>26.69</td><td>21.75</td><td>22.10</td><td>15.94</td><td>20.32</td><td>25.32</td><td>22.78</td><td>31.23</td><td>26.85</td><td>19.83</td><td>21.01</td><td>15.09</td><td>9.30</td></tr><tr><td rowspan="3">150 min</td><td>RMSE</td><td>16.61</td><td>14.98</td><td>11.49</td><td>12.85</td><td>10.24</td><td>8.81</td><td>11.69</td><td>10.92</td><td>18.01</td><td>11.22</td><td>8.91</td><td>11.38</td><td>9.59</td><td>5.78</td></tr><tr><td>MAE</td><td>13.13</td><td>11.83</td><td>9.73</td><td>8.89</td><td>6.31</td><td>6.73</td><td>8.32</td><td>7.90</td><td>14.15</td><td>8.79</td><td>6.61</td><td>7.12</td><td>5.52</td><td>4.10</td></tr><tr><td>MAPE</td><td>34.78</td><td>29.13</td><td>24.69</td><td>22.13</td><td>19.29</td><td>20.36</td><td>35.27</td><td>23.00</td><td>31.86</td><td>26.89</td><td>21.16</td><td>24.98</td><td>16.38</td><td>9.30</td></tr><tr><td rowspan="3">180 min</td><td>RMSE</td><td>17.05</td><td>15.17</td><td>12.44</td><td>13.19</td><td>10.49</td><td>9.14</td><td>15.56</td><td>10.94</td><td>18.85</td><td>12.92</td><td>8.59</td><td>12.40</td><td>9.87</td><td>5.82</td></tr><tr><td>MAE</td><td>13.72</td><td>11.93</td><td>9.79</td><td>9.24</td><td>6.44</td><td>6.90</td><td>11.19</td><td>8.17</td><td>14.18</td><td>9.74</td><td>6.90</td><td>8.17</td><td>5.67</td><td>4.14</td></tr><tr><td>MAPE</td><td>36.07</td><td>31.04</td><td>25.57</td><td>24.93</td><td>20.97</td><td>22.45</td><td>35.73</td><td>23.67</td><td>32.28</td><td>32.83</td><td>22.55</td><td>27.44</td><td>16.72</td><td>9.36</td></tr></table>

<!-- figureText: 15min 15min 15min 30min 180min 30% 30min 25% 20% 15% 45min 150min 45min 60min 120min 60min 75min 90min 75min STGIN0 TimeMixer-*- TSGDiff 180min 30min 180min 10 150min 45min 150min- 120min 60min 120min 90min 75min 90min GRU TCN&- TGCN- ASTGCN- GWN -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_13.jpg?x=184&y=1116&w=1337&h=415&r=0"/>

Fig. 7. Radar chart of prediction metrics for different methods.

<!-- Media -->

We analyzed the error metrics of some baselines and TSGDiff across different prediction horizons and illustrated the results using a radar chart, as shown in Fig. 7. Methods that focus solely on time series features, such as GRU, iTransformer, and TimeMixer, exhibit performance degradation as the prediction horizon increases. Such decline is attributed to the complexity of the traffic system, which is influenced by multiple sources of information; accurate predictions require consideration of both spatial information and environment factors. However, the state-of-the-art TimeMixer performs better than traditional models that incorporate spatiotemporal features, like TGCN and STGCN, in most scenarios. In contrast, TSGDiff takes into account both spatiotemporal dependencies and traffic environment information, so it demonstrates very stable error rates across all prediction horizons, outperforming baseline methods in long-term forecasting. This result suggests that TSGDiff's strategy of generating traffic states while incorporating multi-source information substantially enhances prediction accuracy.

<!-- Meanless: 14-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

<!-- figureText: 4.8 25%~75% Range with 1.5IQR Median Mean 105 120 135 150 165 180 4.6 4.4 4.2 CRPS 4.0 3.8 3.6 3.4 3.2 15 30 45 60 75 90 Prediction Horizon (MIN) -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_14.jpg?x=184&y=154&w=1339&h=375&r=0"/>

Fig. 8. The CRPS results of TSGDiff across different prediction horizons.

Table 3

Model efficiency comparison.

<table><tr><td>Model</td><td>GRU</td><td>LSTM</td><td>TCN</td><td>TGCN</td><td>STGCN</td><td>ASTGCN</td><td>DCRNN</td><td>GWN</td><td>STGIN</td><td>iTransformer</td><td>TimeMixer</td><td>TSGDiff</td></tr><tr><td>Parameters/(K)</td><td>141.75</td><td>182.2</td><td>247.4</td><td>263.42</td><td>162.83</td><td>337.67</td><td>119.43</td><td>290.12</td><td>182.79</td><td>271.52</td><td>793.68</td><td>291.66</td></tr><tr><td>MACs/(M)</td><td>3.66</td><td>4.63</td><td>40.31</td><td>52.00</td><td>187.68</td><td>94.66</td><td>29.73</td><td>239.97</td><td>967.04</td><td>99.87</td><td>2089.70</td><td>2071.18</td></tr></table>

<!-- Media -->

#### 5.5.2. Probabilistic prediction

Although this study primarily reports the mean predictions generated by the TSGDiff model, it is important to note that the model also produces a variety of possible distributions for future traffic states during the generation process. Therefore, we calculated the CRPS for all prediction horizons to analyze TSGDiffs performance from the perspective of probabilistic forecasting. Specifically, for a given prediction horizon, we calculate the CRPS value for each prediction time step based on Eq. (19), and these values are aggregated into a set $S$ . In this case, $n$ in Eq. (19) represents the number of nodes in the road network,meaning the CRPS is the average of the probabilistic prediction indicators for all nodes at each prediction time step. Subsequently, we plot a box plot based on the values in $S$ ,as shown in Fig. 8. From the figure,the CRPS ranges from 3.2 to 4.8,with the mean across all prediction horizons around 3.8. This indicates that TSGDiff maintains strong probabilistic forecasting performance in both short-term and long-term predictions. The generative capability gives TSGDiff a significant advantage in long-term predictions, as it can capture the potential range of state changes. Specifically, during periods when traffic states are complex or uncertain, TSGDiff is able to generate multiple possible traffic states to address these uncertainties, outperforming traditional deterministic prediction models.

### 5.6. Model efficiency

#### 5.6.1. Memory complexity and time complexity

We compared the number of parameters and the Multiply-Accumulate Operations (MACs) for predicting the next three hours between deep learning baselines and TSGDiff in Table 3. These metrics represent the model's memory complexity and time complexity, respectively. From the table, it can be observed that TSGDiff has a parameter count comparable to most baselines, but its MACs are higher. This increased complexity is attributed to the multi-step iterative diffusion inference process, which is computationally intensive. However, TSGDiff demonstrates superior performance in handling multi-source information integration and addressing uncertainties in long-term predictions. Additionally, with the rapid advancement of GPU computational capabilities, this extra computational complexity is acceptable. While GRU and LSTM have low complexity, they perform poorly in long-term predictions. When the model accounts for spatiotemporal relationships in traffic, as seen with TGCN, STGCN, ASTGCN, GWN, and STGIN, model complexity increases. The iTransformer is an efficient model with a larger parameter count but relatively low MACs; however, it lacks consideration for traffic environment factors and the inherent randomness in traffic, leading to ineffective long-term predictions. TimeMixer exhibits the highest memory and time complexity, primarily due to its fully MLP-based architecture.

#### 5.6.2. Sampling acceleration

In TSGDiff, the iterative sampling process of the diffusion model results in high computational costs. To reduce this cost, we adopted a more efficient sampling strategy inspired by Song et al. (2020b), which samples only a subset with diffusion steps of size $L$ . This acceleration method,referred to as TSGDiff_acc,enables non-Markovian jump sampling during the reverse diffusion process requiring no retraining of the model; but only modifying of the sampler. The sampling formula in Algorithm 2 is updated as (20):

$$
{G}_{n,t - p} = \sqrt{\frac{{\bar{\alpha }}_{t - p}}{{\bar{\alpha }}_{t}}}\left( {{G}_{n,t} - \sqrt{1 - {\bar{\alpha }}_{t}}{\varepsilon }_{\theta }\left( {{G}_{n,t},t,c}\right) }\right)  + \sqrt{1 - {\bar{\alpha }}_{t - p} - {\sigma }_{t}^{2}}{\varepsilon }_{\theta }\left( {{G}_{n,t},t,c}\right)  + {\sigma }_{t}z \tag{20}
$$

<!-- Meanless: 15-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

where, $z \sim  \mathcal{N}\left( {0,1}\right) ;{\sigma }_{t}^{2} = \frac{1 - {\bar{\alpha }}_{t - 1}}{1 - {\bar{\alpha }}_{t}}\left( {1 - {\alpha }_{t}}\right)$ controls the stochasticity of the denoising process; $p$ represents the accelerated sampling interval; $L = T/P$ . We experimented with different values of $p$ to evaluate the accelerated results. The results are shown in Table 4. The results demonstrate that as $p$ increases,the time complexity of inference is significantly reduced,but the generation accuracy also declines. When $p = {50}$ ,TSGDiff_acc achieved a nearly ${15} \times$ speedup,but the generated results were almost unusable. However, when $p < 5$ ,TSGDiff_acc effectively accelerated the inference process while maintaining satisfactory generation performance,with time complexity comparable to some deep learning baseline models. For instance,when $p = 2$ ,TSGDiff_acc shows only a 0.5% increase in RMSE compared to TSGDiff but achieves nearly a $2 \times$ improvement in inference speed. Therefore,under resource-constrained scenarios,an appropriate value of $p$ can be selected to balance sampling speed and prediction accuracy based on specific requirements.

<!-- Media -->

Table 4

Results of the TSGDiff acceleration sampling.

<table><tr><td rowspan="2"/><td rowspan="2">TSGDiff</td><td colspan="6">TSGDiff_acc</td></tr><tr><td>$p = 2$</td><td>$p = 4$</td><td>$p = 5$</td><td>$p = {10}$</td><td>$p = {20}$</td><td>$p = {50}$</td></tr><tr><td>MACs/(M)</td><td>2071.18</td><td>1088.14</td><td>596.62</td><td>498.32</td><td>301.71</td><td>203.40</td><td>144.42</td></tr><tr><td>RMSE</td><td>5.78</td><td>5.81</td><td>6.18</td><td>6.58</td><td>8.14</td><td>10.09</td><td>16.04</td></tr><tr><td>MAE</td><td>4.1</td><td>4.20</td><td>4.41</td><td>4.72</td><td>5.60</td><td>7.01</td><td>11.59</td></tr><tr><td>MAPE</td><td>9.31</td><td>9.55</td><td>9.61</td><td>9.96</td><td>12.60</td><td>17.41</td><td>36.96</td></tr><tr><td>CRPS</td><td>3.88</td><td>3.94</td><td>4.12</td><td>4.27</td><td>5.11</td><td>6.49</td><td>9.47</td></tr></table>

<!-- figureText: 0.20 4.6 Cosine Linear Quadratic Sigmoid 0.01 0.02 (b) 0.2 ${\beta }_{\mathrm{T}}$ 0.15 4.5 24.4 0.10 4.2 0.05 4.1 0.00 4.0 0 100 200 300 400 500 Step -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_15.jpg?x=183&y=479&w=1336&h=337&r=0"/>

Fig. 9. (a) Growth curves of different variance schedules. (b) MAE results for different variance schedules.

<!-- Media -->

We observed that, even after the sampling acceleration, TSGDiff's computational cost remains higher than some baselines, such as iTransformer. This is primarily due to the iterative sampling employed by TSGDiff, where each step requires multiple forward processes, increasing the inference time. In contrast, iTransformer performs predictions with a single forward process, resulting in a lower time complexity and the computational cost for inference.

### 5.7. Hyperparameter sensitivity

In this section, we investigated the impact of several key hyperparameters of TSGDiff on the performance of traffic state generation. Unless otherwise specified, all error results presented in the subsequent sections are the average values across all prediction horizons and are based on the non-accelerated version of TSGDiff.

#### 5.7.1. Variance schedule

We examined the impact of four variance schedules of ${\beta }_{t}$ on TSGDiff. These variance schedules are defined as shown in Eq. (21):

$$
\text{1. Cosine:}f\left( t\right)  = {\cos }^{2}\left( {\frac{t/T + {0.008}}{1 + {0.008}} * \frac{\pi }{2}}\right) ,\;{\bar{\alpha }}_{t} = \frac{f\left( t\right) }{f\left( 0\right) },\;{\beta }_{t} = \left( {1 - \frac{{\bar{\alpha }}_{t}}{{\bar{\alpha }}_{t - 1}}}\right)  \tag{21a}
$$

$$
\text{2. Linear:}{\beta }_{t} = {\beta }_{0} + \left( {{\beta }_{T} - {\beta }_{0}}\right)  \times  \frac{t}{T} \tag{21b}
$$

$$
\text{3. Quadratic:}{\beta }_{t} = {\left( \sqrt{{\beta }_{0}} + \left( \sqrt{{\beta }_{T}} - \sqrt{{\beta }_{0}}\right)  \times  \frac{t}{T}\right) }^{2} \tag{21c}
$$

$$
\text{4. Sigmoid:}\sigma \left( t\right)  =  - 6 + {12} \times  \frac{t}{T},\;{\beta }_{t} = {\beta }_{0} + \frac{{\beta }_{T} - {\beta }_{0}}{1 + {e}^{\sigma \left( t\right) }} \tag{21d}
$$

The growth curves of them are shown in Fig. 9(a), with all diffusion steps set to 500. Each variance schedule increases progressively, fulfilling the requirement that longer diffusion steps in the forward diffusion process lead to greater noise. Furthermore, we investigated five types of ${\beta }_{T} \in  \{ {0.01},{0.02},{0.1},{0.2},{1.0}\}$ . We calculated MAE of the prediction results,as displayed in Fig. 9(b) and observed that when the sigmoid variance schedule and ${\beta }_{T} = {0.02}$ is chosen,TSGDiff achieves optimal performance.

In fact, since we selected sufficiently long diffusion steps, the final diffusion results approximate a Gaussian distribution regardless of the variance schedule. However, the generation performance of the sigmoid variance schedule is slightly superior to other schedules. This is because the noise changes smoothly at both the beginning and end of the sigmoid variance schedule. Therefore, at the start of the reverse diffusion process, TSGDiff can better determine the direction of the noise and has ample time to eliminate big noise. At the end of the process, TSGDiff can more precisely remove subtle noise. From Fig. 9(b), we can see that regardless of the variance schedule,as ${\beta }_{T}$ increases,the performance improves before declining. This indicates that controlling the maximum value of the noise affects the models performance,so it necessitates the selection of the most appropriate ${\beta }_{T}$ in practical applications. In this experiment,the optimal ${\beta }_{T}$ appears to be 0.02 .

<!-- Meanless: 16-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

Table 5

The impact of different diffusion steps $T$ on performance. The unit of time cost is second.

<table><tr><td>$T$</td><td>100</td><td>200</td><td>300</td><td>500</td><td>1000</td><td>2000</td></tr><tr><td>RMSE</td><td>14.13</td><td>8.43</td><td>7.64</td><td>5.78</td><td>5.72</td><td>5.55</td></tr><tr><td>MAE</td><td>11.54</td><td>7.08</td><td>6.33</td><td>4.10</td><td>4.08</td><td>4.32</td></tr><tr><td>MAPE</td><td>18.53</td><td>13.27</td><td>10.81</td><td>9.31</td><td>9.28</td><td>9.19</td></tr><tr><td>CRPS</td><td>8.24</td><td>5.01</td><td>4.17</td><td>3.88</td><td>3.85</td><td>3.84</td></tr><tr><td>Time Cost</td><td>3.72</td><td>7.03</td><td>10.26</td><td>18.82</td><td>37.83</td><td>76.07</td></tr></table>

<!-- figureText: MAE RMSE MAPE CRPS Time cost (s) Memory cost (MB) 4000 100 Time cost 3500 5.0 80 Memory cost 3000 4.5 60 2500 4.0 40 2000 1500 3.5 20 1000 10 20 30 40 N RMSE 14% MAE 5.5 MAPE 5.0 CRPS 12% 4.5 - 10% 4.0 8% 0 10 30 40 -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_16.jpg?x=184&y=438&w=1335&h=339&r=0"/>

Fig. 10. The impact of different sampling numbers on the results.

<!-- Media -->

#### 5.7.2. Diffusion step $T$

In Table 5,we studied the impact of different diffusion steps $T$ on the models performance and recorded the time consumed for a single inference. When the number of diffusion step is too small, the result of the forward diffusion process cannot ensure a Gaussian distribution. Consequently, the reverse process starting from a Gaussian distribution is an inaccurate approximation and impairs the model's performance. As the number of diffusion steps increases, the likelihood of achieving promising results becomes higher. However, excessively large diffusion steps result in prolonged inference time. Regarding space complexity, the memory consumption of the model remains consistent at 1281MB regardless of the number of diffusion steps. To achieve an optimal balance between performance and efficiency, we selected 500 diffusion steps in the model, as further increasing the number of steps provides negligible performance improvement.

#### 5.7.3. Sampling number $N$

We investigated the impact of different sampling numbers $N$ on the models performance,as shown in Fig. 10. Fig. 10 presents the error metrics,as well as the time cost and memory consumption per inference for various types of $N$ . The time consumed by TSGDiff to generate a single sample is approximately ${17}\mathrm{\;s}$ . Since the model can generate multiple samples in parallel,the generation time is primarily influenced by the number of diffusion steps when $N$ is small. As long as there is sufficient memory space,a sufficient number of samples can be generated at once.

When $N$ exceeds 5,the time cost exhibits a linear growth trend. In this experiment,the Nvidia 2080Ti*2 GPU with 22 GB memory space can generate up to 320 samples in parallel. For every increment of 5 in $N$ ,memory consumption increases by approximately 300MB. As $N$ increases,the error in the results gradually decreases. This is because,with small $N$ ,the results of the diffusion model exhibit significant randomness,which diminishes as $N$ increases,leading to results that are closer to the true values. When the average of 10 generated samples is taken as the final result, the error significantly decreases, being much lower than that of the result obtained by generating a single sample. However,when $N$ exceeds 10,generating more samples will not substantially decrease errors. For example, $N = {40}$ only reduces the MAE by ${3.89}\%$ compared to $N = {10}$ ,while the space complexity increases by 109.05% and the time complexity by 252.52%. Therefore,we chose the most cost-effective $N$ of 10 .

#### 5.7.4. ResGAT hyperparameter

We explored the impact of three key hyperparameters of ResGAT on the results: the dimension of hidden layers, the number of GAT layers, and the number of attention heads in GAT. Fig. 11 presents the analysis results of these hyperparameters. From the figure, we can draw the following conclusions: (1) The optimal dimension of the hidden layers is 32. Smaller hidden layer dimensions lead to underfitting, while larger dimensions result in overfitting. (2) A reasonable number of GAT layers improves the generation accuracy, and the model achieves the lowest error when using two GAT layers. It is important to note that as the number of GAT layers increases, the generation error rises. This phenomenon can be attributed to the sparse structure of the road network's adjacency matrix. With higher sparsity, using multiple GAT layers carries the risk of over-smoothing, reducing the efficiency of GAT. (3) The number of attention heads in GAT significantly impacts the results, and the optimal number is 4 . Each attention head focuses on one interaction pattern between nodes. With few attention heads, the model cannot capture the various interaction patterns between nodes. Conversely, excessive attention heads lead to overfitting. Therefore, selecting the proper number of attention heads is crucial.

<!-- Meanless: 17-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

<!-- figureText: 4.2 4.2 9.8% 9.6% 4.0 9.2% CRPS 3.6 9.0% 8.8% 3.2 8.6% 3.0 Hidden Dimension GAT Layers GAT Heads GAT Layers GAT Heads 6.0 4.0 RMSE 5.0 3.4 3.0 6326 den Dimension GAT Layers GAT Heads nensio GAT Layers GAT Heads -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_17.jpg?x=183&y=155&w=1337&h=289&r=0"/>

Fig. 11. Performance comparison of different ResGAT hyperparameters.

<!-- figureText: head, head, head. ${\text{head}}_{4}$ 1.0 0.8 0.6 0.4 0.2 ... 0.0 Adjacent Matrix layer. 10 ... layer, -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_17.jpg?x=256&y=537&w=1191&h=481&r=0"/>

Fig. 12. Visualizations of the learned attention weights of ResGAT in the AST-Module.

<!-- Media -->

Additionally, to validate the capability of ResGAT in capturing non-uniform dependencies between road segments, we visualized the attention weights learned by ResGAT in the AST-Module for 50 road segments at 8:00 AM on 7 December 2020. The hyperparameters of ResGAT were set to their optimal values. As shown in Fig. 12, the left side illustrates the adjacency matrix of the road segments, providing a reference for their structural relationships. The right side displays the attention weights of each head in the two layers of the ResGAT network (with four heads per layer). ResGAT assigns varying levels of attention to different road segments based on their importance, rather than distributing uniform weights to all neighboring segments. Moreover, different heads capture distinct dependency patterns, enhancing the model's ability to account for the diverse influences between road segments.

#### 5.7.5. GAUNet hyperparameter

We examined the impact of different numbers of sampling layers of GAUNet on the model's performance, and the results are presented in Table 6. In the table, 0 layer indicates no use of the U-Net structure, instead using a two-layer fully connected layer. From the table, it is evident that employing GAUNet layers effectively enhances the model's performance. This is because the graph neural networks within GAUNet can better capture the spatial correlations between road segments. Simultaneously, the U-shaped network structure is effective at capturing features across different layers of the graph neural network. Our observations indicate that more GAUNet layers do not necessarily lead to better performance. Under our experimental conditions, the model performs best when the number of layers is set to 2 . At this point, the graph node features are down-sampled twice, reducing the dimensionality to one-quarter of the original. Increasing the number of layers only has a slight impact on model performance.

## 6. Discussions

### 6.1. Ablation study

We conducted ablation experiments on the TSGDiff model to investigate the impact of each component on the model's performance.

<!-- Meanless: 18-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

Table 6

Performance of different sampling layers. Layers indicate the number of down-sampling layers, which is symmetrical to the up-sampling layers.

<table><tr><td>Layers</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td></tr><tr><td>RMSE</td><td>10.08</td><td>5.96</td><td>5.78</td><td>5.84</td><td>5.91</td></tr><tr><td>MAE</td><td>6.47</td><td>4.16</td><td>4.10</td><td>4.13</td><td>4.14</td></tr><tr><td>MAPE</td><td>15.46</td><td>9.44</td><td>9.31</td><td>9.38</td><td>9.41</td></tr><tr><td>CRPS</td><td>6.06</td><td>3.97</td><td>3.88</td><td>3.9</td><td>3.95</td></tr></table>

Table 7

Impact of different information on generation results. Values in parentheses indicate the error increase compared to the complete TSGDiff model.

<table><tr><td/><td>w/o AST Module</td><td>w/o Prediction Step</td><td>w/o Semantic Encoding</td><td>TSGDiff</td></tr><tr><td>RMSE</td><td>${8.22}\left( {+{2.44}}\right)$</td><td>${7.63}\left( {+{1.85}}\right)$</td><td>10.19 (+4.41)</td><td>5.78</td></tr><tr><td>MAE</td><td>5.26 (+1.16)</td><td>${4.99}\left( {+{0.89}}\right)$</td><td>${6.50}\left( {+{2.4}}\right)$</td><td>4.10</td></tr><tr><td>MAPE</td><td>12.38 (+3.08)</td><td>11.87 (+2.56)</td><td>15.54 (+6.24)</td><td>9.31</td></tr><tr><td>CRPS</td><td>${5.23}\left( {+{1.35}}\right)$</td><td>4.91 (+1.03)</td><td>${6.24}\left( {+{2.36}}\right)$</td><td>3.88</td></tr></table>

<!-- figureText: 7.70 12% 11.88% 5.0 4.89 11% 10.79% 4.52 4.48 10% 9.64% 9.75% ${07} - {4.12}$ 4.0 3.99 3.88 9% 8% 3.5 MAPE CRPS 7.5 5.0 4.67 4.63 7.0 4.5 4.27 4.10 6.0 5.99 5.5 RMSE MAE -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_18.jpg?x=187&y=727&w=1323&h=280&r=0"/>

Fig. 13. Impact of different traffic environment factors on the results.

<!-- Media -->

#### 6.1.1. Effectiveness of multi-source information

To validate the effectiveness of the different traffic information in TSGDiff, we designed three ablation variants. These variants retain the point-to-point prediction approach and all other modules, with only the specified modules removed.

1. w/o AST Module: This variant removes the historical spatiotemporal information from TSGDiff.

2. w/o Prediction Step: This variant removes the prediction step information from TSGDiff.

3. w/o Semantic Encoding: This variant removes the future traffic environment information from TSGDiff.

The experimental results in Table 7 indicate that removing any type of traffic information results in significant performance degradation. Errors increase the most when the semantic encoding of future traffic environment information is removed (w/o Semantic Encoding). This finding suggests a strong correlation between the traffic state predictions generated by TSGDiff and the traffic environment information. When historical spatiotemporal information is excluded (w/o AST Module), the model fails to capture the spatiotemporal characteristics of traffic flow, leading to a decline in prediction accuracy. When TSGDiff removes prediction step information (w/o Prediction Step), the model's prediction accuracy also declines. In this case, without the guidance of prediction step information, the model fails to effectively perform long-term prediction, making it unable to handle the dynamic changes of long-term dependencies in traffic state. This further demonstrates the crucial role of prediction steps in improving accuracy.

#### 6.1.2. Effectiveness of environment factors

As found in 6.1.1 removing the traffic environment information causes severe model degradation. To investigate the impact of different environment factors on the generation results, we separately analyzed the effect of removing individual environment factors on the model performance. The experimental results are shown in Fig. 13.

Removing any type of traffic environment factors will increase the error of the generated results. This finding indicates a strong correlation between the traffic state and the traffic environment information we considered. Among these, time information is most closely related to the traffic state, followed by the day of the week. The errors in TSGDiff's generated results increase significantly when time and day of the week information are removed. Both types of information are time-related, indicating that TSGDiff has learned the inherent patterns of traffic states at different times, as traffic state changes often exhibit periodicity. The impact of weather information is slightly weaker compared to the first two. Different weather conditions can influence peoples travel plans. The three types of information with the least impact are peak hours, workdays, and traffic control measures. This is understandable because peak hours can be inferred from time information. While learning time information, TSGDiff may implicitly capture some peak hour features. The workday factor is strongly correlated with the day of the week factor, as typically, Monday through Friday are workdays, and Saturday and Sunday are holidays. Additionally, traffic control measures are often related to the day of the week. Therefore, these types of traffic environment information have a weaker effect on the generated traffic state.

<!-- Meanless: 19-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

Table 8

Effects of different information fusion methods. Values in parentheses indicate the error increases relative to the linear attention fusion method.

<table><tr><td/><td>No fusion</td><td>Simple fusion</td><td>Linear attention</td></tr><tr><td>RMSE</td><td>${6.30}\left( {+{0.52}}\right)$</td><td>6.10 (+0.32)</td><td>5.78</td></tr><tr><td>MAE</td><td>5.45 (+1.34)</td><td>${4.60}\left( {+{0.50}}\right)$</td><td>4.10</td></tr><tr><td>MAPE</td><td>10.22 (+0.91)</td><td>9.71 (+0.41)</td><td>9.31</td></tr><tr><td>CRPS</td><td>4.15 (+0.27)</td><td>${4.06}\left( {+{0.18}}\right)$</td><td>3.88</td></tr></table>

<!-- figureText: 7.0 11.0% 10.89% 4.5 $\begin{array}{ll} {4.31} & {4.33} \end{array}$ 10.5% 4.18 10.27% 3.99 10.0% 3.88 9.5% 9.51% 9.0% 3.5 8.5% 8.0% 3.0 MAPE CRPS w/o U-net w/o Diffusion TSGDiff 4.8 4.76 4.6 4.53 4.53 6.37 6.0 6.00 4.2 4.10 5.78 3.8 3.6 5.0 RMSE MAE w/o ResGAT w/o Linear Attention -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_19.jpg?x=186&y=442&w=1333&h=309&r=0"/>

Fig. 14. Results of GAUNet ablation experiments.

<!-- Media -->

#### 6.1.3. Effectiveness of multi-head linear attention

In TSGDiff, we adopted a multi-head linear attention mechanism to fuse multi-source information. To investigate its effectiveness, we compared three methods and presented the experimental results in Table 8. These methods include:

1. No Fusion: Directly concatenating the features after embedding without any fusion.

2. Simple Fusion: Fusing the features from method 1 with a fully connected neural network.

3. Linear Attention Fusion: The method used in this paper, employing the multi-head linear attention for fusion.

From Table 8, we can observe that no fusion results in significant model degradation. Without information fusion, all information is fed into the denoising network GAUNet with the same weight, making it impossible to distinguish the importance of different environment information. The linear attention mechanism reduces errors more effectively than simple fusion. This is because it can more accurately determine the attention weights of different information. The linear attention mechanism considers the interaction between each type of information, whereas the simple fusion mechanism mixes all information, failing to explain the importance of different information. As analyzed in the previous section, different information plays different roles in the generation process. Therefore, the linear attention mechanism is more suitable for TSGDiff.

#### 6.1.4. Effectiveness of gaunet

To study the role of GAUNet in the diffusion process, we designed four variants of TSGDiff and compared them with the complete version of TSGDiff. The experimental results are shown in Fig. 14. The differences are as follows:

1. w/o ResGAT: This variant replaces the ResGAT component in GAUNet with a fully connected network for dimensionality reduction and upscaling.

2. w/o Linear Attention: This variant removes the linear multi-head attention mechanism from GAUNet.

3. w/o U-net: This variant removes the U-net structure and uses only a two-layer ResGAT to predict noise.

4. w/o Diffusion: This variant removes the diffusion process and also uses a two-layer ResGAT to predict future traffic states.

The experimental results indicate that removing the U-net structure leads to significant degradation, demonstrating the U-net structure's superiority in capturing the correlations among features of different dimensions. When ResGAT is removed, the model errors increase, validating the importance of modeling the spatial correlations in the road network. After eliminating the linear attention mechanism, the model's ability to model the road network space slightly decreases, confirming the linear attention mechanism's capacity to capture global features. The prediction accuracy drops most significantly when the diffusion process is removed, highlighting that the multi-step denoising process is well-suited for traffic generation tasks guided by multi-source traffic information.

### 6.2. Case study

To concretely demonstrate the performance of TSGDiff in both short-term and long-term predictions, we randomly selected 50 road segments and visualized the results on 13 January 2020, at 15-min, 1-h, and 3-h prediction horizons. The visualization results are shown in Fig. 15. We chose a sampling number of 10 and plotted their mean and interquartile range (IQR). The experimental results indicate that TSGDiff's predictions are very close to the ground truth and it provides a prediction interval that reflects the data uncertainty. In contrast, traditional methods cannot reveal such uncertainty and only provide a single predicted value. Additionally, TSGDiff is stable across all prediction horizons, as evidenced by the similar lengths of the IQR in Fig. 15. Therefore, TSGDiff achieves satisfactory results across all prediction horizons.

<!-- Meanless: 20-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

<!-- Media -->

<!-- figureText: 13/01/2020/03:00 Ground Truth and Generated Values with IQR 13/01/2020/10:00 Ground Truth and Generated Values with IQR 13/01/2020/18:00 Ground Truth and Generated Values with IQR Speed/ $\left( {\mathrm{{km}}/\mathrm{h}}\right)$ 100 13/01/2020/13:00 Ground Truth and Generated Values with IQR -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_20.jpg?x=184&y=153&w=1337&h=431&r=0"/>

Fig. 15. Road speeds at 3:00, 10:00, 13:00, and 18:00 on 13 January 2020, at different prediction horizons. The $x$ -axis represents the road id, and the $y$ -axis represents the speed.

<!-- figureText: Ground Truth 30min 30min IQR Time/(hour) 18 Moderate congestion Severe congestion Speed/(km/h) 75min IQR 180min IQR Free flow Stable flow -->

<img src="https://cdn.noedgeai.com/bo_d2u6hl3ef24c73b31f10_20.jpg?x=186&y=735&w=1334&h=822&r=0"/>

Fig. 16. Prediction interval results on 13 January 2020, under different prediction horizons and traffic states.

<!-- Media -->

We present the prediction interval variation for Road ID 128 (YueGeZhuangQiao-FengBeiQiao) on 13 January under different prediction horizons and traffic conditions in Fig. 16. The results indicate that TSGDiff provides a relatively stable prediction interval across all prediction horizons. This stability is attributed to our point-to-point generation strategy and the incorporation of traffic environment factors, which reduce the impact of the prediction horizon on the results, highlighting TSGDiff's advantage in long-term prediction. In free flow and stable flow states, the prediction interval is narrow, with the true values positioned near the median of the interval, as vehicle speeds are stable across the network. In contrast, during moderate congestion and severe congestion states-usually corresponding to peak hours ( $8\mathrm{{AM}}$ and $6\mathrm{{PM}}$ )-TSGDiff produces the widest prediction intervals and shows relatively lower accuracy. This is due to the higher uncertainty and complexity of traffic states during these periods. Nonetheless, TSGDiffs s prediction intervals nearly encompass the true values, further confirming its strength in capturing uncertainties in diverse traffic scenarios.

<!-- Meanless: 21-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

### 6.3. Dataset selection

This study utilized a real-world multi-source dataset from Beijing's complex traffic environment to validate the effectiveness of TSGDiff. This dataset includes various environment factors such as weather, holidays, and traffic management measures, all of which have a significant impact on traffic states. By integrating these multi-source data with the traffic state generation model, we are better equipped to address the challenges of urban traffic complexity. However, this does not imply that the applicability of TSGDiff is limited to such datasets. The design principles of multi-source information fusion and generative modeling invest the model with a high level of generalizability, making it adaptable to a broader range of traffic scenarios. The public datasets like the PEMS-Bay dataset contain traffic state information like speed and volume but lack the comprehensive environment information necessary for our study. Therefore, we did not perform direct comparisons using public datasets. This may be a limitation of our current model. But to ensure fairness and validity of the results, we conducted experiments comparing TSGDiff and a set of baseline models under the same Beijing dataset and settings.

## 7. Conclusions

This paper innovatively proposes a diffusion model for traffic state generation guided by multi-source traffic information, named TSGDiff. Unlike traditional models that focus solely on deterministic predictions, TSGDiff leverages a generative modeling framework to not only provide single-point predictions but also generate probabilistic distributions of future traffic states. This generative modeling approach equips the model with the ability to handle complex and dynamic traffic states, particularly excelling in long-term forecasting and in scenarios where traffic states exhibit randomness. TSGDiff combines the advantages of diffusion models with the feature extraction and integration capabilities of deep neural networks, enabling realistic and accurate traffic state generation. Specifically, we extract historical traffic information and traffic environment information into feature representations using an attention-based spatiotemporal extraction module and a multi-head attention extraction module, respectively. Unlike traditional iterative prediction methods, we pass the prediction horizon as a parameter to the model and generate future traffic states point-to-point. The prediction horizon is also transformed into a feature representation by time embedding. Subsequently, we integrate the above feature representations into a conditional representation to guide the diffusion model in generating traffic states. The advanced GAUNet is utilized to implement the diffusion denoising process, preserving the topological information of the road network during diffusion. Additionally, we thoroughly investigate the principles of the conditional diffusion model generating traffic states on the road network, to enhance the model's interpretability.

The introduction of the diffusion model allows TSGDiff to maintain flexibility and robustness in complex traffic scenarios. Our experiments report both deterministic and probabilistic metrics to evaluate the model's performance. TSGDiff's generative properties lay a foundation for broader future applications, such as uncertainty quantification and risk assessment in traffic management strategies. Therefore, diffusion models offer an effective means to address the diversity of traffic states. Future research can explore combining generative diffusion models with powerful sequence modeling techniques (such as Transformer) to further improve the performance of long-term prediction and the ability to capture complex traffic patterns. This will contribute to the advancement of intelligent traffic system management. Moreover, developing more effective sampling schemes to accelerate the inference of diffusion models and reduce computational costs is also crucial for future research.

## CRediT authorship contribution statement

Huipeng Zhang: Writing - original draft, Visualization, Validation, Software, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Honghui Dong: Writing - review & editing, Supervision, Resources, Project administration, Methodology, Funding acquisition, Conceptualization. Zhiqiang Yang: Writing - review & editing, Validation, Investigation, Formal analysis.

## Acknowledgments

The authors wish to thank all team members who participated in the project. This work is supported by the National Key Research and Development Program of China (Grant No. 2022YFB4300400), the National Nature Sciences Foundation of China (Grant No. 61973027), and the Fundamental Research Funds for the Central Universities of Beijing Jiaotong University (Grant No. 2022JBZY031). References

Bai, S., Kolter, J.Z., Koltun, V., 2018. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271.

Cai, L., Janowicz, K., Mai, G., Yan, B., Zhu, R., 2020. Traffic transformer: Capturing the continuity and periodicity of time series for traffic forecasting. Trans. GIS 24 (3), 736-755.

Chen, W., Chen, L., Xie, Y., Cao, W., Gao, Y., Feng, X., 2020. Multi-range attentive bicomponent graph convolutional network for traffic forecasting. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34, pp. 3529-3536.

Chen, P., Zhang, Y., Cheng, Y., Shu, Y., Wang, Y., Wen, Q., Yang, B., Guo, C., 2024. Pathformer: Multi-scale transformers with adaptive pathways for time series forecasting. In: International Conference on Learning Representations. ICLR.

Chung, J., Gulcehre, C., Cho, K., Bengio, Y., 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.

<!-- Meanless: 22-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

Daiya, D., Lin, C., 2021. Stock movement prediction and portfolio management via multimodal learning with transformer. In: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing. ICASSP, IEEE, pp. 3305-3309.

Dhariwal, P., Nichol, A., 2021. Diffusion models beat gans on image synthesis. Adv. Neural Inf. Process. Syst. 34, 8780-8794.

Dinh, L., Sohl-Dickstein, J., Bengio, S., 2016. Density estimation using real nvp. arXiv preprint arXiv:1605.08803.

Duan, P., Mao, G., Zhang, C., Wang, S., 2016. STARIMA-based traffic prediction with time-varying lags. In: 2016 IEEE 19th International Conference on Intelligent Transportation Systems. ITSC, IEEE, pp. 1610-1615.

Feng, S., Wei, S., Zhang, J., Li, Y., Ke, J., Chen, G., Zheng, Y., Yang, H., 2023. A macro-micro spatio-temporal neural network for traffic prediction. Transp. Res. C: Emerg. Technol. 156, 104331.

Gong, S., Li, M., Feng, J., Wu, Z., Kong, L., 2022. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014. Generative adversarial nets. Adv. Neural Inf. Process. Syst. 27.

Guo, S., Lin, Y., Feng, N., Song, C., Wan, H., 2019a. Attention based spatial-temporal graph convolutional networks for traffic flow forecasting. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33, pp. 922-929.

Guo, S., Lin, Y., Li, S., Chen, Z., Wan, H., 2019b. Deep spatial-temporal 3D convolutional neural networks for traffic data forecasting. IEEE Trans. Intell. Transp. Syst. 20 (10), 3913-3926.

Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.P., Poole, B., Norouzi, M., Fleet, D.J., 2022. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303.

Ho, J., Jain, A., Abbeel, P., 2020. Denoising diffusion probabilistic models. Adv. Neural Inf. Process. Syst. 33, 6840-6851.

Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Comput. 9 (8), 1735-1780.

Hong, W.-C., 2011. Traffic flow forecasting by seasonal SVR with chaotic simulated annealing algorithm. Neurocomputing 74 (12-13), 2096-2107.

Huang, R., Lam, M.W., Wang, J., Su, D., Yu, D., Ren, Y., Zhao, Z., 2022. Fastdiff: A fast conditional diffusion model for high-quality speech synthesis. arXiv preprint arXiv:2204.09934.

Huo, G., Zhang, Y., Wang, B., Gao, J., Hu, Y., Yin, B., 2023. Hierarchical spatio-temporal graph convolutional networks and transformer network for traffic flow forecasting. IEEE Trans. Intell. Transp. Syst..

Jia, D., Chen, H., Zheng, Z., Watling, D., Connors, R., Gao, J., Li, Y., 2021. An enhanced predictive cruise control system design with data-driven traffic prediction. IEEE Trans. Intell. Transp. Syst. 23 (7), 8170-8183.

Jiang, B., Fei, Y., 2016. Vehicle speed prediction by two-level data driven models in vehicular networks. IEEE Trans. Intell. Transp. Syst. 18 (7), 17931801.

Ke, J., Zheng, H., Yang, H., Chen, X.M., 2017. Short-term forecasting of passenger demand under on-demand ride services: A spatio-temporal deep learning approach. Transp. Res. C: Emerg. Technol. 85, 591-608.

Kingma, D.P., Dhariwal, P., 2018. Glow: Generative flow with invertible 1x1 convolutions. Adv. Neural Inf. Process. Syst. 31

Kingma, D.P., Welling, M., 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.

Lei, D., Xu, M., Wang, S., 2024. A conditional diffusion model for probabilistic estimation of traffic states at sensor-free locations. Transp. Res. C: Emerg. Technol. 166, 104798.

Li, H., Liu, J., Han, S., Zhou, J., Zhang, T., Chen, C.P., 2024a. STFGCN: Spatial-temporal fusion graph convolutional network for traffic prediction. Expert Syst. Appl. 255, 124648.

Li, X., Thickstun, J., Gulrajani, I., Liang, P.S., Hashimoto, T.B., 2022. Diffusion-Im improves controllable text generation. Adv. Neural Inf. Process. Syst. 35, 4328-4343.

Li, Q., Xu, P., He, D., Wu, Y., Tan, H., Yang, X., 2024b. Multi-source information fusion graph convolution network for traffic flow prediction. Expert Syst. Appl. 252, 124288.

Li, Y., Yu, R., Shahabi, C., Liu, Y., 2017. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926.

Liu, J., Guan, W., 2004. A summary of traffic flow forecasting methods. J. Highw. Transp. Res. Dev. 21 (3), 82-85.

Liu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., Long, M., 2024. Itransformer: Inverted transformers are effective for time series forecasting. In: International Conference on Learning Representations. ICLR.

Liu, L., Qiu, Z., Li, G., Wang, Q., Ouyang, W., Lin, L., 2019. Contextualized spatial-temporal network for taxi origin-destination demand prediction. IEEE Trans. Intell. Transp. Syst. 20 (10), 3875-3887.

Lu, W., Rui, Y., Ran, B., 2020. Lane-level traffic speed forecasting: a novel mixed deep learning model. IEEE Trans. Intell. Transp. Syst. 23 (4), 3601-3612.

Luo, Z., Chen, D., Zhang, Y., Huang, Y., Wang, L., Shen, Y., Zhao, D., Zhou, J., Tan, T., 2023. Videofusion: Decomposed diffusion models for high-quality video generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10209-10218.

Matheson, J.E., Winkler, R.L., 1976. Scoring rules for continuous probability distributions. Manag. Sci. 22 (10), 10871096.

Nichol, A.Q., Dhariwal, P., 2021. Improved denoising diffusion probabilistic models. In: International Conference on Machine Learning. PMLR, pp. 8162-8171. Park, C., Lee, C., Bahng, H., Tae, Y., Jin, S., Kim, K., Ko, S., Choo, J., 2020. ST-GRAT: A novel spatio-temporal graph attention networks for accurately forecasting dynamically changing road speed. In: Proceedings of the 29th ACM International Conference on Information and Knowledge Management. pp. 1215-1224.

Patashnik, O., Wu, Z., Shechtman, E., Cohen-Or, D., Lischinski, D., 2021. Styleclip: Text-driven manipulation of stylegan imagery. In: Proceedings of the IEEE/CUF International Conference on Computer Vision. pp. 2085-2094.

Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M., 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125. Rice, J., Van Zwet, E., 2004. A simple and effective method for predicting travel times on freeways. IEEE Trans. Intell. Transp. Syst. 5 (3), 200-207.

Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B., 2022. High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10684-10695.

Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, pp. 234-241.

Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., 2022. Photorealistic text-to-image diffusion models with deep language understanding. Adv. Neural Inf. Process. Syst. 35, 36479-36494.

Shabani, A., Abdi, A., Meng, L., Sylvain, T., 2023. Scaleformer: Iterative multi-scale refining transformers for time series forecasting. In: International Conference on Learning Representations. ICLR.

Shen, K., Ju, Z., Tan, X., Liu, Y., Leng, Y., He, L., Qin, T., Zhao, S., Bian, J., 2023. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. arXiv preprint arXiv:2304.09116.

Shen, Z., Zhang, M., Zhao, H., Yi, S., Li, H., 2021. Efficient attention: Attention with linear complexities. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 3531-3539.

Shin, J., Sunwoo, M., 2018. Vehicle speed prediction using a Markov chain with speed constraints. IEEE Trans. Intell. Transp. Syst. 20 (9), 3201-3211.

Shin, Y., Yoon, Y., 2024. PGCN: Progressive graph convolutional networks for spatial-temporal traffic forecasting. IEEE Trans. Intell. Transp. Syst.

Smola, A.J., Schlkopf, B., 2004. A tutorial on support vector regression. Stat. Comput. 14, 199-222.

Song, C., Lin, Y., Guo, S., Wan, H., 2020a. Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34, pp. 914-921.

Song, J., Meng, C., Ermon, S., 2020b. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502.

<!-- Meanless: 23-->




<!-- Meanless: H. Zhang et al. Transportation Research Part C 174 (2025) 105081-->

Van Hinsbergen, C.P., Schreiter, T., Zuurbier, F.S., Van Lint, J., Van Zuylen, H.J., 2011. Localized extended kalman filter for scalable real-time traffic state estimation. IEEE Trans. Intell. Transp. Syst. 13 (1), 385-394.

Vaswani, A., Shazer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2017. Attention is all you need. In: Advances in Neural Information Processing Systems. pp. 5998-6008.

Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y., 2017. Graph attention networks. Stat 1050, 20.

Vignac, C., Krawczuk, I., Siraudin, A., Wang, B., Cevher, V., Frossard, P., 2022. Digress: Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734.

Wang, Q., Liu, W., Wang, X., Chen, X., Chen, G., Wu, Q., 2023. GMHANN: A novel traffic flow prediction method for transportation management based on spatial-temporal graph modeling. IEEE Trans. Intell. Transp. Syst..

Wang, S., Wu, H., Shi, X., Hu, T., Luo, H., Ma, L., Zhang, J.Y., ZHOU, J., 2024. TimeMixer: Decomposable multiscale mixing for time series forecasting. In: International Conference on Learning Representations. ICLR.

Wei, S., Feng, S., Yang, H., 2024. Multi-view spatial-temporal graph convolutional network for traffic prediction. IEEE Trans. Intell. Transp. Syst.

Wen, H., Lin, Y., Xia, Y., Wan, H., Wen, Q., Zimmermann, R., Liang, Y., 2023. Diffstg: Probabilistic spatio-temporal graph forecast models. In: Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems. pp. 1-12.

Wu, Y., He, K., 2018. Group normalization. In: Proceedings of the European Conference on Computer Vision. ECCV, pp. 3-19.

Wu, Z., Pan, S., Long, G., Jiang, J., Zhang, C., 2019. Graph wavenet for deep spatial-temporal graph modeling. arXiv preprint arXiv:1906.00121.

Wu, S., Xiao, X., Ding, Q., Zhao, P., Wei, Y., Huang, J., 2020. Adversarial sparse transformer for time series forecasting. Adv. Neural Inf. Process. Syst. 33, 17105-17115.

Yan, X., Gan, X., Tang, J., Zhang, D., Wang, R., 2024. ProSTformer: Progressive space-time self-attention model for short-term traffic flow forecasting. IEEE Trans. Intell. Transp. Syst.,

Yang, H., Liu, C., Zhu, M., Ban, X., Wang, Y., 2021. How fast you will drive? Predicting speed of customized paths by deep neural network. IEEE Trans. Intell. Transp. Syst. 23 (3), 2045-2055.

Yu, B., Yin, H., Zhu, Z., 2017. Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting. arXiv preprint arXiv:1709.04875.

Yuan, Y., Ding, J., Shao, C., Jin, D., Li, Y., 2023. Spatio-temporal diffusion point processes. In: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. pp. 3173-3184.

Zhang, Y., Ma, L., Pal, S., Zhang, Y., Coates, M., 2024. Multi-resolution time-series transformer for long-term forecasting. In: International Conference on Artificial Intelligence and Statistics. PMLR, pp. 4222-4230.

Zhang, C., Zhang, Y., Shao, Q., Li, B., Lv, Y., Piao, X., Yin, B., 2023. ChatTraffic: Text-to-traffic generation via diffusion model. arXiv preprint arXiv:2311.16203.

Zhao, J., Liu, Z., Sun, Q., Li, Q., Jia, X., Zhang, R., 2022. Attention-based dynamic spatial-temporal graph convolutional networks for traffic speed forecasting. Expert Syst. Appl. 204, 117511.

Zhao, L., Song, Y., Zhang, C., Liu, Y., Wang, P., Lin, T., Deng, M., Li, H., 2019. T-scen: A temporal graph convolutional network for traffic prediction. IEEE Trans. Intell. Transp. Syst. 21 (9), 3848-3858.

Zheng, C., Fan, X., Wang, C., Qi, J., 2020. Gman: A graph multi-attention network for traffic prediction. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34, pp. 1234-1241.

Zou, G., Lai, Z., Ma, C., Li, Y., Wang, T., 2023. A novel spatio-temporal generative inference network for predicting the long-term highway traffic speed. Transp. Res. C: Emerg. Technol. 154, 104263.

<!-- Meanless: 24-->

